<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Transformer 结构学习总结 - lxkaka</title><meta name="Description" content="lxkaka&#39;s blog"><meta property="og:title" content="Transformer 结构学习总结" />
<meta property="og:description" content="MathJax.Hub.Config({ tex2jax: { inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]], processEscapes: true}, jax: [&#34;input/TeX&#34;,&#34;input/MathML&#34;,&#34;input/AsciiMath&#34;,&#34;output/CommonHTML&#34;], extensions: [&#34;tex2jax.js&#34;,&#34;mml2jax.js&#34;,&#34;asciimath2jax.js&#34;,&#34;MathMenu.js&#34;,&#34;MathZoom.js&#34;,&#34;AssistiveMML.js&#34;, &#34;[Contrib]/a11y/accessibility-menu.js&#34;], TeX: { extensions: [&#34;AMSmath.js&#34;,&#34;AMSsymbols.js&#34;,&#34;noErrors.js&#34;,&#34;noUndefined.js&#34;], equationNumbers: { autoNumber: &#34;AMS&#34; } } }); transformer 已经成了当前深度学习领域最著名的网络架构，它是各大语言模型的基石。在这篇文章里我把自己在了" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lxkaka.wang/learn-transformer/" />
<meta property="og:image" content="https://lxkaka.wang/logo.png"/>
<meta property="article:published_time" content="2023-10-24T13:34:55+08:00" />
<meta property="article:modified_time" content="2023-10-24T13:34:55+08:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lxkaka.wang/logo.png"/>

<meta name="twitter:title" content="Transformer 结构学习总结"/>
<meta name="twitter:description" content="MathJax.Hub.Config({ tex2jax: { inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]], processEscapes: true}, jax: [&#34;input/TeX&#34;,&#34;input/MathML&#34;,&#34;input/AsciiMath&#34;,&#34;output/CommonHTML&#34;], extensions: [&#34;tex2jax.js&#34;,&#34;mml2jax.js&#34;,&#34;asciimath2jax.js&#34;,&#34;MathMenu.js&#34;,&#34;MathZoom.js&#34;,&#34;AssistiveMML.js&#34;, &#34;[Contrib]/a11y/accessibility-menu.js&#34;], TeX: { extensions: [&#34;AMSmath.js&#34;,&#34;AMSsymbols.js&#34;,&#34;noErrors.js&#34;,&#34;noUndefined.js&#34;], equationNumbers: { autoNumber: &#34;AMS&#34; } } }); transformer 已经成了当前深度学习领域最著名的网络架构，它是各大语言模型的基石。在这篇文章里我把自己在了"/>
<meta name="application-name" content="lxkaka">
<meta name="apple-mobile-web-app-title" content="lxkaka"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://lxkaka.wang/learn-transformer/" /><link rel="prev" href="https://lxkaka.wang/llm-code-review/" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.7.2/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Transformer 结构学习总结",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/lxkaka.wang\/learn-transformer\/"
        },"image": {
                "@type": "ImageObject",
                "url": "https:\/\/lxkaka.wang\/cover.png",
                "width":  800 ,
                "height":  600 
            },"genre": "posts","keywords": "transformer, attention","wordcount":  3409 ,
        "url": "https:\/\/lxkaka.wang\/learn-transformer\/","datePublished": "2023-10-24T13:34:55+08:00","dateModified": "2023-10-24T13:34:55+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
                "@type": "Organization",
                "name": "xxxx",
                "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/lxkaka.wang\/logo.png",
                "width":  127 ,
                "height":  40 
                }
            },"author": {
                "@type": "Person",
                "name": "lxkaka"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="lxkaka"><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw'></i></span>lxkaka</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/friend/"> 友链 </a><a class="menu-item" href="/about/"> 关于 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="lxkaka"><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw'></i></span>lxkaka</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/friend/" title="">友链</a><a class="menu-item" href="/about/" title="">关于</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">Transformer 结构学习总结</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://www.lxkaka.wang" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw"></i>lxkaka</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/ai/"><i class="far fa-folder fa-fw"></i>AI</a>&nbsp;<a href="/categories/%E5%8E%9F%E7%90%86/"><i class="far fa-folder fa-fw"></i>原理</a>&nbsp;<a href="/categories/%E5%9F%BA%E7%A1%80/"><i class="far fa-folder fa-fw"></i>基础</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2023-10-24">2023-10-24</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 3409 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 7 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#multi-head-attention">multi-head attention</a>
          <ul>
            <li><a href="#values-计算过程">&ldquo;values&rdquo; 计算过程</a></li>
            <li><a href="#scaled-dot-product">Scaled-dot product</a></li>
          </ul>
        </li>
        <li><a href="#encoder-decoder-结构">encoder-decoder 结构</a>
          <ul>
            <li><a href="#encoder">encoder</a>
              <ul>
                <li><a href="#残差连接">残差连接</a></li>
              </ul>
            </li>
            <li><a href="#decoder">decoder</a></li>
            <li><a href="#线性层和-softmax">线性层和 softmax</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true},
      jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
      extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
      TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
      equationNumbers: {
      autoNumber: "AMS"
      }
    }
  });
</script>
<p>transformer 已经成了当前深度学习领域最著名的网络架构，它是各大语言模型的基石。在这篇文章里我把自己在了解 transformer 的学习资料做了一个总结，一是加深体会二是希望帮助到其他想学习的同学。
我认为最 transformer 中最关键的也是最不容易懂的部分应该就是 <code>multi-head attention</code>, 所以这部分会重点介绍。在此基础上再去理解整个架构就会比较容易。</p>
<h3 id="multi-head-attention">multi-head attention</h3>
<p><code>multi-head attention</code> 是 transformer 关键组成部分，我们会重点理解这部分实现。下图是此部分的架构图</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/multi-head-att-arch.png"
        data-srcset="https://pics.lxkaka.wang/ai/multi-head-att-arch.png, https://pics.lxkaka.wang/ai/multi-head-att-arch.png 1.5x, https://pics.lxkaka.wang/ai/multi-head-att-arch.png 2x"
        data-sizes="auto"
        alt="multi-head-att-arch"
        title="multi-head-att-arch" /></p>
<p>首先拆分 V：&ldquo;values&rdquo;，K：&ldquo;keys&rdquo;，Q：&ldquo;queries&rdquo;，然后线性层（图中的&quot;linear&rdquo;）来转换 &ldquo;values&rdquo;、&ldquo;keys&rdquo; 和 &ldquo;queries&rdquo;。接下来，计算注意力权重并重新加权 &ldquo;values&rdquo;，并对重新加权的 &ldquo;values&rdquo; 求和，然后拼接所得的总和。最后，将拼接的 &ldquo;values&rdquo; 传递到另一个线性层。scaled dot-product attention 就是如何具体计算这些注意力并重新加权 &ldquo;values&rdquo; 的问题。
&ldquo;keys&rdquo; 和 &ldquo;queries&rdquo; 相同时产生的注意力就是所谓的 &ldquo;self attention&rdquo;。s
我们拿这样一个句子 &ldquo;Anthony Hopkins admired Michael Bay as a great director&rdquo; 来作为计算 <code>multi-head self-attention</code> 的例子。在这个例子中，token 的数量是 9，每个 token 被编码为一个 512 维的嵌入向量。head 的数量是 8。在这种情况下，如下图所示，输入句子 &ldquo;Anthony Hopkins admired Michael Bay as a great director&rdquo; 被表示为 <strong>9 x 512</strong> 矩阵。首先将每个 token 拆分为 <strong>512/8=64</strong> 维，总共 8 个向量，如下图所示，输入矩阵被分为 8 个彩色块，都是 <strong>9 x 64</strong> 矩阵，但是每个矩阵都表达同一个句子。然后在 8 个 head 中独立计算输入句子的<code>self attention</code>，并根据注意力/(权重) 重新加权 &ldquo;values&rdquo;。之后，并将每个头加权后的 value 拼接起来。在重新加权 value 之后，每个彩色块（矩阵）的大小也不会改变。每个 head 都会比较每个子空间的 &ldquo;queries&rdquo; 和 &ldquo;keys&rdquo;。如果一个 Transformer 模型有 4 层，具有 8 头多头注意力，那么它的编码器至少有 4 x 8 = 32 个头，因此编码器学习输入的 token 在 32 种子空间上的关系。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/muti-att-sum.png"
        data-srcset="https://pics.lxkaka.wang/ai/muti-att-sum.png, https://pics.lxkaka.wang/ai/muti-att-sum.png 1.5x, https://pics.lxkaka.wang/ai/muti-att-sum.png 2x"
        data-sizes="auto"
        alt="multi-head-att-sum"
        title="multi-head-att-sum" /></p>
<h4 id="values-计算过程">&ldquo;values&rdquo; 计算过程</h4>
<p>首先 每个 $$[ \cdots ]$$ 表示一个 token，实际中通常是一个嵌入向量。下图展示了 &ldquo;Michael&rdquo; 作为 <code>query</code> 计算 &ldquo;values&rdquo; 的过程。query 与&quot;keys&rdquo; (即输入句子) 进行比较，得到了注意力 (权重) 的直方图，权重之和为 1。使用刚刚计算出来的权重重新加权 &ldquo;values&rdquo;（还是输入句子），并对 &ldquo;values&rdquo; 求和。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/micheael-query.png"
        data-srcset="https://pics.lxkaka.wang/ai/micheael-query.png, https://pics.lxkaka.wang/ai/micheael-query.png 1.5x, https://pics.lxkaka.wang/ai/micheael-query.png 2x"
        data-sizes="auto"
        alt="michael-query"
        title="michael-query" /></p>
<p>假设与 &ldquo;query&rdquo; token &ldquo;Michael&rdquo; 相比，&ldquo;keys&rdquo; token &ldquo;Anthony&rdquo;、&ldquo;Hopkins&rdquo;、&ldquo;admired&rdquo;、&ldquo;Michael&rdquo;、&ldquo;Bay&rdquo;、&ldquo;as&rdquo;、&ldquo;a&rdquo;、&ldquo;great&quot;和&quot;director&rdquo;。分别为 0.06、0.09、0.05、0.25、0.18、0.06、0.09、0.06、0.15。在这种情况下，重新加权的 token 之和为 0.06&quot;Anthony&rdquo; + 0.09&quot;Hopkins&rdquo; + 0.05&quot;admired&rdquo; + 0.25&quot;Michael&rdquo; + 0.18&quot;Bay&rdquo; + 0.06&quot;as&rdquo; + 0.09&quot;a&rdquo; + 0.06&quot;great&rdquo; 0.15&quot;director&rdquo;，这个数字就是我们实际使用的数字。
在实际上上面的 token 就是向量，所以结果就是重新加权后的向量。
对所有 &ldquo;query&rdquo; 重复此过程。如下图所示，会得到 9 对重新加权的 &ldquo;values&rdquo; 的总和，因为使用了输入句子 &ldquo;Anthony Hopkins admired Michael Bay as a great director&rdquo; 的每个 token。作为 &ldquo;query&rdquo;。将重新加权的 &ldquo;values&rdquo; 的总和拼接在一起，如下图紫色矩阵所示，这就是 <code>multi-head attention</code>中的 一个 head 的输出。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/value-weights.png"
        data-srcset="https://pics.lxkaka.wang/ai/value-weights.png, https://pics.lxkaka.wang/ai/value-weights.png 1.5x, https://pics.lxkaka.wang/ai/value-weights.png 2x"
        data-sizes="auto"
        alt="values-weight"
        title="values-weight" /></p>
<h4 id="scaled-dot-product">Scaled-dot product</h4>
<p>为了计算多头注意力，如上所示我们有 8 对 &ldquo;queries&rdquo;、&ldquo;keys&rdquo; 和 &ldquo;values&rdquo;，在第一部分的图中用 8 种不同的颜色展示了它们。我们在 8 个不同的 head 中独立计算注意力并重新加权 &ldquo;values&rdquo;，并且在每个 head 中，重新加权的 &ldquo;values&rdquo; 是用这个非常简单的 scaled dot-produc 公式计算的： $$Attention(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) =softmax(\frac{\boldsymbol{Q} \boldsymbol{K} ^T}{\sqrt{d}_k})\boldsymbol{V}$$
我们拿上面 蓝色 head 计算 <code>scaled dot-product</code>来举例。
下图左侧是 Transformer 原论文的一张图，解释了 <code>multi-head attention</code> 的一个 head。输入句子分为 8 个矩阵块，然后将这些块独立地放入 8 个头中。在一个 head 中，通过三个不同的全连接层（下图中的&quot;Linear&rdquo;）转换输入矩阵，并准备三个矩阵 Q, K, V，分别是 &ldquo;queries&rdquo;、&ldquo;keys&rdquo; 和 &ldquo;values&rdquo;。
如下图所示，计算 $$Attention(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})$$ 实际上只是将三个相同大小的矩阵相乘（不过只有 K 被转置）。生成的 <strong>9 x 64</strong> 矩阵就是 head 的输出。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/scale-dot1.png"
        data-srcset="https://pics.lxkaka.wang/ai/scale-dot1.png, https://pics.lxkaka.wang/ai/scale-dot1.png 1.5x, https://pics.lxkaka.wang/ai/scale-dot1.png 2x"
        data-sizes="auto"
        alt="scale-dot1"
        title="scale-dot1" />
<img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/scale-dot2.png"
        data-srcset="https://pics.lxkaka.wang/ai/scale-dot2.png, https://pics.lxkaka.wang/ai/scale-dot2.png 1.5x, https://pics.lxkaka.wang/ai/scale-dot2.png 2x"
        data-sizes="auto"
        alt="scale-dot2"
        title="scale-dot2" /></p>
<p>计算方式如下图所示。softmax 函数对重新缩放后的乘积 $$\frac{\boldsymbol{Q} \boldsymbol{K} ^T}{\sqrt{d}_k}$$ 的每一行进行正则化，得到的 <strong>9 x9</strong> 矩阵可以理解为是一种自注意力热图。<br>
将每个&quot;quey&quot;与&quot;keys&quot;进行比较的过程是通过向量和矩阵的简单相乘来完成的，如下图所示。可以得到每个&quot;query&quot;的注意力直方图，得到的 9 维向量是注意力 (权重) 的列表，也就是下图中蓝色圆圈的列表。这意味着，在 Transformer 模型中，通过计算内积来比较&quot;query&quot;和&quot;key&rdquo;。通过用 $$\sqrt{d_k}$$ 除以重新缩放向量并使用 <code>softmax</code> 函数对它们进行正则化后，将这些向量堆叠起来，堆叠的向量就是注意力的热图。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/scale-dot-heat.png"
        data-srcset="https://pics.lxkaka.wang/ai/scale-dot-heat.png, https://pics.lxkaka.wang/ai/scale-dot-heat.png 1.5x, https://pics.lxkaka.wang/ai/scale-dot-heat.png 2x"
        data-sizes="auto"
        alt="scarle-dot-heat"
        title="scarle-dot-heat" /></p>
<p>这就是上述蓝色 head 计算 attention(权重) 的过程。每个 head 同时执行安全相同的过程就实现了 <code>mutl-head attention</code> 计算并行。然后我们把所有 head 的输出拼接再传入线性层。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/multi-att-res.png"
        data-srcset="https://pics.lxkaka.wang/ai/multi-att-res.png, https://pics.lxkaka.wang/ai/multi-att-res.png 1.5x, https://pics.lxkaka.wang/ai/multi-att-res.png 2x"
        data-sizes="auto"
        alt="mult-att-res"
        title="mult-att-res" /></p>
<p>在了解完 <code>multi-head attention</code> 的计算过程，我们再来从整体看一下整个 transformer 架构的几大模块。</p>
<h3 id="encoder-decoder-结构">encoder-decoder 结构</h3>
<p>编码部分（encoders）由多层编码器 (Encoder) 组成（Transformer 论文中使用的是 6 层编码器，这里的层数 6 并不是固定的，可以根据实验效果来修改层数）。同理，解码部分（decoders）也是由多层的解码器 (Decoder) 组成（论文里也使用了 6 层解码器）。每层编码器网络结构是一样的，每层解码器网络结构也是一样的。不同层编码器和解码器网络结构不共享参数。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/encoder-docoder.jpeg"
        data-srcset="https://pics.lxkaka.wang/ai/encoder-docoder.jpeg, https://pics.lxkaka.wang/ai/encoder-docoder.jpeg 1.5x, https://pics.lxkaka.wang/ai/encoder-docoder.jpeg 2x"
        data-sizes="auto"
        alt="encoder-decoder-arch"
        title="encoder-decoder-arch" /></p>
<h4 id="encoder">encoder</h4>
<p>编码部分的输入文本序列经过输入处理之后得到了一个向量序列，这个向量序列将被送入第 1 层编码器，第 1 层编码器输出的同样是一个向量序列，再接着送入下一层编码器：第 1 层编码器的输入是融合位置向量的词向量，更上层编码器的输入则是上一层编码器的输出。
下图展示了向量序列在单层 encoder 中的流动：融合位置信息的词向量进入 self-attention 层，self-attention 的输出每个位置的向量再输入 FFN 神经网络得到每个位置的新向量。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/encoder.jpeg"
        data-srcset="https://pics.lxkaka.wang/ai/encoder.jpeg, https://pics.lxkaka.wang/ai/encoder.jpeg 1.5x, https://pics.lxkaka.wang/ai/encoder.jpeg 2x"
        data-sizes="auto"
        alt="encoder"
        title="encoder" /></p>
<h5 id="残差连接">残差连接</h5>
<p>计算得到 self-attention 的输出向量后，单层 encode r 里后续还有两个重要的操作：残差链接、标准化。
编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization），如下图所示。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/add%26norm.jpeg"
        data-srcset="https://pics.lxkaka.wang/ai/add%26norm.jpeg, https://pics.lxkaka.wang/ai/add%26norm.jpeg 1.5x, https://pics.lxkaka.wang/ai/add%26norm.jpeg 2x"
        data-sizes="auto"
        alt="add&amp;amp;norm"
        title="add&amp;amp;norm" /></p>
<h4 id="decoder">decoder</h4>
<p>编码器一般有多层，第一个编码器的输入是一个序列文本，最后一个编码器输出是一组序列向量，这组序列向量会作为解码器的 K、V 输入，其中 K=V=解码器输出的序列向量表示。这些注意力向量将会输入到每个解码器的 Encoder-Decoder Attention 层，这有助于解码器把注意力集中到输入序列的合适位置，如下图所示。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/decoder.jpeg"
        data-srcset="https://pics.lxkaka.wang/ai/decoder.jpeg, https://pics.lxkaka.wang/ai/decoder.jpeg 1.5x, https://pics.lxkaka.wang/ai/decoder.jpeg 2x"
        data-sizes="auto"
        alt="decoder"
        title="decoder" /></p>
<p>解码器中的 Self Attention 层，和编码器中的 Self Attention 层的区别：<br>
在解码器里，Self Attention 层只允许关注到输出序列中早于当前位置之前的单词。具体做法是：在 Self Attention 分数经过 Softmax 层之前，屏蔽当前位置之后的那些位置（将 attention score 设置成-inf）。
解码器 Attention 层是使用前一层的输出来构造 Query 矩阵，而 Key 矩阵和 Value 矩阵来自于编码器最终的输出。</p>
<h4 id="线性层和-softmax">线性层和 softmax</h4>
<p>Decoder 最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是线性层和 softmax 完成的。<br>
线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更大的向量，这个向量称为 logits 向量：假设我们的模型有 10000 个英语单词（模型的输出词汇表），此 logits 向量便会有 10000 个数字，每个数表示一个单词的分数。<br>
然后，Softmax 层会把这些分数转换为概率（把所有的分数转换为正数，并且加起来等于 1）。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/linear-softmax.jpeg"
        data-srcset="https://pics.lxkaka.wang/ai/linear-softmax.jpeg, https://pics.lxkaka.wang/ai/linear-softmax.jpeg 1.5x, https://pics.lxkaka.wang/ai/linear-softmax.jpeg 2x"
        data-sizes="auto"
        alt="linear-softmax"
        title="linear-softmax" /></p>
<p>现在再来看这张我们经常看到的图，脑子里应该已经有了比较清晰的概念和认识，那么这篇文章的目的就达到了。</p>
<p><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="https://pics.lxkaka.wang/ai/transformer.jpeg"
        data-srcset="https://pics.lxkaka.wang/ai/transformer.jpeg, https://pics.lxkaka.wang/ai/transformer.jpeg 1.5x, https://pics.lxkaka.wang/ai/transformer.jpeg 2x"
        data-sizes="auto"
        alt="transformer"
        title="transformer" /></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2023-10-24</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/learn-transformer/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://lxkaka.wang/learn-transformer/" data-title="Transformer 结构学习总结" data-hashtags="transformer,attention"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://lxkaka.wang/learn-transformer/" data-hashtag="transformer"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="https://lxkaka.wang/learn-transformer/" data-title="Transformer 结构学习总结" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://lxkaka.wang/learn-transformer/" data-title="Transformer 结构学习总结"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="分享到 百度" data-sharer="baidu" data-url="https://lxkaka.wang/learn-transformer/" data-title="Transformer 结构学习总结"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@2.12.0/icons/baidu.svg"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/transformer/">transformer</a>,&nbsp;<a href="/tags/attention/">attention</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/llm-code-review/" class="prev" rel="prev" title="AI 助力 Code Review 的高效方案"><i class="fas fa-angle-left fa-fw"></i>AI 助力 Code Review 的高效方案</a></div>
</div>
<div id="comments"><div id="disqus_thread" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://disqus.com/?ref_noscript">Disqus</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Welcome to lxkaka's blog</div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2017 - 2023</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://www.lxkaka.wang" target="_blank">lxkaka</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><script type="text/javascript" src="https://lxkaka.disqus.com/embed.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/smooth-scroll@16.1.3/dist/smooth-scroll.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.37.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.8/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.2.0/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.0/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'UA-173214698-1', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=UA-173214698-1" async></script></body>
</html>
