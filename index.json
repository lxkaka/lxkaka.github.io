[{"categories":["ai","实现","工程效率"],"content":"我们开发了MCP Server（Model Context Protocol Server），这是一个基于Label Studio API构建的服务层，它将传统的手动操作转变为简单的对话式指令。本文将详细介绍这一改造过程、技术实现以及实际效果评估","date":"2025-04-11","objectID":"/mcp-label-studio/","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"在AI训练数据准备过程中，高质量的标注数据是模型性能的关键保障。Label Studio作为一款功能强大的开源数据标注平台，提供了丰富的工具支持多种数据类型的标注工作。然而，在实际项目实践中，我们发现传统的Label Studio项目管理工作流仍有诸多痛点：需要频繁手动操作界面、在多个系统之间切换、编写数据转换脚本等，这些都导致了效率低下和增加了错误风险。 为了解决这些问题，我们开发了MCP Server（Model Context Protocol Server），这是一个基于Label Studio API构建的服务层，它将传统的手动操作转变为简单的对话式指令。本文将详细介绍这一改造过程、技术实现以及实际效果评估。 ","date":"2025-04-11","objectID":"/mcp-label-studio/:0:0","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"传统Label Studio标注项目管理工作流 在没有MCP Server的情况下，我们的标注项目管理流程如下图所示： 这一工作流主要分为两个阶段： ","date":"2025-04-11","objectID":"/mcp-label-studio/:1:0","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"标注启动阶段 创建项目：登录Label Studio管理界面，手动填写项目信息 导入试标数据：通过界面上传初始数据集用于试标 设计标注模板：手动编写和调整XML标注配置 试标：人工在Label Studio界面中进行试标 导出数据：手动导出并分析标注结果格式 ","date":"2025-04-11","objectID":"/mcp-label-studio/:1:1","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"标注完成阶段 数据转换：编写脚本将标注结果转换为所需格式 导入标注结果：将转换后的数据手动上传到Label Studio 验收：人工在Label Studio界面中进行结果验收 这一传统流程存在以下明显问题： 操作繁琐：每个环节都需要登录平台并进行手动操作 系统切换频繁：需要在Label Studio界面、命令行、脚本编辑器之间不断切换 技术门槛高：数据转换环节需要编程技能 效率低下：大量时间花费在重复性操作上 扩展性差：难以同时管理多个标注项目 ","date":"2025-04-11","objectID":"/mcp-label-studio/:1:2","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"MCP Server解决方案 为了解决以上问题，我们开发了MCP Server，它作为连接人与Label Studio的桥梁，提供对话式交互界面，大幅简化了标注项目管理流程。 ","date":"2025-04-11","objectID":"/mcp-label-studio/:2:0","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"MCP Server架构 MCP Server采用了三层架构设计： 交互层（Agent）：提供对话式接口，理解用户自然语言指令 服务层（MCP Server）：封装Label Studio API，提供标准化工具集 执行层（Label Studio）：底层标注平台，执行实际的标注任务 ","date":"2025-04-11","objectID":"/mcp-label-studio/:2:1","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"MCP Server功能集 MCP Server封装了以下核心功能： 功能 描述 参数 create_project 创建新项目 title, description, label_config delete_project 删除项目 project_id export_project_annotations 导出项目标注数据 project_id, export_format, output_path get_export_formats 获取支持的导出格式 project_id get_project_detail 获取项目详细信息 project_id get_projects 获取所有项目信息 无 import_tasks_from_file 从文件导入任务数据 project_id, file_path update_project 更新项目信息 project_id, title, description, label_config ","date":"2025-04-11","objectID":"/mcp-label-studio/:2:2","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"改造后的工作流程 通过MCP Server，我们的工作流程得到了极大简化，如下图所示： 改造后的工作流程主要特点： 标注管理员通过Agent进行对话式交互 Agent理解指令并调用MCP Server提供的工具集 MCP Server通过API与Label Studio交互 人工只需在Label Studio中完成试标和验收两个核心环节 具体流程如下： 标注启动阶段 创建项目：通过对话指令 → create_project 导入试标数据：通过对话指令 → import_tasks_from_file 设计标注模板：通过对话指令 → update_project 试标：人工在Label Studio界面中操作（不变） 导出数据：通过对话指令 → export_project_annotations 标注完成阶段 导入标注结果：通过对话指令 → import_tasks_from_file 验收：人工在Label Studio界面中操作（不变） ","date":"2025-04-11","objectID":"/mcp-label-studio/:2:3","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"技术实现 MCP Server的技术实现主要基于以下几个方面： ","date":"2025-04-11","objectID":"/mcp-label-studio/:3:0","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"1. Label Studio API集成 MCP Server底层调用Label Studio的REST API，我们使用Python SDK封装了这些API调用，提供了更加简洁一致的接口。这样，复杂的API参数和认证逻辑被隐藏在服务层，用户无需了解底层细节。 ","date":"2025-04-11","objectID":"/mcp-label-studio/:3:1","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"2. 对话式交互设计 MCP Server的最大特点是提供对话式交互界面，用户可以通过自然语言描述需求，而不需要记忆具体的命令格式。例如，用户可以说\"创建一个名为’图像分类’的新项目\"，Agent会理解这一指令并调用相应的API。 ","date":"2025-04-11","objectID":"/mcp-label-studio/:3:2","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"3. 环境集成 MCP Server支持与各种开发环境的集成，确保标注项目管理可以无缝融入现有的AI开发流程。这包括： 命令行界面集成 Jupyter Notebook集成 Web界面集成 ","date":"2025-04-11","objectID":"/mcp-label-studio/:3:3","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"效率对比分析 通过在实际项目中的应用，我们对比了传统工作流和MCP Server辅助工作流的效率差异： ","date":"2025-04-11","objectID":"/mcp-label-studio/:4:0","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"时间消耗对比 环节 传统方式 MCP Server 提升比例 创建项目 5分钟 10秒 97% 导入数据 10分钟 30秒 95% 更新标注模板 60分钟 5分钟 92% 导出数据 5分钟 10秒 97% 导入标注结果 60分钟 3 分钟 97% 总计 140分钟 8分50秒 94% ","date":"2025-04-11","objectID":"/mcp-label-studio/:4:1","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"操作步骤对比 环节 传统方式 MCP Server 减少比例 创建项目 8步 1步 88% 导入数据 6步 1步 83% 更新标注模板 10步 1步 90% 导出数据 5步 1步 80% 导入标注结果 7步 1步 86% 总计 36步 5步 86% ","date":"2025-04-11","objectID":"/mcp-label-studio/:4:2","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"实际效益 MCP Server在我们的项目实践中带来了显著的效益： ","date":"2025-04-11","objectID":"/mcp-label-studio/:5:0","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"1. 效率大幅提升 如上述数据所示，标注项目管理的时间成本降低了95%，操作步骤减少了86%。这不仅加快了项目进度，也减轻了标注管理员的工作负担。 ","date":"2025-04-11","objectID":"/mcp-label-studio/:5:1","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"2. 错误率显著降低 由于大部分手动操作被自动化流程替代，人为错误的发生率大幅下降。特别是在数据转换环节，MCP Server能够确保数据格式的一致性，避免了手动脚本可能带来的各种错误。 ","date":"2025-04-11","objectID":"/mcp-label-studio/:5:2","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"3. 专注核心任务 标注管理员可以将更多精力集中在试标和验收这两个需要人工判断的关键环节，而不是被繁琐的操作流程分散注意力。 ","date":"2025-04-11","objectID":"/mcp-label-studio/:5:3","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"4. 标准化与可复制 MCP Server实现了标注项目管理流程的标准化，使得成功经验可以快速复制到新项目中，提高了团队整体的工作效率。 ","date":"2025-04-11","objectID":"/mcp-label-studio/:5:4","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"5. 学习曲线平缓 新团队成员可以通过简单的对话指令快速上手标注项目管理，不需要学习复杂的Label Studio API和界面操作，降低了培训成本。 ","date":"2025-04-11","objectID":"/mcp-label-studio/:5:5","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","工程效率"],"content":"结论 MCP Server作为Label Studio的增强层，成功地将传统的手动标注项目管理工作流转变为对话式指令驱动的自动化流程。这一改造不仅显著提高了工作效率，减少了人为错误，还降低了标注项目管理的技术门槛。通过标准化的工具集和对话式交互界面，MCP Server让标注管理员可以专注于真正需要人工判断的环节，为AI训练数据的准备工作带来了质的飞跃。 我们相信，随着MCP Server的不断完善和推广，这一工具将为更多AI开发团队提供便利，推动整个行业向更高效、更智能的数据标注方向发展。 ","date":"2025-04-11","objectID":"/mcp-label-studio/:6:0","tags":["mcp","label studio","标注"],"title":"对话式指令革新标注管理：从手动Label Studio操作到MCP Server驱动的自动化工作流","uri":"/mcp-label-studio/"},{"categories":["ai","实现","service"],"content":"在人工智能领域，语音交互正变得越来越重要。实时、自然的语音对话体验，是下一代人机交互的关键。本文将深入探讨如何构建一个兼容 OpenAI realtime 协议的端到端的语音模型服务，实现实时的语音识别、自然语言理解和语音合成，最终实现流畅的对话式 AI 体验","date":"2025-01-23","objectID":"/end-to-end-audio-service/","tags":["openai realtime","moshi","audio"],"title":"实时语音服务构建实践：基于 OpenAI Real-time 协议的技术分享","uri":"/end-to-end-audio-service/"},{"categories":["ai","实现","service"],"content":"在这里和大家分享一下构建实时语音服务的实践经验，这个服务是基于 OpenAI 最新推出的 Real-time 协议实现的。 随着 AI 技术的快速发展，实时语音交互的应用前景非常广阔，OpenAI Real-time 协议的出现，为开发者构建高性能的实时语音服务提供了有力的工具。在这篇文章中，我介绍一下这个服务的构建过程，包括技术选型、架构设计、核心组件实现以及性能优化策略。希望这些实践经验能对您有所帮助。 系统架构设计 在深入代码之前，先来了解一下系统的整体架构。在设计初期，主要考虑了实时性、可扩展性以及与 OpenAI Real-time 协议的兼容性。 基于这些考虑，最终确定的系统架构如下： graph LR A[\"客户端 (Web/App)\"] -- \"WebSocket\" --\u003e B[\"AudioService\"]; B -- \"音频数据 (PCM), 协议事件 (JSON)\" --\u003e C[\"ModelManager\"]; C -- \"模型推理\" --\u003e B; C -- \"Mimi 模型 (Encoder/Decoder), LM 模型 (SFT/Base/Pretrain), Tokenizer\" --\u003e D[\"模型权重/配置 (Hugging Face Hub/...)\"]; B -- \"ASR 请求 (音频)\" --\u003e E[\"ASR 服务 (OpenAI Whisper/Deepgram/...)\"]; B -- \"WebSocket, 事件发送 (send_event)\" --\u003e A; style D fill:#f9f,stroke:#333,stroke-width:2px; style E fill:#ccf,stroke:#333,stroke-width:2px; style C fill:#ddf,stroke:#333,stroke-width:2px; style B fill:#eee,stroke:#333,stroke-width:2px; style A fill:#cef,stroke:#333,stroke-width:2px; 架构设计思路: 客户端 (Client): 客户端应用（Web 或 App）负责捕获用户语音，并将音频数据实时推送至音频服务。同时，客户端需要接收并解析服务发出的 OpenAI Real-time 协议事件，最终将语音和文本响应呈现给用户。 音频服务 (AudioService): 音频服务是系统的核心，主要职责包括： 会话管理: 管理客户端会话的生命周期。 WebSocket 通信: 建立和维护与客户端的 WebSocket 连接，接收音频数据和协议事件，并推送事件。 音频处理管线: 构建音频处理流程，包括缓存、分帧、预处理等。 模型推理协调: 调用模型管理器 (ModelManager) 进行模型推理，并接收推理结果。 ASR 集成 (可选): 集成 ASR 服务 (OpenAI Whisper, Deepgram, Gemini) 以支持语音转写功能。 OpenAI Real-time 协议实现: 遵循 OpenAI Real-time 协议，封装事件生成和发送逻辑。 模型管理器 (ModelManager): 模型管理器负责模型资源的加载、管理和推理，其核心职责包括： 模型加载与缓存: 加载 Mimi 模型、LM 模型、Tokenizer 等模型组件并进行缓存。 模型推理接口: 提供 process_audio 接口，简化模型推理调用。 模型预热 (Warmup): 实现模型预热机制，降低首次推理延迟。 并发控制: 实现并发控制，确保模型在并发访问下的安全性和性能。 ASR 服务 (可选): 可选集成第三方 ASR 服务，提供语音转写能力。 模型权重/配置: 模型权重文件和配置信息存储在云端 (Hugging Face Hub) 或本地文件系统。 核心组件实现：代码实践 接下来，将分享 ModelManager 和 AudioService 这两个核心组件的代码实现。 ","date":"2025-01-23","objectID":"/end-to-end-audio-service/:0:0","tags":["openai realtime","moshi","audio"],"title":"实时语音服务构建实践：基于 OpenAI Real-time 协议的技术分享","uri":"/end-to-end-audio-service/"},{"categories":["ai","实现","service"],"content":"ModelManager：模型引擎与资源管理 ModelManager 类在系统中扮演模型引擎和资源管理的角色。为了高效管理模型资源，采用了单例模式来实现 ModelManager 类。 单例模式实现：资源唯一管理 @dataclass class ModelManager: _instance = None _initialized = False # ... 模型组件 ... def __new__(cls, *args, **kwargs): if not cls._instance: cls._instance = super().__new__(cls) return cls._instance @classmethod def get_instance(cls, hf_repo: str = loaders.DEFAULT_REPO, device: str = \"cuda\"): if not cls._instance or not cls._initialized: instance = cls.from_pretrained(hf_repo, device) cls._instance = instance cls._initialized = True logger.info(\"Model manager initialized, starting warmup...\") cls._instance.warmup() logger.info(\"Model warmup completed\") return cls._instance 模型加载与管理：Hugging Face Hub 集成 ModelManager 的 from_pretrained 类方法负责加载模型组件。 使用 huggingface_hub 库从 Hugging Face Hub 下载预训练权重。 关键代码 (模型加载)： @classmethod def from_pretrained(cls, hf_repo: str = loaders.DEFAULT_REPO, device: str = \"cuda\", model_path: str = \"\"): # ... 初始化锁 ... # 加载模型组件 logger.info(\"loading mimi\") mimi_weight = hf_hub_download(hf_repo, loaders.MIMI_NAME) mimi = loaders.get_mimi(mimi_weight, device) logger.info(\"mimi loaded\") # Load SFT model sft_model_path = os.getenv(\"SFT_MODEL\") if not sft_model_path: raise ValueError(\"SFT_MODEL not found\") logger.info(f\"loading sft model {sft_model_path}\") sft_lm = _get_moshi_lm(sft_model_path, device) logger.info(\"sft model loaded\") # Load BASE and PRETRAIN models if ENV is set # ... (加载 BASE 和 PRETRAIN 模型，如果环境变量设置) ... tokenizer_path = hf_hub_download(hf_repo, loaders.TEXT_TOKENIZER_NAME) text_tokenizer = sentencepiece.SentencePieceProcessor(tokenizer_path) sft_lm_gen = LMGen(sft_lm) # 设置持续推理模式 mimi.streaming_forever(1) sft_lm_gen.streaming_forever(1) # ... (其他初始化) ... process_audio 方法：核心推理流程 process_audio 方法是 ModelManager 的核心，负责接收音频 chunk，执行模型推理，并返回文本和音频数据。 关键代码逻辑 (process_audio): 音频编码: 使用 mimi.encode(chunk) 编码音频 chunk。 选择 LM 模型: 根据 model_type 选择 LM 模型生成器。 循环推理 (Streaming): 循环遍历 codes，调用 LM 模型的 step 方法进行流式推理。 音频解码 (如果生成 tokens): 使用 mimi.decode(tokens[:, 1:]) 解码 tokens 为音频 PCM 数据。 文本 token 转换 (如果生成 tokens): 使用 text_tokenizer.id_to_piece 将 token ID 转换为文本片段。 返回结果: 返回文本消息和音频 PCM 数据。 代码片段 (process_audio 核心逻辑): @torch.inference_mode() def process_audio(self, chunk: torch.Tensor, model_type :str = \"sft\") -\u003e tuple[Optional[str], Optional[np.ndarray]]: \"\"\"处理音频并生成响应\"\"\" codes = self.mimi.encode(chunk) text_msg = None audio_pcm = None for c in range(codes.shape[-1]): tokens = self._get_lm_gen(model_type).step(codes[:, :, c : c + 1]) # 模型推理 if tokens is None: continue pcm = self.mimi.decode(tokens[:, 1:]) # 音频解码 audio_pcm = pcm.cpu()[0, 0].numpy() text_token = tokens[0, 0, 0].item() if text_token not in (0, 3): text = self.text_tokenizer.id_to_piece(text_token) # token -\u003e 文本 text = text.replace(\" \", \" \") text_msg = text return text_msg, audio_pcm 并发控制与模型预热：性能提升 asyncio.Lock: 使用 asyncio.Lock 保证并发场景下模型访问安全。 warmup 方法: warmup 方法在初始化后调用，预热模型，降低推理延迟。 ","date":"2025-01-23","objectID":"/end-to-end-audio-service/:0:1","tags":["openai realtime","moshi","audio"],"title":"实时语音服务构建实践：基于 OpenAI Real-time 协议的技术分享","uri":"/end-to-end-audio-service/"},{"categories":["ai","实现","service"],"content":"AudioService：协议实现与会话管理中心 AudioService 负责处理客户端 WebSocket 连接、会话管理、音频处理流程以及 OpenAI Real-time 协议交互。 会话管理与状态维护：SessionState 数据类 使用 self.sessions 字典管理会话状态 (SessionState)。 create_session 方法创建新会话并初始化 SessionState 对象。 SessionState 数据类定义： @dataclass class SessionState: session: Resource.Session conversation: Optional[Resource.Conversation] = None current_response: Optional[Resource.Response] = None audio_state: Optional[AudioState] = None first_sent: bool = False # 是否已经发送过第一个音频响应 item_id: Optional[str] = None # 当前音频响应的item_id # ... 索引计数器 ... def reset_audio(self, sample_rate: int): # ... 重置音频状态和索引 ... WebSocket 事件处理 (handle_client 方法)：事件驱动架构 handle_client 方法处理客户端 WebSocket 连接，监听并处理不同类型的事件。 关键代码逻辑 (handle_client): 创建会话: 调用 self.create_session() 创建会话状态。 发送初始化事件: 发送 session.created 和 conversation.created 事件。 启动音频流处理任务 (handle_audio_stream): 创建异步任务处理音频流。 事件接收循环: 循环监听客户端 WebSocket 消息。 事件解析: 解析 JSON 消息，获取事件类型和数据。 事件分发处理: 根据 event[\"type\"] 执行不同逻辑 (session.update, input_audio_buffer.append, response.cancel 等)。 错误处理和资源清理: 使用 try...except...finally 结构进行错误处理和资源清理。 音频流处理 (handle_audio_stream 方法)：实时响应引擎 handle_audio_stream 方法负责音频数据读取、分帧、模型推理以及实时响应发送。 包含 audio_process_loop 和 transcription_loop 两个异步循环。 关键代码逻辑 (audio_process_loop): 循环读取 PCM 数据: 从 audio_state.pcm_buffer.read_pcm() 读取 PCM 数据。 音频数据累积与分帧: 累积 PCM 数据到 audio_state.all_pcm_data，并分帧处理。 模型推理: 将音频 chunk 转换为 Tensor，调用 self.process_audio_and_generate 进行推理。 音频/文本响应实时发送: 使用 send_event 发送 response.audio.delta 和 response.audio_transcript.delta 事件。 Real-time 协议事件封装: 在首次发送音频响应前，发送 response.output_item.added 和 response.content_part.added 事件。 关键代码逻辑 (transcription_loop): transcription_loop 异步处理音频转写任务，定期检查 audio_state.transcription_buffer 中的音频数据，发送给 ASR 服务转写，并通过 conversation.item.input_audio_transcription.completed 事件发送结果。 Client-Server Event 时序图：协议交互流程 sequenceDiagram participant Client participant Server Note over Client,Server: 会话初始化 Client-\u003e\u003eServer: WebSocket 连接建立 Server-\u003e\u003eClient: event: session.created Server-\u003e\u003eClient: event: conversation.created Note over Client,Server: 音频输入开始 Client-\u003e\u003eServer: event: input_audio_buffer.append (音频 Chunk 1) Server-\u003e\u003eServer: 音频处理 (ModelManager) Server-\u003e\u003eClient: event: response.created Server-\u003e\u003eClient: event: response.output_item.added (音频输出项) Server-\u003e\u003eClient: event: response.content_part.added (音频内容部分) Server-\u003e\u003eClient: event: response.audio.delta (音频 Delta 1) Server-\u003e\u003eClient: event: response.audio_transcript.delta (文本 Delta 1, 可选) Client-\u003e\u003eServer: event: input_audio_buffer.append (音频 Chunk 2) Server-\u003e\u003eServer: 音频处理 (ModelManager) Server-\u003e\u003eClient: event: response.audio.delta (音频 Delta 2) Server-\u003e\u003eClient: event: response.audio_transcript.delta (文本 Delta 2, 可选) Note over Client,Server: (持续音频 Chunk 和 Delta 事件) Note over Client,Server: 音频转写完成 (异步) Server-\u003e\u003eClient: event: conversation.item.input_audio_transcription.completed (转写文本) Note over Client,Server: 会话更新 (可选) Client-\u003e\u003eServer: event: session.update (更新 Session 参数) Server-\u003e\u003eClient: event: session.updated Note over Client,Server: 客户端取消响应 (可选) Client-\u003e\u003eServer: event: response.cancel Server-\u003e\u003eClient: event: response.done (status: cancelled) Note over Client,Server: 会话结束 (隐式 - 连接断开或超时) Client-\u003e\u003eServer: WebSocket 连接关闭 Server-\u003e\u003eServer: 清理会话资源 Note over Server,Client: 错误处理 (Server 侧发生错误) Server-\u003e\u003eClient: event: error (error details) 性能优化：实践经验 性能优化策略包括： GPU 加速： 模型推理 GPU 加速。 异步编程 (asyncio)： AudioService 基于 asyncio 构建，实现高并发和低延迟。 流式推理 (Streaming Inference)： 利用 Mimi 模型和 LM 模型的流式推理特性。 音频分帧处理： 音频流分割成固定大小的帧 (chunk, frame_size) 处理。 模型预热 (Warmup)： 服务启动时进行模型预热。 模型单例模式： ModelManager 单例模式避免重复加载模型。 高效数据结构： 使用 PCM16Buffer 和 TranscriptionBuffer 等高效数据结构。 未来展望：服务进化 未来的改进方向包括： 更强大的 ASR 服务集成： 提升语音转写准确率和鲁棒性。 多轮对话能力： 支持对话上下文管理，实现多轮对话。 模型多样性与灵活配置： 支持更多模型选择和配置选项。 Tool/Function Calling 功能： 集成 Tool/Function Calling 功能。 服务部署与弹性伸缩： 研究服务部署方案和弹性伸缩能力。 安全性加固： 加强服务安全性设计。 总结：构建心得 构建基于 OpenAI Real-time 协议的实时语音服务是一次有益的实践。 通过这篇文章，希望能够分享一些实践经验，帮助开发者了解 OpenAI Real-time 协议，并构建出更优秀的实时语音交互应用。 感谢阅读！ ","date":"2025-01-23","objectID":"/end-to-end-audio-service/:0:2","tags":["openai realtime","moshi","audio"],"title":"实时语音服务构建实践：基于 OpenAI Real-time 协议的技术分享","uri":"/end-to-end-audio-service/"},{"categories":["ai","实现","工程效率"],"content":"介绍一款用于自动生成商品数据解析代码的 Agent，涵盖其设计思路、核心功能、实现细节以及未来展望，旨在帮助开发者提高数据处理效率。","date":"2024-12-27","objectID":"/llm-parser-agent/","tags":["ai agent","代码生成","prompt"],"title":"告别手动写代码：我的商品数据解析器 Agent 设计思路与方案","uri":"/llm-parser-agent/"},{"categories":["ai","实现","工程效率"],"content":"我们经常需要从各种来源抓取和处理商品数据。这些数据格式五花八门，编写解析代码是一项十分繁重但是缺乏技术含量的工作。为了解决这个问题，我设计并实现了一个 Agent，它可以帮助我们自动生成解析商品数据的代码，从而解放生产力。本文将深入介绍这个 Agent 的设计思路和方案。 ","date":"2024-12-27","objectID":"/llm-parser-agent/:0:0","tags":["ai agent","代码生成","prompt"],"title":"告别手动写代码：我的商品数据解析器 Agent 设计思路与方案","uri":"/llm-parser-agent/"},{"categories":["ai","实现","工程效率"],"content":"背景与痛点 手动编写解析代码的痛点显而易见： 重复劳动: 针对不同的数据源，即使结构相似，也需要编写大量的重复代码。这就像不断地重复发明轮子，浪费时间和精力。 容易出错: 手动解析容易出现边界情况考虑不周，导致解析错误。一个小小的疏忽就可能导致数据处理的灾难。 维护困难: 当数据结构发生变化时，需要手动修改大量的解析代码。这就像在一个脆弱的系统中牵一发而动全身。 效率低下: 花费大量时间在编写和调试解析代码上，而不是专注于核心业务逻辑。 宝贵的时间被消耗在了重复性的技术细节上。 因此，我设想了一个能够理解数据结构和解析逻辑的智能助手，它可以根据用户提供的输入和期望的输出，自动生成可靠的解析代码。 ","date":"2024-12-27","objectID":"/llm-parser-agent/:0:1","tags":["ai agent","代码生成","prompt"],"title":"告别手动写代码：我的商品数据解析器 Agent 设计思路与方案","uri":"/llm-parser-agent/"},{"categories":["ai","实现","工程效率"],"content":"整体设计思路 (State Diagram) 我的设计核心理念是构建一个迭代优化的流程，Agent 不仅能生成代码，还能通过验证和反馈不断改进其生成能力。 下面的状态图描绘了 Agent 的主要生命周期： 这个状态图反映了以下关键的设计思想： 清晰的初始化阶段: Init 阶段至关重要，它定义了后续操作的基础。通过明确解析器名称、类型和输入文件，Agent 能够理解用户的意图。 验证驱动的开发: Validate 阶段是循环的核心。通过执行和检查结果，我们能够发现问题并驱动后续的优化。 自动化与人工干预相结合: Auto Update 尝试自动解决问题，而 Manual Improve 则允许用户进行更精细的控制。这种混合模式能够应对不同程度的解析挑战。 迭代改进: Agent 不断地在 Validate、Auto Update 和 Manual Improve 之间循环，直到获得满意的解析结果。 ","date":"2024-12-27","objectID":"/llm-parser-agent/:0:2","tags":["ai agent","代码生成","prompt"],"title":"告别手动写代码：我的商品数据解析器 Agent 设计思路与方案","uri":"/llm-parser-agent/"},{"categories":["ai","实现","工程效率"],"content":"详细的功能流程 (Flowchart) 为了更深入地了解 Agent 的具体工作流程，我设计了如下的流程图，详细描述了用户与 Agent 交互的各个命令及其执行过程： 这个流程图更详细地展示了 Agent 的内部运作机制和用户交互方式： init 命令的灵活性: 允许用户通过命令行参数快速初始化，也支持交互式输入，照顾不同用户的习惯。参数验证确保了初始化的正确性。 validate 命令的智能化: 能够根据上下文判断是否需要用户提供额外信息，并能根据验证结果智能地生成改进提示 (prompt)。 update 命令的多样性: 提供了多种更新解析器的方式，包括使用历史 prompt、交互式输入和命令行参数，满足不同的更新需求。 improve 命令的持久化: 允许用户保存有效的改进 prompt，以便后续复用，提升效率。 clear 命令的便捷性: 方便用户在调试过程中快速重置状态，避免历史信息干扰。 核心模块与实现细节 为了实现上述设计，我构建了几个核心模块，它们协同工作以完成解析代码的生成和优化： ParserGenerator: 这是生成解析器代码的核心模块。它的实现依赖于以下技术： 大型语言模型 (LLMs): 利用 LLMs 的代码生成能力，根据用户提供的输入文件和目标 Schema 生成代码。Prompt 工程在此处至关重要，需要设计有效的 Prompt 指导 LLM 生成高质量的代码。 混合方法: 根据不同的解析器类型， 需要精心设计不同的的 Prompt。 Validator: 负责执行生成的解析器并验证其输出结果。它需要能够： 动态执行代码: 在安全的环境中执行生成的解析器代码。 Schema 比对: 将解析结果与预期的 Schema 进行比对，找出缺失、类型不匹配或格式错误的字段。 错误分析: 提供详细的错误信息，帮助用户理解问题所在。 PromptGenerator: 负责根据验证结果生成改进的 Prompt。其核心功能是： 识别错误模式: 分析验证结果中的错误信息，识别常见的解析错误模式。 生成针对性 Prompt: 根据识别出的错误模式，生成能够指导 ParserGenerator 进行改进的 Prompt。例如，如果发现某个字段缺失，Prompt 可以指示 ParserGenerator 提取该字段。 考虑上下文信息: 在生成 Prompt 时，需要考虑之前的 Prompt 和对话历史，避免重复或矛盾的指令。 AgentShell: 提供用户交互的界面。它可以是一个命令行界面 (CLI)，也可以是一个图形用户界面 (GUI)。CLI 的优点是轻量级和易于自动化，GUI 则更易于上手和操作。我目前选择了 CLI，因为它更适合我的技术背景和目标用户群体。 ","date":"2024-12-27","objectID":"/llm-parser-agent/:0:3","tags":["ai agent","代码生成","prompt"],"title":"告别手动写代码：我的商品数据解析器 Agent 设计思路与方案","uri":"/llm-parser-agent/"},{"categories":["ai","实现","工程效率"],"content":"未来展望 随着模型能力的不断增强，人工介入的成本会越来越低。目前我们解析一个中等复杂度的站点从代码生成到验证完成需要 1-2 小时，我相信半年之内这个时间可以缩短到半小时以内。 当人工介入降低到一个很小的范围时，我们会把整个流程平台化，在平台上操作即可完成代码的生成，校验和发布。 ","date":"2024-12-27","objectID":"/llm-parser-agent/:0:4","tags":["ai agent","代码生成","prompt"],"title":"告别手动写代码：我的商品数据解析器 Agent 设计思路与方案","uri":"/llm-parser-agent/"},{"categories":["ai","实现","工程效率"],"content":"总结 通过这个商品数据解析器 Agent 的设计和实现，我希望能够帮助开发者摆脱繁琐的数据解析编码工作，将更多精力投入到更有价值的业务逻辑和数据采集中。这是一个不断迭代和完善的过程，目前至少帮助我们提效了 3 倍，我期待着在未来的工作中继续改进和扩展这个 Agent，使其能够更好地服务于数据处理的需求。 在实现这个 agent 后，正好 anthropic 发表了一遍文章，文章的内容我深以为然，我提炼出两点自己的切身体会 设计要简介，能实现功能即可，不要依赖框架（我不使用框架） 提供给 LLM 的工具函数和 Prompt 要精心设计，测试完善 ","date":"2024-12-27","objectID":"/llm-parser-agent/:0:5","tags":["ai agent","代码生成","prompt"],"title":"告别手动写代码：我的商品数据解析器 Agent 设计思路与方案","uri":"/llm-parser-agent/"},{"categories":["service","实现","优化"],"content":"自定义 Wordpress WP_HOME WP_SITEURL 的方案","date":"2024-10-23","objectID":"/wordpress-url/","tags":["wordpress","seo","方案设计"],"title":"自定义 Wordpress URL 的方案","uri":"/wordpress-url/"},{"categories":["service","实现","优化"],"content":"WordPress 是全球最流行的内容管理系统(CMS),截至2024年,已有超过43%的网站使用WordPress构建。作为一个开源项目,WordPress 始于2003年,从一个简单的博客平台发展成为一个功能强大的网站管理系统。它具有以下特点: 灵活的URL结构: WordPress 支持多种URL结构配置,包括普通链接和永久链接(Permalinks) 多站点支持: 可以在同一个WordPress安装中管理多个网站 目录灵活性: 支持安装在网站根目录或子目录中 可扩展性: 通过主题和插件系统,可以实现各种定制化需求 ","date":"2024-10-23","objectID":"/wordpress-url/:0:0","tags":["wordpress","seo","方案设计"],"title":"自定义 Wordpress URL 的方案","uri":"/wordpress-url/"},{"categories":["service","实现","优化"],"content":"Wordpress 两个关于 url 的重要概念 ","date":"2024-10-23","objectID":"/wordpress-url/:1:0","tags":["wordpress","seo","方案设计"],"title":"自定义 Wordpress URL 的方案","uri":"/wordpress-url/"},{"categories":["service","实现","优化"],"content":"WP_HOME 定义: 网站的公开访问地址，即用户在浏览器中访问您网站时使用的URL 用途: 决定了网站前台的访问路径 影响: 影响所有前台页面的URL生成，包括文章链接、分类页面等 ","date":"2024-10-23","objectID":"/wordpress-url/:1:1","tags":["wordpress","seo","方案设计"],"title":"自定义 Wordpress URL 的方案","uri":"/wordpress-url/"},{"categories":["service","实现","优化"],"content":"WP_SITEURL 定义: WordPress核心文件所在的地址 用途: 指定WordPress系统文件的位置 一个场景 abc.com 是某公司的的主站，他们部署了一套 wordpress 生产一些内容。但是 wordpress 不可能和主站的前端部署到一起，就意味着 wordpress 可能是通过域名 def.com 来暴露。 这里 wordpress 的内容是为主站 SEO 服务的，所以必须让搜索引擎认为 wordpress 也是直接部署在 abc.com 下。 最直接的想法就是 abc.com/post 转发到 def.com 对于其他站点或者服务这是可行的，但是对于 wordpress 的 blog 会在静态页面里包含较多 def.com 也就是上面 WP_HOME 的信息。这样会严重影响 SEO。 解决方案 ","date":"2024-10-23","objectID":"/wordpress-url/:1:2","tags":["wordpress","seo","方案设计"],"title":"自定义 Wordpress URL 的方案","uri":"/wordpress-url/"},{"categories":["service","实现","优化"],"content":"1. 域名转发 abc.com 网关这一层把 /post 前缀的请求转发到 k8s 集群的 Loadbalancer IP(假设 wordpress 部署在 k8s 中)而不是 redirect 到 def.com ","date":"2024-10-23","objectID":"/wordpress-url/:1:3","tags":["wordpress","seo","方案设计"],"title":"自定义 Wordpress URL 的方案","uri":"/wordpress-url/"},{"categories":["service","实现","优化"],"content":"2. k8s ingress 配置 在 k8s 中我们需要配置 ingress rule 把接受 host 为 abc.com 转发到 wordpress 如下配置 - host: abc.com http: paths: - backend: service: name: wordpress-1-wordpress-svc port: number: 80 pathType: ImplementationSpecific ","date":"2024-10-23","objectID":"/wordpress-url/:1:4","tags":["wordpress","seo","方案设计"],"title":"自定义 Wordpress URL 的方案","uri":"/wordpress-url/"},{"categories":["service","实现","优化"],"content":"3. WP_HOME 和 WP_SITEURL 定义 修改 wordpress 配置文件把默认配置 define('WP_SITEURL', 'http://' . $_SERVER['HTTP_HOST'] . '/'); define('WP_HOME', 'http://' . $_SERVER['HTTP_HOST'] . '/'); 修改为 define('WP_HOME', 'https://abc.com/devpost'); define('WP_SITEURL', 'https://abc.com/devpost'); 默认情况下 wordpress 的 apache 理由规则是从根目录 / 下加载静态资源，到这里会有两个明显问题 静态资源404: CSS、JavaScript等资源文件路径不正确 重定向循环: 不当的URL配置导致的无限重定向 所以还必须调整 wordpress 的部署目录和转发规则 ","date":"2024-10-23","objectID":"/wordpress-url/:1:5","tags":["wordpress","seo","方案设计"],"title":"自定义 Wordpress URL 的方案","uri":"/wordpress-url/"},{"categories":["service","实现","优化"],"content":"4. 定义部署目录 volumeMounts: - mountPath: /var/www/html/post name: wordpress-1-wordpress-pvc subPath: wp ","date":"2024-10-23","objectID":"/wordpress-url/:1:6","tags":["wordpress","seo","方案设计"],"title":"自定义 Wordpress URL 的方案","uri":"/wordpress-url/"},{"categories":["service","实现","优化"],"content":"5. 修改 apache 转发规则 修改 .htaccess 如下 # BEGIN WordPress \u003cIfModule mod_rewrite.c\u003e RewriteEngine On RewriteBase /devpost/ RewriteRule ^index\\.php$ - [L] RewriteCond %{REQUEST_FILENAME} !-f RewriteCond %{REQUEST_FILENAME} !-d # Allow Apache mod_status RewriteCond %{REQUEST_URI} !=/server-status RewriteRule . /devpost/index.php [L] \u003c/IfModule\u003e # END WordPress ","date":"2024-10-23","objectID":"/wordpress-url/:1:7","tags":["wordpress","seo","方案设计"],"title":"自定义 Wordpress URL 的方案","uri":"/wordpress-url/"},{"categories":["service","实现","infra"],"content":" AB test 基础设施设计和搭建","date":"2024-09-06","objectID":"/ab-test-design/","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":" 作者注：文章的结构是我让 LLM 参考我的设计简稿生成的 ","date":"2024-09-06","objectID":"/ab-test-design/:0:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"引言 AB Test 是现代软件开发和产品管理中的一项关键技术。它允许团队通过比较网页或应用功能的两个或多个版本，来确定哪个表现更好，从而做出数据驱动的决策。在这篇文章中，介绍一下我们如何快速设计一个低成本和扩展性高的 AB Test 基础设施。 ","date":"2024-09-06","objectID":"/ab-test-design/:1:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"框架概述 我们的AB测试框架设计注重简洁性和可扩展性。它由四个主要组件组成： 创建实验规则 基于规则的流量路由 用户分段和请求头注入 用于流量分发的API网关 让我们详细探讨每个组件。 ","date":"2024-09-06","objectID":"/ab-test-design/:2:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"1. 创建实验规则 我们AB测试框架的基础在于创建清晰灵活的实验规则。这些规则定义了每个测试的参数，包括： Project（项目名称，即实验标识符） Router（路由器，即分段或分桶名称） Weight（权重，即流量分配） Service（目标服务名称） URI（目标端点） Rewrite URI（可选的修改后端点） 为了创建这些规则，我们实现了一个简单的API端点，接受JSON格式的数据。以下是创建规则的示例： curl --location '127.0.0.1:8081/internal/ab/config' \\ --header 'Content-Type: application/json' \\ --data '{ \"project\": \"exp_discover_for_you_tab\", \"router\": \"ad_line\", \"weight\": 50, \"uri\": \"/recmd/homepage\", \"rewrite_uri\": \"/deal/homepage_recommendation_v2\", \"service\": \"kaka-recmd-service.kaka.svc.clusterset.local\" }' 这种灵活性使产品经理和开发人员能够轻松设置和修改实验，而无需更改底层代码。 ","date":"2024-09-06","objectID":"/ab-test-design/:3:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"2. 基于规则的流量路由 一旦规则设置完成，我们的框架就会创建流量路由配置。这些配置决定了如何在不同版本或正在测试的功能之间分配用户流量。 路由机制考虑以下因素： 用户参与的实验（project） 用户所属的分段（router） 分配给每个变体的权重（weight） 这确保了流量按照预定义的比例进行分割，从而可以准确比较不同版本之间的表现。 ","date":"2024-09-06","objectID":"/ab-test-design/:4:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"3. 用户分段和请求头注入 AB测试的一个关键方面是一致的用户分段。我们的框架通过以下方式处理这一问题： 识别用户（通过cookies、用户ID或其他方式） 确定用户有资格参与哪些实验 为每个相关实验将用户分配到特定的分段 将这些信息注入到请求头中 具体而言，我们向每个Web请求添加一个自定义头部 Kaka-Ex，其中包含该用户的路由信息。这个头部可能看起来像这样： kaka-Ex: exp_discover_for_you_tab=ad_line 这种方法确保用户在多次访问中始终看到相同的变体，这对于测试结果的有效性至关重要。 ","date":"2024-09-06","objectID":"/ab-test-design/:5:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"4. 用于流量分发的API网关 我们AB测试框架的最后一个部分是API网关，它充当流量警察的角色，根据实验规则和用户分段将请求定向到适当的服务或端点。 API网关的工作流程： 读取来自传入请求的 Kaka-Ex 头部 将头部信息与路由规则匹配 将请求转发到适当的服务和URI 如果规则中指定了，则可选地重写URI 这种设置允许在测试中具有极大的灵活性，因为您可以将流量路由到完全不同的服务，或同一服务中的不同端点，而无需更改前端代码。 ","date":"2024-09-06","objectID":"/ab-test-design/:6:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"实现细节 ","date":"2024-09-06","objectID":"/ab-test-design/:7:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"用于获取头部的中间件 为了简化获取分段信息的过程，我们计划实现一个中间件，自动检索和处理 kaka-Ex 头部。这将使开发人员更容易在其应用程序代码中访问AB测试信息。 ","date":"2024-09-06","objectID":"/ab-test-design/:7:1","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"日志记录 我们的框架会自动记录每个请求的AB测试信息。这包括： 用户参与的实验 分配给他们的分段 他们看到的变体 这些数据对于分析AB测试结果和基于用户行为做出明智决策至关重要。 ","date":"2024-09-06","objectID":"/ab-test-design/:7:2","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"结论 这个AB测试框架为在应用程序中实现分割测试提供了一个强大、灵活且可扩展的解决方案。通过分离实验定义、用户分段和流量路由的关注点，我们创建了一个可以适应各种测试场景的系统。 该框架的主要优势包括： 通过简单的API轻松设置实验 跨会话的一致用户体验 灵活的流量路由，包括服务级别的分割 在实施这个框架时，请记住AB测试的真正价值不仅在于技术实现，还在于提出正确的问题、形成清晰的假设，以及仔细分析结果以推动产品的有意义改进。 ","date":"2024-09-06","objectID":"/ab-test-design/:8:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"下一步 虽然这个框架为AB测试提供了坚实的基础，但总有改进的空间。未来可能的增强领域包括： 自动日志记录，便于分析 用于创建和管理实验的用户界面 实时监控和可视化测试结果 ","date":"2024-09-06","objectID":"/ab-test-design/:9:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["service","实现","infra"],"content":"API Gateway 这里我使用的 APISIX 部署方式 helm repo add apisix https://charts.apiseven.com helm repo update helm install apisix apisix/apisix -n apisix --create-namespace -f values.yaml 推荐必须修改的配置如下 # replicaCount: 1 # production 环境 replicaCount: 2 # 自定义 storage class 名称 etcd: persistence: storageClass: standard-rwo size: 25Gi apisix: # 自定义 admin API key admin: credentials: admin: xxxx viewer: view # 自定义 dashboard 账号密码 dashboard: enabled: true config: authentication: users: - username: xxx password: xxx image: tag: 3.0.1-alpine global: storageClass: standard-rwo ","date":"2024-09-06","objectID":"/ab-test-design/:10:0","tags":["AB Test","流量管理","方案设计"],"title":"AB Test 框架方案设计","uri":"/ab-test-design/"},{"categories":["数据库","存储","原理"],"content":"对以 Mongodb 默认存储引擎 WiredTiger 数据结构的探究","date":"2024-07-08","objectID":"/wiredtiger/","tags":["WiredTiger","Mongodb","B-tree","LSM-tree"],"title":"Mongodb 默认存储引擎 WiredTiger 的数据结构探究","uri":"/wiredtiger/"},{"categories":["数据库","存储","原理"],"content":"我们知道 Mongodb 默认使用的存储引擎是 WiredTiger。了解存储引擎的数据结构是十分重要的尤其是对于服务端的同学，只有你了解了它才能更好的使用 Mongodb。关于 Mongodb 我们在这里不做介绍，我们主要四回答以下两个问题才搞清楚 WiredTiger 的数据存储结构。 问题一 B-tree 还是 LSM tree ? ","date":"2024-07-08","objectID":"/wiredtiger/:0:0","tags":["WiredTiger","Mongodb","B-tree","LSM-tree"],"title":"Mongodb 默认存储引擎 WiredTiger 的数据结构探究","uri":"/wiredtiger/"},{"categories":["数据库","存储","原理"],"content":"LSM tree MongoDB 中的 LSM (Log-Structured Merge-tree) 是 WiredTiger 存储引擎提供的一种索引数据结构选项。它主要有这些特性： 内存写入缓冲: 新的写入操作首先被添加到内存中的一个称为memtable的数据结构中。 memtable通常使用平衡树(如红黑树或AVL树)实现,以保持键的有序性。 2， 顺序写入磁盘: 当 memtable 达到一定大小阈值时,其内容会被刷新到磁盘上,形成一个不可变的 SSTable (Sorted String Table)文件。 这个过程是异步进行的,不会阻塞新的写入请求。 多层结构: LSM树通常维护多层SSTable文件。 较新的数据在较高层(如Level 0),较旧的数据在较低层。 每一层的大小通常比上一层大几倍。 读取操作: s 读取时,首先检查 memtable。 如果 memtable 中没有找到,则按从新到旧的顺序检查SSTable文件。 可能使用布隆过滤器来快速确定一个key是否可能存在于某个SSTable中。 合并和压缩: 后台进行定期的合并和压缩操作,将多个小的SSTable合并成更大的SSTable。 这个过程会删除过时的数据,保留最新的值。 写放大: LSM树的设计会导致写放大,即实际写入磁盘的数据量大于用户请求写入的数据量。 这是因为同一条数据可能被多次写入不同的SSTable。 空间放大: 由于存在多个版本的数据和未压缩的文件,LSM树可能会导致一定程度的空间放大。 读放大: 读取操作可能需要检查多个 SSTable,导致读放大。 这可以通过布隆过滤器和缓存等技术来缓解。 配置选项: MongoDB允许用户通过命令行参数来选择使用LSM索引,并配置相关参数。 性能特点: LSM树在写入密集的场景下表现优异,特别是对于随机插入操作。 读取性能,尤其是点查询,可能不如B树索引。 ","date":"2024-07-08","objectID":"/wiredtiger/:1:0","tags":["WiredTiger","Mongodb","B-tree","LSM-tree"],"title":"Mongodb 默认存储引擎 WiredTiger 的数据结构探究","uri":"/wiredtiger/"},{"categories":["数据库","存储","原理"],"content":"B-tree B-tree 是 Mongodb 中默认使用的数据结构。特性包括这些: 树形结构: B-tree是一种自平衡的树形数据结构。 它由根节点、内部节点和叶子节点组成。 每个节点可以包含多个键值对和指向子节点的指针。 节点组成: 每个节点(称为bucket)有固定大小,通常为8192字节。 节点包含keynode和keydata两部分。 keynode存储固定大小的结构,包含指向子节点的指针和文档位置。 keydata存储可变长度的BSON格式键值。 数据排序: 键值对在节点内按键的顺序排列。 这种排序允许快速的查找、范围查询和排序操作。 查询过程: 从根节点开始,根据键值比较逐层向下遍历。 通常只需3-4次I/O操作就能定位到目标文档。 插入和分裂: 当节点满时,会发生分裂操作。 分裂会创建新节点,并将一半数据移到新节点中。 这个过程可能会一直传播到根节点,保持树的平衡。 性能特点: 提供稳定的查询性能,最坏情况下复杂度为O(log n)。 适合大规模数据集和磁盘存储系统。 灵活性: 支持精确匹配、范围查询和排序操作。 可以创建复合索引,包含多个字段。 优化技术: 使用前缀压缩等技术来减少索引大小和提高效率。 ","date":"2024-07-08","objectID":"/wiredtiger/:2:0","tags":["WiredTiger","Mongodb","B-tree","LSM-tree"],"title":"Mongodb 默认存储引擎 WiredTiger 的数据结构探究","uri":"/wiredtiger/"},{"categories":["数据库","存储","原理"],"content":"为什么 MongoDB 默认使用 B-tree 作为其数据存储机制？ B-tree 能够在读取和写入操作之间提供良好的平衡。 B-tree 结构支持高效的数据检索和范围查询，因为数据是有序存储的，这使得对连续键值范围的查询变得快速和直接。此外， B-tree的设计允许在不影响整体性能的情况下进行高效的插入和更新操作，这对于维护数据的一致性和完整性至关重要。 B-tree 的这些特性使其成为在需要频繁读写操作的场景中的一个理想选择。MongoDB 作为一个通用的 NoSQL 数据库，需要能够处理各种各样的工作负载，包括但不限于实时分析、内容管理和移动应用程序后端。在这些应用中，平衡读写性能对于确保系统响应性和用户体验至关重要。 此外， B-tree的结构允许 MongoDB 实现强大的事务支持，这对于需要高度数据一致性和原子性操作的应用来说是非常重要的。通过使用 B-tree，MongoDB 能够确保事务的原子性、一致性、隔离性和持久性（ACID 属性）。 总的来说，MongoDB 选择 B-tree作为默认存储机制，是为了在广泛的应用场景中提供平衡的读写性能、高效的查询能力和强大的事务支持。 问题二 B-tree 还是 B+ tree 有不少文档有的说 Mongodb 使用的是 B-tree, 有的说是 B+ tree。 这个问题让我也很郁闷，在这里我们追根溯源一下。这篇帖子相信能回答。 在网页内容中，用户 Ars Roseregen 表达了对 WiredTiger 索引数据结构的疑惑。他认为 WiredTiger 使用的是 B-Tree，直到他查阅了一些文档，这些文档称 WiredTiger 使用的是 B-Tree（具体指的是 B+Tree）。然而，当他检查 WiredTiger 的源代码时，没有找到叶节点之间的指针，这是 B+Tree 的一个典型特征。这个发现让他感到困惑，因为另一份文档又提到 WiredTiger 使用的是 B-Tree。 WiredTiger 团队成员 Keith Smith 回应了这个问题，解释说他们称为 “B-Trees”，但这种称呼可能并不精确。他指出，WiredTiger 的设计包含了一些不符合传统 B-Tree 或 B+Tree 定义的优化。WiredTiger 的 B-Trees 存储了所有的键和值在叶子页面中，这一点与 B+Trees 相似。但是，WiredTiger 没有提供从一个叶子页面直接到另一个叶子页面的链接，因为它总是将更新后的页面写入到文件的新位置。WiredTiger 通过父页面来移动到下一个叶子页面，这种方法因为缓存机制而高效。最后，Keith Smith 引用了 Douglas Comer 的论文，指出即使在 B+Trees 中，叶子节点的链接也不是一个严格的要求。 ","date":"2024-07-08","objectID":"/wiredtiger/:2:1","tags":["WiredTiger","Mongodb","B-tree","LSM-tree"],"title":"Mongodb 默认存储引擎 WiredTiger 的数据结构探究","uri":"/wiredtiger/"},{"categories":["数据库","存储","原理"],"content":"怎么理解 WiredTiger moves from one leaf page to the next by going back through the parent page. WiredTiger 如何通过父页面移动到下一个叶子页面 在 WiredTiger 的索引结构中，当需要从一个叶子页面移动到另一个叶子页面时，它并不直接链接到下一个叶子页面。相反，它采用了一种通过父页面来实现这种移动的机制。这意味着： 树状结构的特点：WiredTiger 的索引结构类似于树形结构，其中每个节点都有一个或多个子节点。叶子页面位于树的底部，包含了实际的数据。 无直接链接：与 B+Tree 不同，WiredTiger 的叶子页面之间并没有直接的链接指针，通常 B+Tree 的叶子页面会有一个指向下一个叶子页面的指针，以便顺序遍历。 通过父页面导航：当需要访问当前叶子页面之后的叶子页面时，WiredTiger 会首先返回到它们共同的父页面。父页面包含了指向其子叶子页面的指针。 高效的缓存策略：WiredTiger 的设计利用了缓存策略，通常在访问叶子页面时，相关的父页面也会被缓存。这样，当需要通过父页面移动到下一个叶子页面时，可以直接从缓存中获取父页面的信息，从而提高效率。 文件写入优化：WiredTiger 在更新页面时，通常会将更新后的页面写入到文件的新位置。这种做法避免了在原有位置进行更新，从而减少了碎片化，并且允许在不同的文件版本之间进行快速切换。 灵活性和性能：通过这种设计，WiredTiger 能够在保持高性能的同时，提供更大的灵活性。它可以更好地管理磁盘空间，并且在处理大量数据时，能够更有效地进行页面的读写操作。 总结来说，WiredTiger 通过父页面来移动到下一个叶子页面的设计，是一种优化的策略，它利用了缓存机制，以提高数据访问的效率和性能。这种设计虽然与传统的 B-Tree 或 B+Tree 有所不同，但它为数据库的索引结构提供了更多的灵活性和优化空间。 ","date":"2024-07-08","objectID":"/wiredtiger/:3:0","tags":["WiredTiger","Mongodb","B-tree","LSM-tree"],"title":"Mongodb 默认存储引擎 WiredTiger 的数据结构探究","uri":"/wiredtiger/"},{"categories":["AI","“实现\"","系统"],"content":"如何快速利用 twilio 的 js sdk 构建 webrtc 语音聊天","date":"2024-05-29","objectID":"/twilio-web/","tags":["webRTC","实时语音","twilio"],"title":"利用 Twilio 快速构建 web 语音聊天","uri":"/twilio-web/"},{"categories":["AI","“实现\"","系统"],"content":"web 应用程序越来越强调交互性和实时通信功能。无论是在线客服、网络会议,还是网络电话等,都需要将语音和视频通信无缝集成到应用程序中,以提供更丰富的用户体验。然而,构建自己的实时通信基础设施是一项艰巨的任务,需要处理复杂的信令、编解码、NAT/防火墙遍历等问题。 这就是 Twilio 的用武之地。Twilio 作为一个云通信平台,为我们屏蔽了通信基础设施的复杂性,通过强大而简单的 API, 我们可以比较容器的在应用中集成语音、视频、消息等通信功能。Twilio 拥有遍布全球的通信基础设施,确保高质量的实时通信体验。 其中 Twilio JS Voice SDK 专门面向 Web 应用,利用 WebRTC 技术,让我们可以在浏览器中直接实现语音通话功能,无需用户安装任何插件或应用程序。该SDK提供了丰富的 API,支持拨打、接听、保持、静音等基本通话控制,还支持高级功能如录制、监控等。 通过Twilio JS Voice SDK, 我们可以轻松构建网络电话、在线会议、网上客服等富有互动性的实时通信应用。不仅节省了开发成本,还能为用户带来流畅的通信体验。 我们在构建实时语音聊天机器人，希望能把这里面 web 实时语音构建的实践方案分享出来。这对于任何希望在 web 应用中集成语音通信的开发者来说,都是一个很好的入门和参考。 ","date":"2024-05-29","objectID":"/twilio-web/:0:0","tags":["webRTC","实时语音","twilio"],"title":"利用 Twilio 快速构建 web 语音聊天","uri":"/twilio-web/"},{"categories":["AI","“实现\"","系统"],"content":"原理 Twilio Voice JavaScript SDK 允许在网页浏览器和 Twilio TwiML 语音应用程序之间建立语音通话连接。以下是该 SDK 运作原理的说明： 1. 设备初始化与连接 开发者需要使用 Twilio.Device 对象来设置用户的设备，并建立与 Twilio 的连接。这个过程涉及到从用户的设备（如电脑或移动设备）捕获麦克风音频，并将其发送到 Twilio。同时，从 Twilio 返回的音频会通过设备的扬声器播放出来，类似于普通电话通话的过程。 2. 呼叫控制 与使用传统电话不同，使用 Twilio.Device 发起的呼叫不是直接拨打到另一台电话的号码，而是连接到 Twilio 服务器，并指示 Twilio 从开发者的服务器获取 TwiML（Twilio Markup Language）来处理呼叫的逻辑。这与 Twilio 处理来自真实电话的来电类似。 3. TwiML 应用程序 TwiML 是一套 XML 指令，用于控制 Twilio 服务器如何处理通信。由于 Twilio.Device 发起的呼叫没有特定的电话号码目标，Twilio 依赖于开发者账户中的 TwiML 应用程序来确定如何与服务器交互。TwiML 应用程序包含了一组 URL，这些 URL 会在呼叫时被 Twilio 请求，以获取如何处理呼叫的指令。这种方式允许开发者灵活地控制呼叫的流程，而不需要将逻辑绑定到特定的电话号码。 4. 呼叫流程 当 Twilio.Device 发起呼叫时，Twilio 会向账户中的应用程序的 VoiceUrl 发送请求。开发者通过一个访问令牌（Access Token）来指定要连接的 TwiML 应用程序。Twilio 根据该应用程序的 VoiceUrl 返回的 TwiML 响应来指导呼叫的进一步操作。 ","date":"2024-05-29","objectID":"/twilio-web/:0:1","tags":["webRTC","实时语音","twilio"],"title":"利用 Twilio 快速构建 web 语音聊天","uri":"/twilio-web/"},{"categories":["AI","“实现\"","系统"],"content":"实践 创建 TwiML app 明白了原理后。我们首先要在 Twilio 后台创建一个 application, 这个应用关键的配置是要配置一个我们自己服务器的 url。这个 url 就是当 web 发起连接后，twilio 会回调这个 url 获取后续处理的指令。 提供 access token web 应用需要服务端提供 access token 来与 Twilio 交互 def token(): \"\"\" generate JWT token for js voice sdk \"\"\" # Generate a random user name and store it alphanumeric_only = re.compile(r\"[\\W_]+\") identity = alphanumeric_only.sub(\"\", faker.user_name()) # Create access token with credentials token = AccessToken( SETTINGS.TWILIO_ACCOUNT_SID, SETTINGS.TWILIO_API_KEY, SETTINGS.TWILIO_API_SECRET, identity=identity ) # Create a Voice grant and add to token voice_grant = VoiceGrant( outgoing_application_sid=SETTINGS.TWILIO_APPLICATION_SID, incoming_allow=True, ) token.add_grant(voice_grant) # Return token info as JSON token = token.to_jwt() # Return token info as JSON print(token, identity) return {\"token\": token, \"identity\": identity} 回调指令 通过回调接口指示 twilio 后续与服务器的交互，这里我们告诉 twilio 通过 websocket 与服务器进行通信。后续直接在 websocket 进行语音数据的交互。 def start(request: Request, sid: str | None = None): \"\"\" stream webhook \"\"\" form_data = await request.form() if sid is None: sid = form_data.get(\"sid\") logger.info(\"start streaming......sid:%s\", sid) response = VoiceResponse() webhook = SETTINGS.WEBHOOK_URL.replace(\"https\", \"wss\") + \"/ws\" logger.info(\"webhook:%s\", webhook) connect = Connect() stream = connect.stream( name=\"test stream\", url=webhook, ) stream.parameter(\"sid\", sid) response.append(connect) response.pause(length=1000) return Response(str(response), media_type=\"application/xml\") 注意 twilio 收发 stream message 都需要遵循数据格式 收到的数据如下所示 { \"event\": \"media\", \"sequenceNumber\": \"3\", \"media\": { \"track\": \"outbound\", \"chunk\": \"1\", \"timestamp\": \"5\", \"payload\": \"no+JhoaJjpz...\" } , \"streamSid\": \"MZXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" } 发送的数据如下所示 { \"event\": \"media\", \"streamSid\": \"MZXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\", \"media\": { \"payload\": \"a3242sa...\" } } 与其他设备通信 上面的回调示例是直接 web 与服务器进行语音交互的例子。我们当然也可用用其他指令来指示与其他设备交互。 from twilio.twiml.voice_response import Dial, VoiceResponse response = VoiceResponse() dial = Dial() # Route to the client dial.client(identity=\"test_123\") response.append(dial) 这样我们就可以和设备 id 为 “test_123\"直接语音交互。 ","date":"2024-05-29","objectID":"/twilio-web/:0:2","tags":["webRTC","实时语音","twilio"],"title":"利用 Twilio 快速构建 web 语音聊天","uri":"/twilio-web/"},{"categories":["AI","“实现\"","优化"],"content":"在 RAG 中怎么提高文档的检索准确性","date":"2024-03-25","objectID":"/rag_doc/","tags":["rag","semantic search","llm"],"title":"基于文档的 RAG 应用优化方案","uri":"/rag_doc/"},{"categories":["AI","“实现\"","优化"],"content":"基于内部文档的知识问答是我们内部 AI 助手的一个模块，这个模块构建在之前文章基础之上即完整的 RAG(Retrieval-Augmented Generation) 应用。 我们知道对于 RAG 应用来说，生成结果准确与否的关键因素是检索的结果即给 LLM 提供的上下文，在这篇文章里我们介绍基于文档类型的 RAG 构建并介绍如何优化检索结果。 ","date":"2024-03-25","objectID":"/rag_doc/:0:0","tags":["rag","semantic search","llm"],"title":"基于文档的 RAG 应用优化方案","uri":"/rag_doc/"},{"categories":["AI","“实现\"","优化"],"content":"一般的 Pipeline 1. 索引工具类 class VectorModel: def __init__(self, index_name=None): if not index_name: raise ValueError('no index') self.index_name = index_name logger.info(\"model init download and load index...\") service_context = new_context(index_name) weaviate_client = weaviate.Client( url=\"xxxx\", timeout_config=(3, 30), ) vector_store = WeaviateVectorStore( weaviate_client=weaviate_client, index_name=self.index_name, ) # load index self.index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context) 2. 构建索引 def make_index(): node_parser = SentenceSplitter() documents = SimpleDirectoryReader( input_files=[\"//Downloads/公司制度文档/人力资源制度/员工福利管理制度.pdf\"], # input_dir=\"/Downloads/公司制度文档\", recursive=True, ).load_data() # extract nodes nodes = node_parser.get_nodes_from_documents(documents) # build index vm = VectorModel(index_name=\"TestIndex_v1\") vm.index.build_index_from_nodes(nodes) 3. retriver from llama_index.core.retrievers import VectorIndexRetriever retriever = VectorIndexRetriever( index=vm.index, similarity_top_k=3, vector_store_query_mode=VectorStoreQueryMode.HYBRID, ) 4. 构建 chatengine def query(q, index=None): vm = VectorModel(index_name=index) chat_engine = vm.index.as_chat_engine( chat_mode=\"context\", similarity_top_k=2, vector_store_query_mode=VectorStoreQueryMode.HYBRID, # alpha=0.5, system_prompt=\"你是一个公司内部的智能助手，你会根据 context 回答用户的问题。当没有 context information 或者 context information 没有提供与用户问题相关且有用的信息你必须提示用户没找到相关信息并且不做进一步的回答。\", # the target key defaults to `window` to match the node_parser's default node_postprocessors=[ # reranker, MetadataReplacementPostProcessor(target_metadata_key=\"window\"), ] ) answer = chat_engine.chat(q) return answer 示例结果如下 query('婚假有几天', index='TestIndex_v1') # answer 如下 婚假的具体天数并未在给出的上下文中明确说明，但公司会根据工作地城市政策给予婚假，并且员工需要提前15天提交休假申请。建议您查看最新的工作地城市政策或直接咨询人力资源部门获取准确的婚假天数信息。 我们的文档中其实包含了婚假的信息，但最终 LLM 并没有正确回答。通过 debug 我们发现问题是检索到的参考信息缺失了一部分。 通过分析具体的文档和对应的索引我们发现主要存在两类主要问题 PDF 文档带有表格或者其他类型的格式，解析后的文本可能会出现错乱 默认的 PDF 文档解析器按照 page 读取，同一段上下文分属连续的两页就可能导致检索的信息不完整 优化方案如下 ","date":"2024-03-25","objectID":"/rag_doc/:0:1","tags":["rag","semantic search","llm"],"title":"基于文档的 RAG 应用优化方案","uri":"/rag_doc/"},{"categories":["AI","“实现\"","优化"],"content":"自定义 PDFReader 为了兼容不同格式的 PDF 我们自定义了 PDF 读取类，在这里我们引入了 llmsherpa LayoutPDFReader \"\"\"Custom PDF Loader.\"\"\" from typing import List, Optional from llama_index.core import SimpleDirectoryReader from llama_index.core.readers.base import BaseReader from llama_index.core.schema import Document class CustomPDFLoader(BaseReader): \"\"\"CustomPDFLoader uses nested layout information such as sections, paragraphs, lists and tables to smartly chunk PDFs for optimal usage of LLM context window. Args: llmsherpa_api_url (str): Address of the service hosting llmsherpa PDF parser \"\"\" def __init__( self, llmsherpa_api_url: str = None, input_dir: Optional[str] = None, input_files: Optional[List] = None, ) -\u003e None: super().__init__() from llmsherpa.readers import LayoutPDFReader self.base_reader = SimpleDirectoryReader(input_dir=input_dir, input_files=input_files, recursive=True) self.pdf_reader = LayoutPDFReader(llmsherpa_api_url) def load_data(self) -\u003e List[Document]: \"\"\"Load data and extract table from PDF file. Returns: List[Document]: List of documents. \"\"\" documents = [] files = self.base_reader.input_files for file in files: filename = str(file) metadata = self.base_reader.file_metadata(filename) doc = self.pdf_reader.read_pdf(filename) for chunk in doc.chunks(): metadata.update({\"chunk_type\": chunk.tag}) document = Document( text=chunk.to_context_text(), metadata=metadata ) document.excluded_embed_metadata_keys.extend( [ \"chunk_type\" \"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\", ] ) document.excluded_llm_metadata_keys.extend( [ \"chunk_type\" \"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\", ] ) documents.append(document) return documents ","date":"2024-03-25","objectID":"/rag_doc/:0:2","tags":["rag","semantic search","llm"],"title":"基于文档的 RAG 应用优化方案","uri":"/rag_doc/"},{"categories":["AI","“实现\"","优化"],"content":"SentenceWindowNodeParser SentenceWindowNodeParser 将所有文档拆分为单独的句子。生成的节点还包含元数据中每个节点周围的句子“窗口”，结合 MetadataReplacementNodePostProcessor，在将 node 发送到 LLM 之前将句子替换为其周围的上下文。 这个 node parser 对于比较大的文档非常有用，能帮助我们检索更细粒度的详细信息。 llmsherpa_api_url = \"xxx\" reader = CustomPDFLoader( llmsherpa_api_url=llmsherpa_api_url, # input_files=[\"/Downloads/公司制度文档/人力资源制度/ZH-rl-2024-13 员工福利管理制度.pdf\"], input_dir=\"/Users/wanglinxiao/Downloads/公司制度文档\", ) documents = reader.load_data() node_parser = SentenceWindowNodeParser.from_defaults( window_size=3, window_metadata_key=\"window\", original_text_metadata_key=\"original_text\", ) nodes = node_parser.get_nodes_from_documents(documents) # build index vm = VectorModel(index_name=\"TestIndex_v2\") vm.index.build_index_from_nodes(nodes) ","date":"2024-03-25","objectID":"/rag_doc/:0:3","tags":["rag","semantic search","llm"],"title":"基于文档的 RAG 应用优化方案","uri":"/rag_doc/"},{"categories":["AI","“实现\"","优化"],"content":"SemanticSplitterNodeParser SemanticSplitterNodeParser 不使用固定块大小对文本进行分块，而是使用嵌入相似性 (embedding similarity) 自适应地选择句子之间的断点。这样确保了 “chunk” 包含语义上彼此相关的句子。 但是这个分割器有个比较大的缺点是大量文档下构建索引速度会很慢（毕竟要计算 embedding similarity）。 embed_model = get_bge_embedding() node_parser = SemanticSplitterNodeParser( buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model ) ","date":"2024-03-25","objectID":"/rag_doc/:0:4","tags":["rag","semantic search","llm"],"title":"基于文档的 RAG 应用优化方案","uri":"/rag_doc/"},{"categories":["AI","“实现\"","优化"],"content":"三种 NodeParser 检索对比 检索结果里省略了部分与 query 无关的信息 SentenceSplitter 假和地方法规规定的额外婚假。 ●休婚假须提前15天通过企业微信—再惠人事自助平台提交 休假申请，由部门负责人安排人员接替其工作，并需将结婚证以照片 形式上传到企业微信—再惠人事自助平台“附件”一栏，经批准后方 可休假。 ●按照工作地城市政策发放婚假工资。 ●婚假为连续日假期，须在结婚证书颁发日期后6个月内一次 休完。 5.丧假 xxx 6.工伤假 xxx SentenceWindowNodeParser + CustomPDFReader original_text: 婚假 ● 员工符合法定结婚年龄，即男方不早于 22 周岁，女方不早 于 20 周岁，可享受婚假。 ● 员工在公司工作期间依法登记结婚的可享有 3 个日历日婚 休婚假须提前 15 天通过企业微信—再惠人事自助平台提交 休假申请，由部门负责人安排人员接替其工作，并需将结婚证以照片 形式上传到企业微信—再惠人事自助平台“附件”一栏，经批准后方 可休假。 ● 按照工作地城市政策发放婚假工资。 ● 婚假为连续日假期，须在结婚证书颁发日期后 6 个月内一次 休完。 5. window: 4. 婚假 ● 员工符合法定结婚年龄，即男方不早于 22 周岁，女方不早 于 20 周岁，可享受婚假。 ● 员工在公司工作期间依法登记结婚的可享有 3 个日历日婚 休婚假须提前 15 天通过企业微信—再惠人事自助平台提交 休假申请，由部门负责人安排人员接替其工作，并需将结婚证以照片 形式上传到企业微信—再惠人事自助平台“附件”一栏，经批准后方 可休假。 ● 按照工作地城市政策发放婚假工资。 ● 婚假为连续日假期，须在结婚证书颁发日期后 6 个月内一次 休完。 5. 丧假 ● xxx 7. 产假 ● xxx8. SemanticSplitterNodeParser + CustomPDFReader 婚假 ● 员工符合法定结婚年龄，即男方不早于 22 周岁，女方不早 于 20 周岁，可享受婚假。 ● 员工在公司工作期间依法登记结婚的可享有 3 个日历日婚 休婚假须提前 15 天通过企业微信—再惠人事自助平台提交 休假申请，由部门负责人安排人员接替其工作，并需将结婚证以照片 形式上传到企业微信—再惠人事自助平台“附件”一栏，经批准后方 可休假。 ● 按照工作地城市政策发放婚假工资。 ● 婚假为连续日假期，须在结婚证书颁发日期后 6 个月内一次 休完。 5. 丧假 ● xxx 7. 产假 ● xxx。 8. 产检假 ● xxx 9. 陪产假 ● xxx 10. 哺乳假 ● xxx 我们可以看到上面方案 2 和方案 3 检索“婚假有几天”的结果更完整，包含了我们需要的所有上下文信息。 最终我们采用的方案是 SentenceWindowNodeParser + CustomPDFReader 示例结果如下： query('婚假有几天', index='TestIndex_v2') # answer 如下 员工在公司工作期间依法登记结婚的可享有 3 个日历日的婚假。 ","date":"2024-03-25","objectID":"/rag_doc/:0:5","tags":["rag","semantic search","llm"],"title":"基于文档的 RAG 应用优化方案","uri":"/rag_doc/"},{"categories":["AI","“实现\"","系统"],"content":"如何高效搭建 RAG 中的搜索系统","date":"2024-02-28","objectID":"/rag_rerank/","tags":["semantic search","embedding","rag","rerank"],"title":"RAG 应用的关键步骤 - 优化搜索结果","uri":"/rag_rerank/"},{"categories":["AI","“实现\"","系统"],"content":"在这篇文章里我们介绍了快速搭建语义搜索的实践方案。在这里我们会介绍语义搜索的进阶方案，包括使用开源模型构建文本 embedding、更好用的向量数据库以及介绍如何提高搜索的准确率。 ","date":"2024-02-28","objectID":"/rag_rerank/:0:0","tags":["semantic search","embedding","rag","rerank"],"title":"RAG 应用的关键步骤 - 优化搜索结果","uri":"/rag_rerank/"},{"categories":["AI","“实现\"","系统"],"content":"开源 embedding 模型 我们之前使用的是 OpenAI 的 text-embedding-ada-002，目前有一些国内的开源模型在中文领域效果比 text-embedding-ada-002 要好。评测结果可以参考这个排行榜。 这里我们以模型 BAAI/bge-large-zh-v1.5 为例演示如何使用开源模型。 我们还是使用 llmindex 来作为工具包创建向量索引。 def new_context(index_name): endpoint = \"xxxx\" api_version = \"2023-05-15\" llm = AOpenAI( engine=\"gpt35\", model=\"gpt-35-turbo-16k\", temperature=0.7, azure_endpoint=endpoint, api_key=key, api_version=api_version, ) # 这部分代码展示了使用开源模型的方式 if 'Bge' in index_name: model_name = \"BAAI/bge-large-zh-v1.5\" model_kwargs = {\"device\": \"cpu\"} encode_kwargs = {\"normalize_embeddings\": True} embedding = HuggingFaceBgeEmbeddings( model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs ) elif 'Openai' in index_name: embedding = AzureOpenAIEmbedding( mode=OpenAIEmbeddingMode.SIMILARITY_MODE, model=\"text-embedding-ada-002\", azure_deployment=\"text-embedding-ada-002\", api_key=key, azure_endpoint=endpoint, api_version=api_version, max_retries=1, request_timeout=20, ) else: raise ValueError('no valid model') service_context = ServiceContext.from_defaults( llm=llm, embed_model=embedding ) return service_contex ","date":"2024-02-28","objectID":"/rag_rerank/:0:1","tags":["semantic search","embedding","rag","rerank"],"title":"RAG 应用的关键步骤 - 优化搜索结果","uri":"/rag_rerank/"},{"categories":["AI","“实现\"","系统"],"content":"向量数据库 目前可供选择的 vector database 还是很多的，大家可以参考这个榜单。 从部署成本、使用便捷、文档易读和性能等角度调研下来我们选择的是 weaviate。 Weaviate 是一个低延迟矢量数据库，对不同媒体类型（文本、图像等）提供开箱即用的支持。它提供语义搜索、问答提取、分类、可定制模型 (PyTorch/TensorFlow/Keras) 等。Weaviate 在 Go 中从头开始构建，可存储对象和向量，允许将向量搜索与结构化过滤以及容错能力相结合。云原生数据库。这一切都可以通过 GraphQL、REST 和各种客户端编程语言进行访问。 快速查询 Weaviate 通常在不到 100 毫秒的时间内对数百万个对象执行最近邻 (NN) 搜索。您可以在我们的基准页面上找到更多信息。 使用 Weaviate 模块摄取任何媒体类型 使用最先进的 AI 模型推理（例如 Transformer）在搜索和查询时访问数据（文本、图像等），让 Weaviate 为您管理数据矢量化过程 - 或提供您自己的数据矢量化过程 向量。 组合向量和标量搜索 Weaviate 允许高效、组合的矢量和标量搜索。例如，“过去 7 天内发表的与 COVID-19 大流行相关的文章”。Weaviate 存储对象和向量，并确保两者的检索始终高效。不需要第三方对象存储。 实时且持久 即使当前正在导入或更新数据，Weaviate 也可让您搜索数据。此外，每次写入都会写入预写日志 (WAL)，以便立即持久写入 - 即使发生崩溃也是如此。 水平扩展性 根据您的具体需求扩展 Weaviate，例如最大摄取量、最大可能的数据集大小、每秒最大查询数等。 成本效益 非常大的数据集不需要完全保存在 Weaviate 的内存中。同时，可以利用可用内存来提高查询速度。这样可以有意识地进行速度/成本权衡，以适应每个用例。 对象之间类似图形的连接 以类似图形的方式在对象之间建立任意连接，以类似于数据点之间的现实生活连接。使用 GraphQL 遍历这些连接。 使用示例如下： def create_weavite_vectorstore(): service_context = context() storage_context = StorageContext.from_defaults(persist_dir=store_dir) # load old index index = load_index_from_storage(storage_context) embed_dict = index._vector_store._data.embedding_dict node_dict = index._docstore.docs nodes = [] for node_id, node in node_dict.items(): vector = embed_dict[node_id] node.embedding = vector nodes.append(node) # create client and a new collection weaviate_client = weaviate.Client( url=\"xxx\", ) vector_store = WeaviateVectorStore( weaviate_client=weaviate_client, index_name=\"TestIndex\", ) storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex(nodes, storage_context=storage_context, service_context=service_context) return index ","date":"2024-02-28","objectID":"/rag_rerank/:0:2","tags":["semantic search","embedding","rag","rerank"],"title":"RAG 应用的关键步骤 - 优化搜索结果","uri":"/rag_rerank/"},{"categories":["AI","“实现\"","系统"],"content":"管理索引 我们先实现一个工具类，每次都能使用这个类来方便的创建索引，更新索引或者查询。 class VectorModel: def __init__(self, index_name=None): if not index_name: raise ValueError('no index') self.index_name = index_name logger.info(\"model init download and load index...\") service_context = new_context(index_name) weaviate_client = weaviate.Client( url=\"xxxx\", timeout_config=(3, 30), ) vector_store = WeaviateVectorStore( weaviate_client=weaviate_client, index_name=self.index_name, ) # load index self.index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context) self.retriever = VectorIndexRetriever( index=self.index, similarity_top_k=10, # vector_store_query_mode=VectorStoreQueryMode.HYBRID, ) 创建索引 测试数据格式如下所示 id biz 描述 5 业务 1 [“查看客户拜访记录”, “浏览陪同拜访资料”, “检索客户拜访详情”, “筛选按姓名查找拜访记录”, “按拜访地址寻找记录”, “根据联系方式查询拜访信息”] 6 业务 2 [“创建和发布公司通知”, “根据部门发送公告”, “按工作年限筛选接收公告的员工”, “向特定群体发送消息”, “定向发布内部通知”, “管理和发送组织公告”] … 根据上面的数据创建索引 def make_index(): file = '/test.csv' vm = VectorModel(index_name=\"TestIndex_Bge\") documents = [] with open(file) as f: reader = csv.reader(f) for i, r in enumerate(reader): id = r[0] biz = r[1] try: fns = json.loads(r[2]) except: print(i, id) continue for fn in fns: documents.append(Document( text=fn, metadata={\"page_id\": id, \"biz\": biz} )) print(id, biz, fn) vm.index.refresh_ref_docs(documents) 删除部分索引 删除部分索引示例 def update_objects(): client = weaviate.Client(\"xxx\") all_objects = client.data_object.get(class_name=\"TestIndex_Bge\", limit=100) print('total objects: ', all_objects['totalResults']) for ob in all_objects['objects']: if ob['properties']['page_id'] in ['10', '27', '29', '42', '49']: print(ob['properties']['page_id'], ob['properties']['text']) client.data_object.delete( uuid=ob['id'], class_name=ob['class'] ) 查询 vm = VectorModel(index_name=\"Testndex_Bge\") res = m1.retriever.retrieve(query)[0] 示例结果如下 Node ID: c7436e4d-db8b-4bd5-a891-b86ae8b2e550 Text: 浏览陪同拜访资料 Score: 0.486 ","date":"2024-02-28","objectID":"/rag_rerank/:0:3","tags":["semantic search","embedding","rag","rerank"],"title":"RAG 应用的关键步骤 - 优化搜索结果","uri":"/rag_rerank/"},{"categories":["AI","“实现\"","系统"],"content":"Rerank Rerank 模型通过对候选文档列表进行重新排序，以提高其与用户查询语义的匹配度，从而优化排序结果。该模型的核心在于评估用户问题与每个候选文档之间的关联程度，并基于这种相关性给文档排序，使得与用户问题更为相关的文档排在更前的位置。这种模型的实现通常涉及计算相关性分数，然后按照这些分数从高到低排列文档。 这里我们使用并测试了两种 rerank 模型：闭源的 Cohere Rerank 模型 rerank-multilingual-v2.0 和 开源的 bge-reranker-large。 使用示例如下： reranker = CohereRerank(model=\"rerank-multilingual-v2.0\", api_key=\"xxx\") def bge_rerank(nodes, query): reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) reqs = [[query, n.text] for n in nodes] scores = reranker.compute_score(reqs) scores_with_index = [(i, s) for i, s in enumerate(scores)] return [nodes[s[0]] for s in sorted(scores_with_index, key=lambda x: x[1], reverse=True)] 完整示例 完整的评测代码如下 file = '/test.csv' data = [] m1 = VectorModel(index_name=\"TestIndex_Openai\") m2 = VectorModel(index_name=\"TestIndex_Bge\") with open(file) as f: reader = csv.reader(f) header = next(reader) for i, r in enumerate(reader): if not r[1]: continue res1 = m1.retriever.retrieve(r[1]) r[3] = f\"{res1[0].score} {res1[0].text} {res1[0].metadata}\" r[4] = 'False' if res1[0].metadata.get('url') in r[2]: r[4] = 'True' res2 = m2.retriever.retrieve(r[1]) r[5] = f\"{res2[0].score} {res2[0].text} {res2[0].metadata}\" r[6] = 'False' if res2[0].metadata.get('url') in r[2]: r[6] = 'True' # cohere rerank res4 = m1.reranker.postprocess_nodes(res1, query_str=r[1])[0] # bge rerank res5 = bge_rerank(res2, r[1])[0] r[7] = f\"{res4.score} {res4.text} {res4.metadata}\" r[8] = 'False' if res4.metadata.get('url') in r[2]: r[8] = 'True' r[9] = f\"{res5.score} {res5.text} {res5.metadata}\" r[10] = 'False' if res4.metadata.get('url') in r[2]: r[10] = 'True' data.append(r) print(i, r[1]) with open('/test_res.csv', 'w') as w: writer = csv.writer(w) writer.writerow(header) for d in data: writer.writerow(d) 测试结果如下所示（省略内容） query 预期 id openai bge openai+cohere rerank bge+bge rerank 看拜访记录 5 5 5 5 5 查看商户数据 10 8 46 10 10 … - - 72.5% 67.5% 77.5% 85% 在这篇文章里介绍了我们怎么去创建 RAG 应用的思路和实践，相信能提供一些实际的帮助。 ","date":"2024-02-28","objectID":"/rag_rerank/:0:4","tags":["semantic search","embedding","rag","rerank"],"title":"RAG 应用的关键步骤 - 优化搜索结果","uri":"/rag_rerank/"},{"categories":["AI","实现","原理"],"content":"单独使用语言模型（LLM）对于简单的任务比如让模型写一段文案来说是可以的，但更复杂的任务往往只依靠 LLM 很难取得良好的效果甚至不可用。 所以 AI agent 的提出就是为了弥补当前 LLM 在很多方面的能力不足。可以这么理解 LLM 就像人的大脑，可以理解人类的语言，agent 就好比给 LLM 装上了五官和四肢。 ","date":"2024-01-12","objectID":"/crewai-practice/:0:0","tags":["AI agent","ReAct","crewAI","langchain"],"title":"crewAI 搭建 AI agent 的原理及应用","uri":"/crewai-practice/"},{"categories":["AI","实现","原理"],"content":"AI agent AI 代理（AI agent）是指使用 AI 技术设计和编程的一种计算机程序，其可以独立地进行某些任务并对环境做出反应。AI 代理可以被视为一个智能体，它能够感知其环境，通过自己的决策和行动来改变环境，并通过学习和适应来提高其性能。这种智能体同时使用短期记忆（上下文学习）和长期记忆（从外部向量存储中检索信息），有能力通过逐步“思考”来计划、将目标分解为更小的任务，并反思自己的表现。AI 代理通常包含多种技术，如机器学习、自然语言处理、计算机视觉、规划和推理等，这些技术使代理能够自主地处理信息并作出决策。 LangChain中已经实现了我们想要的部分 agent 功能。agent 会自己分析问题，选择合适的工具，最终解决问题。其中应用很广泛的一类就是 ReAct。 ReAct ReAct 是 Reasoning and Acting（也有一说是 Reason Act）缩写，意思是 LLM 可以根据逻辑推理（Reason），构建完整系列行动（Act），从而达成期望目标。LLM 灵感来源是人类行为和推理之间的协同关系。人类根据这种协同关系学习新知识，做出决策，然后执行。LLM 在逻辑推理上有着非常优秀的表现，因此有理由相信 LLM 也可以像人类一样进行逻辑推理，学习知识，做出决策，并执行。在实际使用中，LLM 会发生幻觉和错误判断的情况。这是因为 LLM 在训练的时候接触到的知识有限。因此对超出训练过程中使用的数据进行逻辑分析时，LLM 就会开始不懂装懂地编造一些理由。因此对于解决这个问题最好的办法是，可以保证 LLM 在做出分析决策时，必须将应该有的知识提供给 LLM。 ReAct 方式的作用就是协调 LLM 和外部的信息获取，与其他功能交互。如果说 LLM 是大脑，那 ReAct 框架就是这个大脑的手脚和五官。同时具备帮助 LLM 模型获取信息、输出内容与执行决策的能力。对于一个指定的任务目标，ReAct 框架会自动补齐 LLM 应该具备的知识和相关信息，然后再让 LLM 做出决策，并执行 LLM 的决策。 一个 ReAct 流程里，关键是三个概念： Thought：由 LLM 生成，是 LLM 产生行为和依据。可以根据 LLM 的思考，来衡量他要采取的行为是否合理。这是一个可用来判断本次决策是否合理的关键依据。相较于人类，thought 的存在可以让 LLM 的决策变得更加有可解释性和可信度。 Act：Act 是指 LLM 判断本次需要执行的具体行为。Act 一般由两部分组成：行为和对象。用编程的说法就是 API 名称和对应的入参。LLM 模型最大的优势是，可以根据 Thought 的判断，选择需要使用的 API 并生成需要填入 API 的参数。从而保证了 ReAct 框架在执行层面的可行性。 Obs：LLM 框架对于外界输入的获取。它就像 LLM 的 五官，将外界的反馈信息同步给 LLM，协助 LLM 进一步的做分析或者决策。 一个完整的 ReAct 的行为，包涵以下几个流程： 1.输入目标：任务的起点。可以是用户的手动输入，也可以是依靠触发器（比如系统故障报警）。 2.LOOP：LLM 模型开始分析问题需要的步骤（Thought），按步骤执行 Act，根据观察到的信息（Obs），循环执行这个过程。直到判断任务目标达成。 3.Finish：任务最终执行成功，返回最终结果。 ","date":"2024-01-12","objectID":"/crewai-practice/:0:1","tags":["AI agent","ReAct","crewAI","langchain"],"title":"crewAI 搭建 AI agent 的原理及应用","uri":"/crewai-practice/"},{"categories":["AI","实现","原理"],"content":"crewAI CrewAI 是一个基于角色扮演的 AI agent 团队自动化协作框架，它旨在为 AI agent 提供自动化设置，以促进 AI agent 之间的合作，使这些代理能够共同解决复杂问题。CrewAI 的主要构建模块包括代理、任务、工具和团队。 在 CrewAI 中，agent 是一个被编程为执行任务、做出决策并与其他代理进行通信的自治单元。将 agent 视为团队的成员，具有特定的技能和特定的工作要做。agent 可以担任不同的角色，例如“研究员”、“作家”或“客户支持”，每个角色都有助于团队的总体目标。agent 的关键属性包括角色、目标、背景故事和工具集。 任务是 CrewAI 中的基本工作单元，是给定代理应完成的小型、专注的任务。任务设计灵活，可根据需要进行简单和复杂的操作。任务的关键属性包括描述、分配给它的代理以及所需的任何特定工具。 CrewAI 的主要优势在于它的自动化协作框架，它可以帮助 AI agent 之间更好地合作，共同解决复杂问题。CrewAI 通过提供自动化设置，使代理之间的合作变得更加容易，从而提高了团队的效率和生产力。 ","date":"2024-01-12","objectID":"/crewai-practice/:0:2","tags":["AI agent","ReAct","crewAI","langchain"],"title":"crewAI 搭建 AI agent 的原理及应用","uri":"/crewai-practice/"},{"categories":["AI","实现","原理"],"content":"实践 有了上述的知识之后我们再来是 crewAI 来搭建 agent 会变的容易，并且能够较好的理解 agent 的工作流。 “每天吃什么是个烦人的问题”在这里我们就以解决这个问题为目的来搭建一个 AI agent 来帮我们安排工作日的午餐。 定义 agents 在这个任务里我们定义两个 agent 一个用来选择餐厅，另外一个用来安排顺序。 class FoodAgents(): def restaurant_select_agent(self): return Agent( role='餐厅挑选达人', goal='根据预算和距离，餐厅好评度挑选合适的餐厅名单', backstory='擅长根据餐厅数据挑选就餐地方', tools=[ SearchTools.search_internet, BrowserTools.scrape_and_summarize_website, ], llm=llm(), verbose=True ) def keeper_agent(self): return Agent( role='每日午餐管家', goal='根据预算合理安排最近一周工作日 5 天的每日午餐就餐餐厅', backstory='在餐饮和营养方面拥有数十年经验的专家', tools=[ SearchTools.search_internet, BrowserTools.scrape_and_summarize_website, ], llm=llm(), verbose=True ) 定义 tasks 给上述 agent 定义他们要完成的任务 class FoodTasks(): def seclet_task(self, agent, budget, district): return Task( description=dedent(f\"\"\" 作为餐厅挑选达人，你会根据距离、预算、餐厅好评度、餐厅品类挑选出最适合 工作日午餐就餐的餐厅。 挑选出的餐厅价格不能超出预算范围，就餐的时间不能过长。 用中文回复。 预算:{budget} 工作地:{district} \"\"\"), agent=agent ) def plan_task(self, agent, budget, district): return Task( description=dedent(f\"\"\" 根据餐厅备选清单，你必须从就餐新鲜度和营养角度合理安排最近一周 5 天工作日每天的餐厅。 你最终的答案是一个完整的包含工作日 5 天的就餐计划格式化为 markdown，包括每日就餐餐厅名称，餐厅简介。 用中文回复。 预算:{budget} 工作地:{district} \"\"\"), agent=agent ) 启动 agents class RestaurantPlan: def __init__(self, district, budget): self.district = district self.budget = budget def run(self): agents = FoodAgents() tasks = FoodTasks() restaurant_agent = agents.restaurant_select_agent() plan_agent = agents.keeper_agent() select_task = tasks.seclet_task( restaurant_agent, self.budget, self.district ) plan_task = tasks.plan_task( plan_agent, self.budget, self.district ) crew = Crew( agents=[restaurant_agent, plan_agent], tasks=[select_task, plan_task], verbose=True ) result = crew.kickoff() return result if __name__ == '__main__': budget = input(dedent(\"预算：\")) district = input(dedent(\"工作地点：\")) c = RestaurantPlan(district, budget) result = c.run() print(\"工作日吃什么\\n\") print(result) ","date":"2024-01-12","objectID":"/crewai-practice/:0:3","tags":["AI agent","ReAct","crewAI","langchain"],"title":"crewAI 搭建 AI agent 的原理及应用","uri":"/crewai-practice/"},{"categories":["AI","实现","原理"],"content":"分析工作流 Entering new AgentExecutor chain… 下面的文本展示了让 LLM 推理的 prompt (为了便于展示省略了部分信息) You are 餐厅挑选大人. 擅长根据餐厅数据挑选就餐地方 Your personal goal is: 根据预算和距离，餐厅好评度挑选合适的餐厅名单 TOOLS: ------ You have access to the following tools: Search the internet: Search the internet(query) - Useful to search the internet about a a given topic and return relevant results Scrape website content: xxxxxx`. Ask Question to Co-Worker: xxx`. To use a tool, please use the exact following format: Thought: Do I need to use a tool? Yes Action: the action to take, should be one of [Search the internet, Scrape website content, Delegate Work to Co-Worker, Ask Question to Co-Worker] Action Input: the input to the action Observation: the result of the action When you have a response for your task, or if you do not need to use a tool, you MUST use the format: Thought: Do I need to use a tool? No Final Answer: [your response here] This is the summary of your work so far: Begin! This is VERY important to you, your job depends on it! Current Task: 作为餐厅挑选大人，你会根据距离、预算、餐厅好评度、餐厅品类挑选出最适合 工作日午餐就餐的餐厅。 挑选出的餐厅价格不能超出预算范围，就餐的时间不能过长。 用中文回复。 预算:20-50 工作地:长风大悦城 LLM 推理结果 Thought: Do I need to use a tool? Yes Action: Delegate Work to Co-Worker Action Input: 每日午餐管家|询问长风大悦城附近的餐厅有哪些|工作地:长风大悦城, 餐厅品类:不限 省略中间的搜索和总结搜索结果过程… 3. 继续推理 You are 每日午餐管家. 在餐饮和营养方面拥有数十年经验的专家 Your personal goal is: 根据预算合理安排最近一周工作日5天的每日午餐就餐餐厅 TOOLS: ------ You have access to the following tools: Search the internet: Search the internet(query) - Useful to search the internet about a a given topic and return relevant results Scrape website content: Scrape website content(website) - Useful to scrape and summarize a website content To use a tool, please use the exact following format: Thought: Do I need to use a tool? Yes Action: the action to take, should be one of [Search the internet, Scrape website content] Action Input: the input to the action Observation: the result of the action When you have a response for your task, or if you do not need to use a tool, you MUST use the format: Thought: Do I need to use a tool? No Final Answer: [your response here] This is the summary of your work so far: Begin! This is VERY important to you, your job depends on it! Current Task: 询问长风大悦城附近的餐厅有哪些 This is the context you are working with: 工作地:长风大悦城, 餐厅品类:不限 Thought: Do I need to use a tool? Yes Action: Search the internet Action Input: \"长风大悦城 附近 餐厅\"Title: 长风大悦城- 上海餐厅,酒吧,夜生活,Spa,娱乐,购物 - Shanghai WOW! Link: http://www.shanghaiwow.com/merchant/detail/id/2129 Snippet: 新天地超隐蔽酒廊，为你揭秘魔都名流的奢华派对！ 想要成功打入魔都精英人士的社交圈， 能顺利进入少数那几间高端会所，是关键所在。 近期我们在时尚潮人扎堆的新天地， ... ----------------- Title: 1點點(長風大悅城店)餐廳、地址- 上海 - Trip.com Link: https://hk.trip.com/travel-guide/shanghai-2-restaurant/dian-dian-chang-feng-jing-pan-an-chang-dian-61284402/ Snippet: 1點點(長風大悅城店)附近的餐廳 ; 福和慧. 4.8 · 條評價. HK$1249. 素食 ; 甬府（黃浦店）. 4.7 · 條評價. HK$1058. 江浙菜景觀餐廳 ; くろぎKurogi 黑木. 4.7 · 條評價. HK$3599. ----------------- Title: 顺峰顺水顺德菜(长风大悦城店) - you.ctrip.com Link: https://you.ctrip.com/food/2/78842745.html Snippet: 距餐厅2km. 食廬(虹桥南丰城店). 4.5 ; 距餐厅2.1km. Erman岩烧牛排(虹桥南丰城店). 订. 4.8 ; 距餐厅1.8km. 瑞华樟园. 4.6 ... ----------------- Title: 味千拉面(长风大悦城店)平面地图- 餐饮美食 - 城市吧 Link: https://ditu.city8.com/sh/cater/8d8mim794w7vb060f5?map Snippet: 味千拉面(长风大悦城店)地址位于上海市普陀区大渡河路196号长风大悦城F1，靠近中江路、云岭东路、大渡河路和古北路，联系电话为(021)61437703。 LLM 推理结果 Do I need to use a tool? No Final Answer: Some nearby restaurants to 长风大悦城 are: 1點點, 湘庭精致湖南菜, 味千拉面, 顺峰顺水顺德菜, 西贝莜面村, and 大肆兄. 继续推理 You are 每日午餐管家. 在餐饮和营养方面拥有数十年经验的专家 Your personal goal is: 根据预算合理安排最近一周工作日5天的每日午餐就餐餐厅 TOOLS: ------ You have access to the following tools: Search the internet: Search the internet(query) - Useful to search the internet about a a given topic and return relevant results Scrape website content: Scrape website content(website) - Useful to scrape and summarize a website content Delegate Work to Co-Worker: xxx. Ask Question to Co-Worker: xxx. To use a tool, please use the exact following format: Thought: Do I need to use a tool? Yes Action: the action to take, should be one ","date":"2024-01-12","objectID":"/crewai-practice/:0:4","tags":["AI agent","ReAct","crewAI","langchain"],"title":"crewAI 搭建 AI agent 的原理及应用","uri":"/crewai-practice/"},{"categories":["AI","实现","原理"],"content":"ReAct 问题 不稳定 非常依赖 LLM 的性能。即使目前最好的 LLM gpt-4 仍然存在幻觉和内容输出不稳定的情况，这样导致最后的结果不够稳定; 成本高 如果使用推理性能好像闭源的 gpt-4 因为执行流程长，LLM 会被反复调用，这里的 token 成本会非常高； 响应慢 LLM 推理慢，加上反复多次调用，整个执行过程时间会比较长 目前 CrewAI 支持调用在 Ollama 框架下启动的语言模型，但是这块支持的还不够完善并且规模较小的 7B、13B 等开源模型性能不够好，导致整体 agent 生产环境的可用性还有待提高。这方面进展我们将持续关注。 ","date":"2024-01-12","objectID":"/crewai-practice/:0:5","tags":["AI agent","ReAct","crewAI","langchain"],"title":"crewAI 搭建 AI agent 的原理及应用","uri":"/crewai-practice/"},{"categories":["AI","实现","系统"],"content":"高效搭建文本搜索的方案","date":"2023-12-16","objectID":"/text-similarity/","tags":["semantic search","embedding"],"title":"快速搭建文本语义搜索","uri":"/text-similarity/"},{"categories":["AI","实现","系统"],"content":"想象这样一个场景我们文案团队每天都会根据需求产出高质量的文案，随着需求的增加我们系统已经累积了很多优质文案。那么我们是否可以根据新的需求描述去搜索文案库里已有的文案呢？利用文本相似度检测我们能实现一个这样既能提高效率又能节省成本的文案搜索方案。 在这篇文章里我们介绍一下快速搭建文本语义搜索系统的方法。 ","date":"2023-12-16","objectID":"/text-similarity/:0:0","tags":["semantic search","embedding"],"title":"快速搭建文本语义搜索","uri":"/text-similarity/"},{"categories":["AI","实现","系统"],"content":"文本处理 文本由两部分组成 text + metadata，这里的 metadata 可以理解成是 text 的一些属性。通过这些属性我们可以找到跟 text 关联的其他信息。 在我们这个例子里我们的每段 text 对应一个 task_id ，并且 metadata 我们只关心这个 task id。部分数据如下 571600,\"xxx豆花渎鱼——入口图文案： 1、宠粉双十二，桌桌有礼拿 2、「黔」方双十二，霸王餐送达 3、双十二来袭， 免单指到「礼」 4、12.12 | 抽奖赢好礼，这单老板请\" 571599,\"黔渔翁豆花渎鱼——商户通文案： 1、双十二「渔」你同乐，免单奖砸中「礼」了 2、\u003c12.12剧透\u003e抽奖大乐透，壕礼人人有 | 这一桌老板买单！ 3、双十二提「黔」到，免单大奖播报 | 逢抽必中，来者就送 4、爱要「奖」出来，双十二特派🎁敢抽就敢送，霸王餐派送 +辅助文案： 凭买单小票抽奖 免单、霸王餐、代金券、神秘礼拼手气\" 572021,\"1、优雅姿造匠心制，现做鲜美宴贵宾 2、海鲜姿造聚高端食材，商务宴请迎八方贵客 3、深海鲜美入馔，视味姿造盛宴，静待诸位入席\" 571440,\"xxx核桃炭烤串 - 商户通 1. 烤肉局申请加入跨年PLAN！牛羊串X排骨筋，好友欢聚撸过瘾！ 2. 元旦当“燃”这一串！牛羊日日鲜，老铁酷酷炫！ 3. 2023-2024舌尖换乘点 | 和快乐“串”通，欢聚更尽兴！ 4. “元”气开烤，“旦”愿美好！围炉聚老友，火热烤肉季！ \" 570781,\"xxx小酒馆——推荐菜文案： 【羊肉汤锅】 暖身汤力荐，领鲜三十年。慢火足时酝酿，开锅满室浓香，除了爱无添加。汤色乳白，汤味鲜香，羊肉无膻，香醇浓厚。人生百味，羊汤抚慰 【现烤羊腿】 技法炉火纯青，方得羊香四溢。千里挑一羊腿，传统炭火炙烤，披上油亮新装。外焦香酥脆，内鲜嫩多汁，撕着吃才够味！ 【火焰鱼羊鲜】 双鲜论剑，唇齿流连。烈烈火焰之上，锡纸一方天地，鱼与羊尽释本味。鱼肉吸收羊肉浓香，羊肉吸收鱼肉鲜甜，尽释“鲜”字奥义。\" 我们把历史数据先导出到 csv, 然后进行如下处理 def load(file): documents = [] with open(file) as f: reader = csv.reader(f) next(reader) for record in reader: documents.append(record) return documents ","date":"2023-12-16","objectID":"/text-similarity/:0:1","tags":["semantic search","embedding"],"title":"快速搭建文本语义搜索","uri":"/text-similarity/"},{"categories":["AI","实现","系统"],"content":"创建索引 想通过语义准确的匹配到文本，最关键的是把文本转换为 embedding然后建立向量索引库。这里我们用的模型是 openai 的 text-embedding-ada-002。为了高效的创建以及后续的查询，这些繁琐的工作我们不需要每行都自己实现，llmindex可以帮助我们快速来搭建。 通过下面的代码我们创建了索引并持久化到了本地文件 def context(): embedding_llm = AzureOpenAIEmbedding( mode=OpenAIEmbeddingMode.SIMILARITY_MODE, model=\"text-embedding-ada-002\", azure_deployment=\"text-embedding-ada-002\", api_key=openai.api_key, azure_endpoint=endpoint, api_version=openai.api_version, max_retries=1, request_timeout=20, ) # max LLM token input size max_input_size = 4096 # set number of output tokens num_output = 500 # set maximum chunk overlap max_chunk_overlap = 20 chunk_size_limit = 1 # prompt_helper = PromptHelper(max_input_size, num_output) service_context = ServiceContext.from_defaults( embed_model=embedding_llm, # prompt_helper=prompt_helper ) return service_context def make_index(file): service_context = context() documents = [] docs = load(file) for doc in docs: documents.append(Document( text=doc[1], metadata={\"id\": doc[0]} )) index = VectorStoreIndex.from_documents(documents, service_context=service_context) index.storage_context.persist(persist_dir=store_dir) return index ","date":"2023-12-16","objectID":"/text-similarity/:0:2","tags":["semantic search","embedding"],"title":"快速搭建文本语义搜索","uri":"/text-similarity/"},{"categories":["AI","实现","系统"],"content":"查询 我们需要先把索引加载到内存中，然后查出相似度最高的 3 个文案 def query(q): service_context = context() storage_context = StorageContext.from_defaults(persist_dir=store_dir) # load index index = load_index_from_storage(storage_context, service_context=service_context) retriever = VectorIndexRetriever( index=index, similarity_top_k=3, ) res = retriever.retrieve(q) for node in res: print(node.score) print(node.text) print(node.metadata.get(\"id\")) 结果示例： from redbook_notes.similarity.service_context import make_index file = \"/Users/wanglinxiao/Desktop/文案提需.csv\" make_index(file) Out[4]: \u003cllama_index.indices.vector_store.base.VectorStoreIndex at 0x1695375e0\u003e from redbook_notes.similarity.service_context import query q = \"羊肉推荐菜\" query(q) 0.8669767534968357 xx小酒馆——推荐菜文案： 【羊肉汤锅】 暖身汤力荐，领鲜三十年。慢火足时酝酿，开锅满室浓香，除了爱无添加。汤色乳白，汤味鲜香，羊肉无膻，香醇浓厚。人生百味，羊汤抚慰 【现烤羊腿】 技法炉火纯青，方得羊香四溢。千里挑一羊腿，传统炭火炙烤，披上油亮新装。外焦香酥脆，内鲜嫩多汁，撕着吃才够味！ 【火焰鱼羊鲜】 双鲜论剑，唇齿流连。烈烈火焰之上，锡纸一方天地，鱼与羊尽释本味。鱼肉吸收羊肉浓香，羊肉吸收鱼肉鲜甜，尽释“鲜”字奥义。 570781 0.862087523108285 新鲜事： 寻欢主推•重磅食材 本店当家花旦，牛羊大有来头 澳洲谷饲牛肉，脂香浓郁，鲜嫩多汁 天然草原羊肉，口感细腻，味美无膻 缤纷串串尽情开涮，从这「娌」开启温暖冬日！ 571812 0.8364552439695493 金都城——推荐菜文案： 【黄金脆皮猪手】 食指大动，肉控特供。寻味三峡库区，上乘巴东黑猪，蒸卤炸三部曲，十几小时工序，蜕变喷香油亮。咔滋咔滋入嘴，内里多汁Q弹，滋味妙不可言。 【酥不腻烤鸭】 金黄酥脆，京城至味。烹鸭自有金牌法则，×道工序换得焦糖鸭色。皮质酥脆入魂，鸭肉细嫩多汁， 味入肌理，带来酥而不腻的舌尖体验。 【石锅蝶鱼头】 锅气食足，心胃满足。水越深鱼越鲜，拾味深海海域，石锅慢入味，锁住丰腴鲜美，开盖满室鱼香。裹满汤汁入口，滑嫩味厚，回味绵长。 【金都特色脆皮肉丁】 金字招牌，下饭好菜。黄金六两松板肉，一头猪仅产半斤，辅以秘法烹制，释放无尽鲜美，口感层层递进。物以稀为贵，唇齿流连回味。 【梅干菜红烧肉】 人间尤物，味蕾倾慕。灵魂CP同烹，呈现酱红色泽。食之软烂醇香，口感肥而不腻。梅干菜增添清新气息，乍吃无波澜，再品回味长。 【酥皮虾】 热卖风味，快乐来会。刚捞出来的鲜虾，热油磨炼金皇甲。一口咔滋脆，一口Q弹爽，吮指回味，老少皆宜。 572779 可以看到最匹配的结果是我们示例数据中的第三段文案，内容还是比较匹配的。 ","date":"2023-12-16","objectID":"/text-similarity/:0:3","tags":["semantic search","embedding"],"title":"快速搭建文本语义搜索","uri":"/text-similarity/"},{"categories":["AI","实现","系统"],"content":"Redis Vector Search 上面的方案有两个问题 索引持久化到了文件，对于容器化服务，发版之前我们需要把索引文件先上传，等待新容器启动后需要下载索引文件。整个过程容易出错和耗时，可用性不高； 当文本量巨大不光需要更大的内存，更显著的问题是查询性能会下降的很厉害 为了解决上面的问题我们需要引入向量数据库，这里我们使用了 redis-search 来存储和查询 embedding。 创建索引 def new_make_index(file): service_context = context() documents = [] docs = load(file) for doc in docs: documents.append(Document( text=doc[1], metadata={\"id\": doc[0]} )) vector_store = RedisVectorStore( index_name=\"vector_index\", index_prefix=\"zh\", redis_url=\"redis://localhost:6379\", # Default overwrite=True, ) # create storage context storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex.from_documents(documents, service_context=service_context, storage_context=storage_context) return index 查询 def new_query(d): service_context = context() vector_store = RedisVectorStore( index_name=\"vector_index\", index_prefix=\"zh\", redis_url=\"redis://localhost:6379\", # Default overwrite=True, ) index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context) retriever = VectorIndexRetriever( index=index, similarity_top_k=3, ) res = retriever.retrieve(d) for node in res: print(node.score) print(node.text) print(node.metadata.get(\"id\")) 结果示例 from redbook_notes.similarity.service_context import new_make_index file = \"/Users/wanglinxiao/Desktop/文案提需.csv\" new_make_index(file) Out[4]: \u003cllama_index.indices.vector_store.base.VectorStoreIndex at 0x16f841a00\u003e from redbook_notes.similarity.service_context import new_query q = \"羊肉推荐菜\" new_query(q) 0.866977393627 太和小酒馆——推荐菜文案： 【羊肉汤锅】 暖身汤力荐，领鲜三十年。慢火足时酝酿，开锅满室浓香，除了爱无添加。汤色乳白，汤味鲜香，羊肉无膻，香醇浓厚。人生百味，羊汤抚慰 【现烤羊腿】 技法炉火纯青，方得羊香四溢。千里挑一羊腿，传统炭火炙烤，披上油亮新装。外焦香酥脆，内鲜嫩多汁，撕着吃才够味！ 【火焰鱼羊鲜】 双鲜论剑，唇齿流连。烈烈火焰之上，锡纸一方天地，鱼与羊尽释本味。鱼肉吸收羊肉浓香，羊肉吸收鱼肉鲜甜，尽释“鲜”字奥义。 570781 可以看到和上面的内存索引结果是一致的 更新索引 service_context = context() vector_store = RedisVectorStore( index_name=\"vector_index\", index_prefix=\"zh\", redis_url=\"redis://localhost:6379\", # Default overwrite=False, ) index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context) index.insert(Document(text=\"text\", metadata={\"id\": 123})) ","date":"2023-12-16","objectID":"/text-similarity/:0:4","tags":["semantic search","embedding"],"title":"快速搭建文本语义搜索","uri":"/text-similarity/"},{"categories":["AI","原理","基础"],"content":" transformer 已经成了当前深度学习领域最著名的网络架构，它是各大语言模型的基石。在这篇文章里我把自己在了解 transformer 的学习资料做了一个总结，一是加深体会二是希望帮助到其他想学习的同学。 我认为最 transformer 中最关键的也是最不容易懂的部分应该就是 multi-head attention, 所以这部分会重点介绍。在此基础上再去理解整个架构就会比较容易。 ","date":"2023-10-24","objectID":"/learn-transformer/:0:0","tags":["transformer","attention"],"title":"Transformer 结构学习总结","uri":"/learn-transformer/"},{"categories":["AI","原理","基础"],"content":"multi-head attention multi-head attention 是 transformer 关键组成部分，我们会重点理解这部分实现。下图是此部分的架构图 首先拆分 V：“values”，K：“keys”，Q：“queries”，然后线性层（图中的\"linear\"）来转换 “values”、“keys” 和 “queries”。接下来，计算注意力权重并重新加权 “values”，并对重新加权的 “values” 求和，然后拼接所得的总和。最后，将拼接的 “values” 传递到另一个线性层。scaled dot-product attention 就是如何具体计算这些注意力并重新加权 “values” 的问题。 “keys” 和 “queries” 相同时产生的注意力就是所谓的 “self attention”。s 我们拿这样一个句子 “Anthony Hopkins admired Michael Bay as a great director” 来作为计算 multi-head self-attention 的例子。在这个例子中，token 的数量是 9，每个 token 被编码为一个 512 维的嵌入向量。head 的数量是 8。在这种情况下，如下图所示，输入句子 “Anthony Hopkins admired Michael Bay as a great director” 被表示为 9 x 512 矩阵。首先将每个 token 拆分为 512/8=64 维，总共 8 个向量，如下图所示，输入矩阵被分为 8 个彩色块，都是 9 x 64 矩阵，但是每个矩阵都表达同一个句子。然后在 8 个 head 中独立计算输入句子的self attention，并根据注意力/(权重) 重新加权 “values”。之后，并将每个头加权后的 value 拼接起来。在重新加权 value 之后，每个彩色块（矩阵）的大小也不会改变。每个 head 都会比较每个子空间的 “queries” 和 “keys”。如果一个 Transformer 模型有 4 层，具有 8 头多头注意力，那么它的编码器至少有 4 x 8 = 32 个头，因此编码器学习输入的 token 在 32 种子空间上的关系。 “values” 计算过程 首先 每个 $$[ \\cdots ]$$ 表示一个 token，实际中通常是一个嵌入向量。下图展示了 “Michael” 作为 query 计算 “values” 的过程。query 与\"keys\" (即输入句子) 进行比较，得到了注意力 (权重) 的直方图，权重之和为 1。使用刚刚计算出来的权重重新加权 “values”（还是输入句子），并对 “values” 求和。 假设与 “query” token “Michael” 相比，“keys” token “Anthony”、“Hopkins”、“admired”、“Michael”、“Bay”、“as”、“a”、“great\"和\"director”。分别为 0.06、0.09、0.05、0.25、0.18、0.06、0.09、0.06、0.15。在这种情况下，重新加权的 token 之和为 0.06\"Anthony\" + 0.09\"Hopkins\" + 0.05\"admired\" + 0.25\"Michael\" + 0.18\"Bay\" + 0.06\"as\" + 0.09\"a\" + 0.06\"great\" 0.15\"director\"，这个数字就是我们实际使用的数字。 在实际上上面的 token 就是向量，所以结果就是重新加权后的向量。 对所有 “query” 重复此过程。如下图所示，会得到 9 对重新加权的 “values” 的总和，因为使用了输入句子 “Anthony Hopkins admired Michael Bay as a great director” 的每个 token。作为 “query”。将重新加权的 “values” 的总和拼接在一起，如下图紫色矩阵所示，这就是 multi-head attention中的 一个 head 的输出。 Scaled-dot product 为了计算多头注意力，如上所示我们有 8 对 “queries”、“keys” 和 “values”，在第一部分的图中用 8 种不同的颜色展示了它们。我们在 8 个不同的 head 中独立计算注意力并重新加权 “values”，并且在每个 head 中，重新加权的 “values” 是用这个非常简单的 scaled dot-produc 公式计算的： $$Attention(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) =softmax(\\frac{\\boldsymbol{Q} \\boldsymbol{K} ^T}{\\sqrt{d}_k})\\boldsymbol{V}$$ 我们拿上面 蓝色 head 计算 scaled dot-product来举例。 下图左侧是 Transformer 原论文的一张图，解释了 multi-head attention 的一个 head。输入句子分为 8 个矩阵块，然后将这些块独立地放入 8 个头中。在一个 head 中，通过三个不同的全连接层（下图中的\"Linear\"）转换输入矩阵，并准备三个矩阵 Q, K, V，分别是 “queries”、“keys” 和 “values”。 如下图所示，计算 $$Attention(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V})$$ 实际上只是将三个相同大小的矩阵相乘（不过只有 K 被转置）。生成的 9 x 64 矩阵就是 head 的输出。 计算方式如下图所示。softmax 函数对重新缩放后的乘积 $$\\frac{\\boldsymbol{Q} \\boldsymbol{K} ^T}{\\sqrt{d}_k}$$ 的每一行进行正则化，得到的 9 x9 矩阵可以理解为是一种自注意力热图。 将每个\"quey\"与\"keys\"进行比较的过程是通过向量和矩阵的简单相乘来完成的，如下图所示。可以得到每个\"query\"的注意力直方图，得到的 9 维向量是注意力 (权重) 的列表，也就是下图中蓝色圆圈的列表。这意味着，在 Transformer 模型中，通过计算内积来比较\"query\"和\"key\"。通过用 $$\\sqrt{d_k}$$ 除以重新缩放向量并使用 softmax 函数对它们进行正则化后，将这些向量堆叠起来，堆叠的向量就是注意力的热图。 这就是上述蓝色 head 计算 attention(权重) 的过程。每个 head 同时执行安全相同的过程就实现了 mutl-head attention 计算并行。然后我们把所有 head 的输出拼接再传入线性层。 在了解完 multi-head attention 的计算过程，我们再来从整体看一下整个 transformer 架构的几大模块。 ","date":"2023-10-24","objectID":"/learn-transformer/:0:1","tags":["transformer","attention"],"title":"Transformer 结构学习总结","uri":"/learn-transformer/"},{"categories":["AI","原理","基础"],"content":"encoder-decoder 结构 编码部分（encoders）由多层编码器 (Encoder) 组成（Transformer 论文中使用的是 6 层编码器，这里的层数 6 并不是固定的，可以根据实验效果来修改层数）。同理，解码部分（decoders）也是由多层的解码器 (Decoder) 组成（论文里也使用了 6 层解码器）。每层编码器网络结构是一样的，每层解码器网络结构也是一样的。不同层编码器和解码器网络结构不共享参数。 encoder 编码部分的输入文本序列经过输入处理之后得到了一个向量序列，这个向量序列将被送入第 1 层编码器，第 1 层编码器输出的同样是一个向量序列，再接着送入下一层编码器：第 1 层编码器的输入是融合位置向量的词向量，更上层编码器的输入则是上一层编码器的输出。 下图展示了向量序列在单层 encoder 中的流动：融合位置信息的词向量进入 self-attention 层，self-attention 的输出每个位置的向量再输入 FFN 神经网络得到每个位置的新向量。 残差连接 计算得到 self-attention 的输出向量后，单层 encode r 里后续还有两个重要的操作：残差链接、标准化。 编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization），如下图所示。 decoder 编码器一般有多层，第一个编码器的输入是一个序列文本，最后一个编码器输出是一组序列向量，这组序列向量会作为解码器的 K、V 输入，其中 K=V=解码器输出的序列向量表示。这些注意力向量将会输入到每个解码器的 Encoder-Decoder Attention 层，这有助于解码器把注意力集中到输入序列的合适位置，如下图所示。 解码器中的 Self Attention 层，和编码器中的 Self Attention 层的区别： 在解码器里，Self Attention 层只允许关注到输出序列中早于当前位置之前的单词。具体做法是：在 Self Attention 分数经过 Softmax 层之前，屏蔽当前位置之后的那些位置（将 attention score 设置成-inf）。 解码器 Attention 层是使用前一层的输出来构造 Query 矩阵，而 Key 矩阵和 Value 矩阵来自于编码器最终的输出。 线性层和 softmax Decoder 最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是线性层和 softmax 完成的。 线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更大的向量，这个向量称为 logits 向量：假设我们的模型有 10000 个英语单词（模型的输出词汇表），此 logits 向量便会有 10000 个数字，每个数表示一个单词的分数。 然后，Softmax 层会把这些分数转换为概率（把所有的分数转换为正数，并且加起来等于 1）。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词。 现在再来看这张我们经常看到的图，脑子里应该已经有了比较清晰的概念和认识，那么这篇文章的目的就达到了。 ","date":"2023-10-24","objectID":"/learn-transformer/:0:2","tags":["transformer","attention"],"title":"Transformer 结构学习总结","uri":"/learn-transformer/"},{"categories":["AI","infra","实现"],"content":"背景 随着 LLM 的日益强大，我们可以借助 LLM 来提高多方面的生产效率。很多的应用场景我们就不在这里展开，其中一方面就是用来给程序员提效，比如大名鼎鼎的 github Copilot。在这篇文章里我们就介绍一下借助 LLM 提效开发的一个点：Code Review. LLM 能 revview 代码这个大家肯定都知道，但是选取哪个模型来实现呢？ 可以说选择比较多如开源模型 StarCoder，CodeLLama, WizardCoder 等是代码方面评测结果比较优秀的存在，再比如闭源方案 GPT4, Claude 等综合能力超强的大模型。 在经过了多种实际 case 的实验我们最终选了 Claude 作为 Code Reveiw 的底座。 首先从 Codew Reveiw 效果方面来看 GPT4 \u003e Claude \u003e Wizardoder \u003e CodeLLama，其中 Claude 相比 GPT4 代码理解能力并不逊色多少； 从成本方面来看 GPT4 API 价格明显高于 Claude，而开源模型的显卡成本也不低 (至少需要一张 V100 才能把模型跑起来)。 再者 Claude 除了它超强的综合能力，超长的 context window(可以达到 100k) 也是非常强力的点，对于 Code Review 来说大的 context window 十分重要。 下面就介绍一下我们如何高效率低成本地实现 Code Review。 ","date":"2023-08-20","objectID":"/llm-code-review/:1:0","tags":["code reiviw","llm","claude","实践"],"title":"AI 助力 Code Review 的高效方案","uri":"/llm-code-review/"},{"categories":["AI","infra","实现"],"content":"方案 ","date":"2023-08-20","objectID":"/llm-code-review/:2:0","tags":["code reiviw","llm","claude","实践"],"title":"AI 助力 Code Review 的高效方案","uri":"/llm-code-review/"},{"categories":["AI","infra","实现"],"content":"Slack Claude 企业使用 Claude 首先可以通过 Claude API，但是需要申请，这个申请审核时间可能持续比较久。在我们做该方案时 API 暂时还没申请下来。所以我们把解决方案转向了 Slack 中的 Claude bot。 在 Slack 中我们首先在 App 中把 Claude 添加进来，然后创建 channel, 并把 Claude 加入到 channel。此后，就可以像普通同事一样 @Claude 进行对话。 我们可以借助 slack sdk 来实现于 Claude 的交互，从而使用 Claude 辅助 Code Review 就有了明确的实现方案。 总体来说实现方案如下： ","date":"2023-08-20","objectID":"/llm-code-review/:2:1","tags":["code reiviw","llm","claude","实践"],"title":"AI 助力 Code Review 的高效方案","uri":"/llm-code-review/"},{"categories":["AI","infra","实现"],"content":"步骤 MR 处理 开发在 gitlab 提交 Merge Request, 通过项目配置好的 webhook 触发请求把 MR 信息发给 bot service（bot service 用来处理各种 webhook）； bot service 拿到 MR 后通过 API 获取 MR 的 diffs 和 commit message # 获取 mr diffs url = f\"{settings.GITLAB_API_URL}/projects/{pr.project_id}/merge_requests/{pr.iid}/diffs\" headers = {\"PRIVATE-TOKEN\": settings.GITLAB_TOKEN} response = requests.get(url, headers=headers) if response.status_code != 200: cls.make_new_thread(pr, \"获取 mr diffs 失败\") return # 获取 mr commits messages url = f\"{settings.GITLAB_API_URL}/projects/{pr.project_id}/merge_requests/{pr.iid}/commits\" response = requests.get(url, headers=headers) 处理 diffs 中可能包含的敏感信息 这里可以有很多的实现方案，这里我们推荐一个库 detect-secrets再搭配一些正则表达式，可以来过滤敏感信息。 组装 Prompt 不同的项目差别可能比较大，我们可能需要根据项目不同提供给模型不同的上下文来达到更好的 review 效果。在这里我们支持了项目模版自定义，每个 gitlab repository 通过 wiki 可以配置自己项目的 Prompt 模版。 bot service 在组装 Prompt 的时候会优先使用项目的配置模版。我们的默认模版如下 你将扮演一个资深的软件程序员进行代码审查，严格按照以下要求审查提供的合并请求（Merge Request, MR）内容。给出审查结果： 1. 前提: 请务必用中文回复； 2. 总结：请简洁准确地总结MR的改动内容； 3. 问题与优化点识别：请识别MR中存在的问题和需要优化的地方，包括但不限于代码逻辑、代码风格、命名规范、拼写错误、性能和错误处理等； 4. 优化建议：针对识别的问题和优化点，给出具体可执行的优化建议, 并详细说明为什么要这么改, 如果建议涉及代码改动，请直接提供优化后的代码； 5. Commit Message 审查：请检查 commit message 是否准确反映了MR的改动内容，如果存在不一致，必须指出； 6. MR原子性的审查：判断一个MR里是否包含了过多不同类型的改动，或者一个MR里包含了多个commit，如果有请指出。 请确保审查结果的准确性和专业性。 MR 内容如下: {content} 调用 LLM service 获取 Review 结果 对话历史 slcak thread 可以保存完整的对话历史，所以我们不需要自己保存保存对话，这是一个十分方便的特性。只要我们记住每个 thread id，那么就能进行完整的对话。在 MR review 中我们也就可以持续和模型对话，来更好的 review 代码； 如果 MR 首次触发 review, 我们会创建并缓存 thread ts, 如果非首次对话那么就获取已有 thread ts继续对话。示例代码如下 key = f\"claude_{body.user}\" prompt = f\"\u003c@{CLAUDE_BOT_ID}\u003e {body.prompt} \" thread_ts = None value = await app.state.redis.get(key) if value: thread_ts = value.split(\"_\")[0] last_ts = await client.chat(prompt, thread_ts=thread_ts) if not value: await app.state.redis.setex(name=key, time=3600, value=last_ts) thread_ts = last_ts 超长文本 我们 MR 往往包含的文本非常多，而 slack server 对普通的消息提交有长度限制，所以我们这里把超长的消息体分成了多段组装成 blocks 提交。 if not self.CHANNEL_ID: raise Exception(\"Channel not found.\") if len(text) \u003c= self.MAX_LEN: resp = await self.chat_postMessage(channel=self.CHANNEL_ID, text=text, thread_ts=thread_ts) # slack server 会自动切分超长消息，为了解决此问题使用 blocks 传递消息 else: texts = split_message(text, self.MAX_LEN) blocks = [] for t in texts: blocks.append({ \"type\": \"section\", \"text\": { \"type\": \"mrkdwn\", \"text\": t, } }) resp = await self.chat_postMessage(channel=self.CHANNEL_ID, blocks=blocks, thread_ts=thread_ts) 轮询结果 Claude 在 slack 中流式回复，我们这里需要等到回复完整了才能返回结果。所以需要轮询回复状态，并且得设置一定的超时策略防止异常 case for _ in range(300): try: resp = await self.conversations_replies(channel=self.CHANNEL_ID, ts=thread_ts, oldest=last_ts, limit=1) msg = [msg[\"text\"] for msg in resp[\"messages\"] if msg[\"user\"] == CLAUDE_BOT_ID] if msg and not msg[-1].endswith(\"Typing…_\"): return unescape_text(msg[-1]) except (SlackApiError, KeyError) as e: print(f\"Get reply error: {e}\") await asyncio.sleep(1) 结果 comment 当 bot service 拿到返回的 review 结果后需要以 comment 的形式呈现到 gitlab 的 comment 列表中 url = f\"{settings.GITLAB_API_URL}projects/{pr.project_id}/merge_requests/{pr.iid}/discussions\" headers = {\"PRIVATE-TOKEN\": settings.GITLAB_TOKEN, \"Content-Type\": \"application/json\"} payload = json.dumps( { \"body\": content, } ) response = requests.post(url, headers=headers, data=payload) if response.status_code \u003e= 400: raise CodeBaseException(\"make mr thread failed\") 基于模型 review 结果我们可以继续在 comment 列表和模型对话，把问题讨论清楚。 ","date":"2023-08-20","objectID":"/llm-code-review/:2:2","tags":["code reiviw","llm","claude","实践"],"title":"AI 助力 Code Review 的高效方案","uri":"/llm-code-review/"},{"categories":["AI","infra","实现"],"content":"效果呈现 下面展示了某次 review 的 case 在这篇文章里介绍了我们借助 LLM 辅助 Code Review 的实现过程，重要的细节和代码实现我们都有提到，希望能给需要的同学带来有指导性的参考。 在未来的工作中 Code Review 随着 LLM 的进步，我们可以随时切换更优秀的底座，并且把 review 结果精确到行。 ","date":"2023-08-20","objectID":"/llm-code-review/:2:3","tags":["code reiviw","llm","claude","实践"],"title":"AI 助力 Code Review 的高效方案","uri":"/llm-code-review/"},{"categories":["infra","大数据","存储"],"content":"Hudi 作为最热的数据湖技术框架之一，用于构建具有增量数据处理管道的流式数据湖。其核心的能力包括对象存储上数据行级别的快速更新和删除，增量查询 (Incremental queries,Time Travel)，小文件管理和查询优化 (Clustering,Compactions,Built-in metadata)，ACID 和并发写支持。Hudi 不是一个 Server，它本身不存储数据，也不是计算引擎，不提供计算能力。其数据存储在 S3(也支持其它对象存储和 HDFS)，Hudi 来决定数据以什么格式存储在 S3(Parquet,Avro,…), 什么方式组织数据能让实时摄入的同时支持更新，删除，ACID 等特性。Hudi 通过 Spark，Flink 计算引擎提供数据写入，计算能力，同时也提供与 OLAP 引擎集成的能力，使 OLAP 引擎能够查询 Hudi 表。从使用上看 Hudi 就是一个 JAR 包，启动 Spark, Flink 作业的时候带上这个 JAR 包即可。Amazon EMR 上的 Spark，Flink，Presto，Trino 原生集成 Hudi, 且 EMR 的 Runtime 在 Spark，Presto 引擎上相比开源有 2 倍以上的性能提升。在多库多表的场景下 (比如：百级别库表)，当我们需要将数据库 (mysql,postgres,sqlserver,oracle,mongodb 等) 中的数据通过 CDC 的方式以分钟级别 (1minute+) 延迟写入 Hudi，并以增量查询的方式构建数仓层次，对数据进行实时高效的查询分析时。我们要解决三个问题： 一. 如何使用统一的代码完成百级别库表 CDC 数据并行写入 Hudi，降低开发维护成本。 二. 源端 Schema 变更如何同步到 Hudi 表。 三. 使用 Hudi 增量查询构建数仓层次比如 ODS-\u003eDWD-\u003eDWS(各层均是 Hudi 表)，DWS 层的增量聚合如何实现。 这篇文章里我们的选型方案是：使用 Flink CDC DataStream API(非 SQL) 先将 CDC 数据写入 Kafka，而不是直接通过 Flink SQL 写入到 Hudi 表，主要原因如下 第一，在多库表且 Schema 不同的场景下，使用 SQL 的方式会在源端建立多个 CDC 同步线程，对源端造成压力，影响同步性能。 第二，没有 kafka 做 CDC 数据上下游的解耦和数据缓冲层，下游的多端消费和数据回溯比较困难。 CDC 数据写入到 kafka 后，推荐使用 Spark Structured Streaming DataFrame API 或者 Flink StatementSet 封装多库表的写入逻辑，但如果需要源端 Schema 变更自动同步到 Hudi 表，使用 Spark Structured Streaming DataFrame API 实现更为简单，使用 Flink 则需要基于 HoodieFlinkStreamer 做额外的开发。Hudi 增量 ETL 在 DWS 层需要数据聚合的场景的下，可以通过 Flink Streaming Read 将 Hudi 作为一个无界流，通过 Flink 计算引擎完成数据实时聚合计算写入到 Hudi 表。 ","date":"2023-03-19","objectID":"/cdc-stream/:0:0","tags":["实时数仓","flink cdc","hudi","spark"],"title":"实时数据入湖的选型和实践","uri":"/cdc-stream/"},{"categories":["infra","大数据","存储"],"content":"架构设计和方案 CDC 数据实时写入 kafka 图中标号 1, 2 是将数据库中的数据通过 CDC 方式实时发送到 kafka。flink-cdc-connectors 是当前比较流行的 CDC 开源工具。它内嵌 debezium 引擎，支持多种数据源，对于 MySQL 支持 Batch 阶段 (全量同步阶段) 并行，无锁，Checkpoint(可以从失败位置恢复，无需重新读取，对大表友好)。支持 Flink SQL API 和 DataStream API，这里需要注意的是如果使用 SQL API 对于库中的每张表都会单独创建一个链接，独立的线程去执行 binlog dump。如果需要同步的表比较多，会对源端产生较大的压力。在需要整库同步表非常多的场景下，应该使用 DataStream API 写代码的方式只建一个 binlog dump 同步所有需要的库表。另一种场景是如果只同步分库分表的数据，比如 user 表做了分库，分表，其表 Schema 都是一样的，Flink CDC 的 SQL API 支持正则匹配多个库表，这时使用 SQL API 同步依然只会建立一个 binlog dump 线程。需要说明的是通过 Flink CDC 可以直接将数据 Sink 到 Hudi, 中间无需 kaffka，但考虑到上下游的解耦，数据的回溯，多业务端消费，多表管理维护，依然建议 CDC 数据先到 kafka，下游再从 kafka 消费写入 Hudi。 Spark Structured Streaming 多库表并行写 Hudi 及 Schema 变更 图中标号 4，CDC 数据到了 kafka 之后，可以通过 Spark/Flink 计算引擎消费数据写入到 Hudi 表，我们把这一层我们称之为 ODS 层。无论 Spark 还是 Flink 都可以做到数据 ODS 层的数据落地，使用哪一个我们需要综合考量，这里阐述一些相对重要的点。首先对于 Spark 引擎，我们一定是使用 Spark Structured Streaming 消费 kafka 写入 Hudi，由于可以使用 DataFrame API 写 Hudi, 因此在 Spark 中可以方便的实现消费 CDC Topic 并根据其每条数据中的元信息字段 (数据库名称，表名称等) 在单作业内分流写入不同的 Hudi 表，封装多表并行写入逻辑，一个 Job 即可实现整库多表同步的逻辑。 CDC 数据中是带着 I(insert)、U(update)、D(delete) 信息的，不同的 CDC 工具数据格式不同，但要表达的含义是一致的。使用 Spark 写入 Hudi 我们主要关注 U、D 信息，数据带着 U 信息表示该条数据是一个更新操作，对于 Hudi 而言只要设定源表的主键为 Hudi 的 recordKey，同时根据需求场景设定 precombineKey 即可。这里对 precombineKey 做一个说明，它表示的是当数据需要更新时 (recordKey 相同), 默认选择两条数据中 precombineKey 的大保留在 Hudi 中。其实 Hudi 有非常灵活的 Payload 机制，通过参数 hoodie.datasource.write.payload.class 可以选择不同的 Payload 实现，比如 Partial Update(部分字段更新) 的 Payload 实现 OverwriteNonDefaultsWithLatestAvroPayload，也可以自定义 Payload 实现类，它核心要做的就是如何根据 precombineKey 指定的字段更新数据。所以对于 CDC 数据 Sink Hudi 而言，我们需要保证上游的消息顺序，只要我们表中有能判断哪条数据是最新的数据的字段即可，那这个字段在 MySQL 中往往我们设计成数据更新时间 modify_time timestamp DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP。如果没有类似字段，建议定义设计规范加上这个字段，否则就必须保证数据有序 (这会给架构设计和性能带来更多的阻力)，不然数据在 Hudi 中 Updata 的结果可能就是错的。对于带着 D 信息的数据，它表示这条数据在源端被删除，Hudi 是提供删除能力的，其中一种方式是当一条数据中包含 _hoodie_is_deleted 字段，且值为 true 是，Hudi 会自动删除此条数据，这在 Spark Structured Streaming 代码中很容易实现，只需在 map 操作实现添加一个字段且当数据中包含 D 信息设定字段值为 true 即可。 Flink StatementSet 多库表 CDC 并行写 Hudi 对于使用 Flink 引擎消费 kafka 中的 CDC 数据落地到 ODS 层 Hudi 表，如果想要在一个 JOB 实现整库多张表的同步，Flink StatementSet 来实现通过一个 Kafka 的 CDC Source 表，根据元信息选择库表 Sink 到 Hudi 中。但这里需要注意的是由于 Flink 和 Hudi 集成，是以 SQL 方式先创建表，再执行 Insert 语句写入到该表中的，如果需要同步的表有上百之多，封装一个自动化的逻辑能够减轻我们的工作，你会发现 SQL 方式写入 Hudi 虽然对于单表写入使用上很方便，不用编程只需要写 SQL 即可，但也带来了一些限制，由于写入 Hudi 时是通过 SQL 先建表，Schema 在建表时已将定义，如果源端 Schema 变更，通过 SQL 方式是很难实现下游 Hudi 表 Schema 的自动变更的。虽然在 Hudi 的官网并未提供 Flink DataStream API 写入 Hudi 的例子，但 Flink 写入 Hudi 是可以通过 HoodieFlinkStreamer 以 DataStream API 的方式实现，在 Hudi 源码中可以找到。因此如果想要更加灵活简单的实现多表的同步，以及 Schema 的自动变更，需要自行参照 HoodieFlinkStreamer 代码以 DataStream API 的方式写 Hudi。对于 I,U,D 信息，Flink 的 debezium ,maxwell,canal format 会直接将消息解析 为 Flink 的 changelog 流，换句话说就是 Flink 会将 I,U,D 操作直接解析成 Flink 内部的数据结构 RowData，直接 Sink 到 Hudi 表即可，我们同样需要在 SQL 中设定 recordKey，precombineKey，也可以设定 Payload class 的不同实现类。 Flink Streaming Read 模式读 Hudi 实现 ODS 层聚合 图中标号 5，数据通过 Spark/Flink 落地到 ODS 层后，我们可能需要构建 DWD 和 DWS 层对数据做进一步的加工处理，（DWD 和 DWS 并非必须的，根据你的场景而定，你可以直接让 OLAP 引擎查询 ODS 层的 Hudi 表）我们希望能够使用到 Hudi 的增量查询能力，只查询变更的数据来做后续 DWD 和 DWS 的 ETL，这样能够加速构建同时减少资源消耗。对于 Spark 引擎，在 DWD 层如果仅仅是对数据做 map,fliter 等相关类型操作，是可以使用增量查询的，但如果 DWD 层的构建有 Join 操作，是无法通过增量查询实现的，只能全表 (或者分区) 扫描。DWS 层的构建如果聚合类型的操作没有去重，窗口类型的操作，只是 SUM, AVG，MIN, MAX 等类型的操作，可以通过增量查询之后和目标表做 Merge 实现，反之，只能全表 (或者分区) 扫描。对于 Flink 引擎来构建 DWD 和 DWS, 由于 Flink 支持 Hudi 表的 streaming read, 在 SQL 设定 read.streaming.enabled=true,changelog.enabled=true 等相关流式读取的参数即可。设定后 Flink 把 Hudi 表当做了一个无界的 changelog 流表，无论怎样做 ETL 都是支持的，Flink 会自身存储状态信息，整个 ETL 的链路是流式的。 OLAP 引擎查询 Hudi 表 图中标号 6, EMR Hive/Presto/Trino 都可以查询 Hudi 表，但需要注意的是不同引擎对于查询的支持是不同的，参见官网，这些引擎对于 Hudi 表只能查询，不能写入。关于 Schema 的自动变更，首先 Hudi 自身是支持 Schema Evolution，我们想要做到源端 Schema 变更自动同步到 Hudi 表，通过上文的描述，可以知道如果使用 Spark 引擎，可以通过 DataFrame API 操作数据，通过 from_json 动态生成 DataFrame，因此可以较为方便的实现自动添加列。如果使用 Flink 引擎上文已经说明想要自动实现 Schema 的变更，通过 HoodieFlinkStreamer 以 DataStream API 的方式实现 Hudi 写入的同时融入 Schema 变更的逻辑。 上述我们介绍了实时数据入湖的各个阶段以及各阶段采用的方案以及背后的原因。下面我们以一个实际的例子来实践上述方案。 ","date":"2023-03-19","objectID":"/cdc-stream/:0:1","tags":["实时数仓","flink cdc","hudi","spark"],"title":"实时数据入湖的选型和实践","uri":"/cdc-stream/"},{"categories":["infra","大数据","存储"],"content":"方案实践 实践中选择 RDS MySQL 作为数据源，Flink CDC DataStream API 同步库中的所有表到 Kafka，使用 Spark 引擎消费 Kafka 中 binlog 数据实现多表写入 ODS 层 Hudi，使用 Flink 引擎以 streaming read 的模式做 DWD 和 DWS 层的 Hudi 表构建。 环境信息 EMR 6.9.0 Hudi 0.12.1 Spark 3.3.0 Flink 1.15.2 MySQL 8.0.22 创建源表 在 MySQL 中创建 test_db 库及 user,product,user_order 三张表，插入样例数据，后续 CDC 先加载表中已有的数据，之后源添加新数据并修改表结构添加新字段，验证 Schema 变更自动同步到 Hudi 表。 -- create databases create database if not exists test_db default character set utf8mb4 collate utf8mb4_general_ci; use test_db; -- create user table drop table if exists user; create table if not exists user ( id int auto_increment primary key, name varchar(155) null, device_model varchar(155) null, email varchar(50) null, phone varchar(50) null, create_time timestamp default CURRENT_TIMESTAMP not null, modify_time timestamp default CURRENT_TIMESTAMP null on update CURRENT_TIMESTAMP )charset = utf8mb4; -- insert data insert into user(name,device_model,email,phone) values ('customer-01','dm-01','abc01@email.com','188776xxxxx'), ('customer-02','dm-02','abc02@email.com','166776xxxxx'); -- create product table drop table if exists product; create table if not exists product ( pid int not null primary key, pname varchar(155) null, pprice decimal(10,2) , create_time timestamp default CURRENT_TIMESTAMP not null, modify_time timestamp default CURRENT_TIMESTAMP null on update CURRENT_TIMESTAMP )charset = utf8mb4; -- insert data insert into product(pid,pname,pprice) values ('1','prodcut-001',125.12), ('2','prodcut-002',225.31); -- create order table drop table if exists user_order; create table if not exists user_order ( id int auto_increment primary key, oid varchar(155) not null, uid int , pid int , onum int , create_time timestamp default CURRENT_TIMESTAMP not null, modify_time timestamp default CURRENT_TIMESTAMP null on update CURRENT_TIMESTAMP )charset = utf8mb4; -- insert data insert into user_order(oid,uid,pid,onum) values ('o10001',1,1,100), ('o10002',1,2,30), ('o10001',2,1,22), ('o10002',2,2,16); Flink CDC 发送数据到 Kafka 示例代码片段 def main(args: Array[String]): Unit = { val env = StreamExecutionEnvironment.getExecutionEnvironment val params = Config.parseConfig(MySQLCDC, args) env.enableCheckpointing(params.checkpointInterval.toInt ` 1000) env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE) env.getCheckpointConfig.setMinPauseBetweenCheckpoints(500) env.getCheckpointConfig.setCheckpointTimeout(60000) env.getCheckpointConfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION) val rocksBackend: StateBackend = new RocksDBStateBackend(params.checkpointDir) env.setStateBackend(rocksBackend) env.fromSource(createCDCSource(params), WatermarkStrategy.noWatermarks(), \"mysql cdc source\") .addSink(createKafkaSink(params)).name(\"cdc sink kafka\") .setParallelism(params.parallel.toInt) env.execute(\"MySQL Binlog CDC\") } 打包和提交 flink 任务 # 创建topic, bs为 kafka broker地址 kafka-topics.sh --create ----bootstrap-server ${bs} --replication-factor 2 --partitions 8 --topic cdc_topic # 编译打包 mvn clean package -Dscope.type=provided -DskipTests # disalbe check-leaked-classloader sudo sed -i -e '$a\\classloader.check-leaked-classloader: false' /etc/flink/conf/flink-conf.yaml # 启动flink cdc 发送数据到Kafka sudo flink run -m yarn-cluster \\ -yjm 1024 -ytm 2048 -d \\ -ys 4 -p 8 \\ -c com.kezaihui.MySQLCDC \\ /home/hadoop/emr-flink-cdc-1.0-SNAPSHOT.jar \\ -b xxxxx.amazonaws.com:9092 \\ -t cdc_topic \\ -c s3://xxxxx/flink/checkpoint/ \\ -l 30 -h xxxxx.rds.amazonaws.com:3306 -u admin \\ -P admin123456 \\ -d test_db -T test_db.` \\ -p 4 \\ -e 5400-5408 下图是在 flink dashboard 上查看到任务状态 Spark 消费 CDC 数据整库同步 示例代码片段 val ds = df.selectExpr(\"CAST(value AS STRING)\").as[String] val query = ds .writeStream .queryName(\"debezium2hudi\") .option(\"checkpointLocation\", params.checkpointDir) // if set 0, as fast as possible .trigger(Trigger.ProcessingTime(params.trigger + \" seconds\")) .foreachBatch { (batchDF: Dataset[String], batchId: Long) =\u003e log.warn(\"current batch: \" + batchId.toString) val po","date":"2023-03-19","objectID":"/cdc-stream/:0:2","tags":["实时数仓","flink cdc","hudi","spark"],"title":"实时数据入湖的选型和实践","uri":"/cdc-stream/"},{"categories":["infra","原理","存储"],"content":"在当前的业务系统中我们会遇到多种多样的数据存储和读取场景。在对于大量数据(G以上级别)的存储和读取我们就基本会采用分布式文件系统比如 Amazon S3、Apache HDFS。目前为了系统的可用性和稳定性计算和存储往往是分开部署，在这样的背景下对于大数据的读写性能和兼容性就会成为很大的挑战。于是我们看到了一种解决方案 Alluxio。 ","date":"2022-12-22","objectID":"/alluxio-info/:0:0","tags":["alluxio","数据编排","大数据","机器学习"],"title":"数据编排技术-Alluxio 的原理介绍","uri":"/alluxio-info/"},{"categories":["infra","原理","存储"],"content":"Alluxio 简介 Alluxio 是世界上第一个面向基于云的数据分析和人工智能的开源的数据编排技术。 它为数据驱动型应用和存储系统构建了桥梁, 将数据从存储层移动到距离数据驱动型应用更近的位置从而能够更容易被访问。 这还使得应用程序能够通过一个公共接口连接到许多存储系统。 Alluxio内存至上的层次化架构使得数据的访问速度能比现有方案快几个数量级。 在大数据生态系统中，Alluxio 位于数据驱动框架或应用（如 Apache Spark、Presto、Tensorflow、Apache Hive 或 Apache Flink）和各种持久化存储系统（如 Amazon S3、Google Cloud Storage、OpenStack Swift、HDFS、IBM Cleversafe、EMC ECS、Ceph、NFS 、Minio和 Alibaba OSS）之间。 Alluxio 统一了存储在这些不同存储系统中的数据，为其上层数据驱动型应用提供统一的客户端 API 和全局命名空间。 ","date":"2022-12-22","objectID":"/alluxio-info/:0:1","tags":["alluxio","数据编排","大数据","机器学习"],"title":"数据编排技术-Alluxio 的原理介绍","uri":"/alluxio-info/"},{"categories":["infra","原理","存储"],"content":"Alluxio 重要特性 统一数据访问接口 Alluxio 通过挂载功能在不同的存储系统之间实现高效的数据管理。并且，透明命名在持久化这些对象到底层存储系统时可以保留这些对象的文件名和目录层次结构。如下图, 可将 HDFS、S3 的数据 mount 到 Alluxio 的随意目录下，这样就可以通过一个 API 访问不同的主流的持久化存储系统，这样也解决了存储系统的单一依赖性。 内存速度 I/O Alluxio 能够用作分布式共享缓存服务，这样与 Alluxio 通信的计算应用程序可以透明地缓存频繁访问的数据（尤其是从远程位置），以提供内存级 I/O 吞吐率。此外，Alluxio的层次化存储机制能够充分利用内存、固态硬盘或者磁盘，降低具有弹性扩张特性的数据驱动型应用的成本开销。 简化数据管理 Alluxio 提供对多数据源的单点访问。除了连接不同类型的数据源之外，Alluxio 还允许用户同时连接同一存储系统的不同版本，如多个版本的 HDFS，并且无需复杂的系统配置和管理。 应用程序部署简易 Alluxio 管理应用程序和文件或对象存储之间的通信，将应用程序的数据访问请求转换为底层存储接口的请求。Alluxio 与 Hadoop 兼容,现有的数据分析应用程序,如Spark 和 MapReduce 程序,无需更改任何代码就能在 Alluxio 上运行。 智能多层缓存 Alluxio 集群能够充当底层存储系统中数据的读写缓存。可配置自动优化数据放置策略，以实现跨内存和磁盘（SSD/HDD）的性能和可靠性。缓存对用户是透明的，使用缓冲来保持与持久存储的一致性 方便迁移可插拔 在容错方面，Alluxio 备份内存数据到底层存储系统。Alluxio 提供了通用接口以简化插入不同的底层存储系统。 数据共享 Alluxio可以帮助实现跨计算、作业间的数据快速复用和共享。 对于用户应用程序和大数据计算框架来说，Alluxio 存储通常与计算框架并置。这种部署方式使 Alluxio 可以提供快速存储，促进作业之间的数据共享，无论它们是否在同一计算 ","date":"2022-12-22","objectID":"/alluxio-info/:0:2","tags":["alluxio","数据编排","大数据","机器学习"],"title":"数据编排技术-Alluxio 的原理介绍","uri":"/alluxio-info/"},{"categories":["infra","原理","存储"],"content":"Alluxio 架构 Alluxio 由 master、worker 、client 组成； master 支持1主多从的高可用架构，master 之前通过 raft 协议或 zookeeper 进行选主 主 master 用于对外提供 RPC 服务和管理全局的元数据。这里面包含文件系统元数据（文件系统节点树）、数据块元数据（数据块位置）、以及 worker 元数据（存储空间等） 备用 master 读取主 master 写入的 journa 日志，以保持与主 master 的状态同步。它们会对 journal 日志写入检查点，用于快速恢复。它们不处理来自 Alluxio 组件的任何请求 worker 会定期向 master上报心跳 worker 主要是管理存储资源和底层文件系统的操作，将远程文件存储到本地的内存/SSD/HDD 中 硬件资源是有限的，worker 还需要根据策略，负责多级智能缓存，数据淘汰、数据过期删除 Client 在应用侧，负责向 master 请求文件的元信息，然后从 worker 中取回文件数据；Client 不会直接访问远程存储 ","date":"2022-12-22","objectID":"/alluxio-info/:0:3","tags":["alluxio","数据编排","大数据","机器学习"],"title":"数据编排技术-Alluxio 的原理介绍","uri":"/alluxio-info/"},{"categories":["infra","原理","存储"],"content":"数据流 Alluxio 与计算集群部署在一起，持久化存储系统可以为远程存储系统或云存储。在底层存储和计算集群间，Alluxio是作为一个缓存层存在的。 读 本地缓存命中 本地缓存命中发生在请求数据位于本地 Alluxio worker。举例说明，如果一个应用通过 Alluxio client 请求数据，client 向 Alluxio master 请求数据所在的worker。如果数据在本地可用，Alluxio client 使用“短路”读取来绕过 Alluxio worker，并直接通过本地文件系统读取文件。短路读取避免通过TCP套接字传输数据，并提供数据的直接访问。 还要注意，Alluxio 除了内存之外还可以管理其他存储介质(例如SSD、HDD)，因此本地数据访问速度可能会因本地存储介质的不同而有所不同 远程缓存命中 当请求的数据存储在 Alluxio 中，而不是存储在 client 的本地 worker 上时，client 将对具有数据的 worker 进行远程读取。client 完成读取后，会要求本地的worker（如果存在）创建一个copy，这样以后读取的时候可以在本地读取相同的数据。远程缓存击中提供了网络级别速度的数据读取。Alluxio 优先从远程 worker 读取数据，而不是从底层存储，因为 Alluxio worker 间的速度一般会快过 Alluxio workers 和底层存储的速度。 缓存 miss 如果数据在 Alluxio 中找不到，则会发生缓存丢失，应用将不得不从底层存储读取数据。Alluxio client 会将数据读取请求委托给 worker（有限本地worker）。这个worker 会从底层存储读取数据并缓存。缓存丢失通常会导致最大的延迟，因为数据必须从底层存储获取。 当 client 只读取块的一部分或不按照顺序读取块时，client 将指示 worker 异步缓存整个块。异步缓存不会阻塞 client，但是如果 Alluxio 和底层存储系统之间的网络带宽是瓶颈，那么异步缓存仍然可能影响性能。 图中绿色示意为命中本地缓存，蓝色为命中远程缓存，红色为缓存 miss 写 用户可以通过选择不同的写类型来配置应该如何写数据。写类型可以通过Alluxio API设置，也可以通过在客户机中配置属性Alluxio .user.file.writetype.default来设置。 写缓存 当写类型设置为 MUST_CACHE，Alluxio client 将数据写入本地 Alluxio worker，而不会写入到底层存储。如果“短路”写可用，Alluxio client 直接写入到本地 RAM 的文件，绕过 Alluxio worker，避免网络传输。由于数据没有持久存储在 under storage 中，因此如果机器崩溃或需要释放数据以进行更新的写操作，数据可能会丢失。当可以容忍数据丢失时，MUST_CACHE 设置对于写临时数据非常有用。 同步持久化写 使用 CACHE_THROUGH 写类型，数据被同步地写到一个Alluxio worker 和底层存储。Alluxio client 将写操作委托给本地 worker，而 worker 同时将对本地内存和底层存储进行写操作。由于底层存储的写入速度通常比本地存储慢，所以 client 的写入速度将与底层存储的速度相匹配。当需要数据持久化时，建议使用 CACHE_THROUGH 写类型。在本地还存了一份副本，以便可以直接从本地内存中读取数据。 异步持久化写 Alluxio 提供了一个叫做ASYNC_THROUGH 的写类型。数据被同步地写入到一个 Alluxio worker，并异步地写入到底层存储。ASYNC_THROUGH 可以在持久化数据的同时以内存速度提供数据写入。 这张图示意了写缓存的流程，绿色示意为 bypass 机制，黑色和蓝色线表示数据缓存到 worker 这张图展示了持久化写的过程 在这篇文章里我们介绍了 Alluxio 的基本原理，知道了 Alluxio 是什么东西。在后面的文章里我们再来介绍 Alluxio 在实际业务中的使用。 ","date":"2022-12-22","objectID":"/alluxio-info/:0:4","tags":["alluxio","数据编排","大数据","机器学习"],"title":"数据编排技术-Alluxio 的原理介绍","uri":"/alluxio-info/"},{"categories":["golang","service","实现"],"content":"系统自适应限流算法 BBR 的介绍和使用","date":"2022-11-25","objectID":"/limit-bbr/","tags":["限流","bbr","高可用"],"title":"系统自适应限流算法-BBR","uri":"/limit-bbr/"},{"categories":["golang","service","实现"],"content":"在 Golang 服务中我们经常会选择用官方库自带的令牌桶算法来实现服务实例级别的限流。 使用示例如下 package main import ( \"context\" \"log\" \"time\" \"golang.org/x/time/rate\" ) func main() { l := rate.NewLimiter(1, 2) // 构造方法接受两个参数 limit表示每秒产生 token 数，burst 最多存令牌数 for i := 0; i \u003c 50; i++ { //这里是阻塞等待的，一直等到取到一个令牌为止 log.Println(\"... ... Wait\") c, _ := context.WithTimeout(context.Background(), time.Second*2) //Wait 阻塞等待 if err := l.Wait(c); err != nil { log.Println(\"limiter wait error : \" + err.Error()) } log.Println(\"Wait ... ... \") // Reserve 返回等待时间，再去取令牌 // 返回需要等待多久才有新的令牌, 这样就可以等待指定时间执行任务 r := l.Reserve() log.Println(\"reserve time :\", r.Delay()) //判断当前是否可以取到令牌 //Allow 判断当前是否可以取到令牌 a := l.Allow() log.Println(\"Allow == \", a) } } 上述令牌桶限流器确实能够保护系统不被拖垮, 但这样的保护方法都是设定一个 quota, 当超过该 quota 后就阻止或减少流量的继续进入，当系统负载降低到某一水平后则恢复流量的进入。但其通常都是被动的，其实际效果取决于限流阈值设置是否合理，但往往设置合理不是一件容易的事情。 阿里开源的 Sentinel基于 TCP BBR 算法的思想开发了系统自适应限流。 Sentinel 系统自适应限流从整体维度对应用入口流量进行控制，结合应用的 Load、CPU 使用率、总体平均 RT、入口 QPS 和并发线程数等几个维度的监控指标，通过自适应的流控策略，让系统的入口流量和系统的负载达到一个平衡，让系统尽可能跑在最大吞吐量的同时保证系统整体的稳定性。 在我们的 Golang 服务中同样采用了系统自适应限流算法来提高系统稳定性。这里面用到算法就是 Kratos中借鉴 Sentinel 设计而实现的 Golang 版本 BBR。在这篇文章我们介绍一下 BBR 的实现原理。 ","date":"2022-11-25","objectID":"/limit-bbr/:0:0","tags":["限流","bbr","高可用"],"title":"系统自适应限流算法-BBR","uri":"/limit-bbr/"},{"categories":["golang","service","实现"],"content":"限流指标 Kratos BBR 通过综合分析服务的 cpu 使用率、请求成功的 qps 和请求成功的 rt 来做自适应限流保护。 cpu: 最近 1s 的 CPU 使用率均值，使用滑动平均计算，采样周期是 250ms inflight: 当前处理中正在处理的请求数量 pass: 请求处理成功的量 rt: 请求成功的响应耗时 ","date":"2022-11-25","objectID":"/limit-bbr/:0:1","tags":["限流","bbr","高可用"],"title":"系统自适应限流算法-BBR","uri":"/limit-bbr/"},{"categories":["golang","service","实现"],"content":"滑动窗口 在自适应限流保护中，采集到的指标的时效性非常强，系统只需要采集最近一小段时间内的 qps、rt 即可，对于较老的数据，会自动丢弃。为了实现这个效果，kratos 使用了滑动窗口来保存采样数据。 如上图，展示了一个具有两个桶（bucket）的滑动窗口（rolling window）。整个滑动窗口用来保存最近 1s 的采样数据，每个小的桶用来保存 500ms 的采样数据。 当时间流动之后，过期的桶会自动被新桶的数据覆盖掉，在图中，在 1000-1500ms 时，bucket 1 的数据因为过期而被丢弃，之后 bucket 3 的数据填到了窗口的头部。 ","date":"2022-11-25","objectID":"/limit-bbr/:0:2","tags":["限流","bbr","高可用"],"title":"系统自适应限流算法-BBR","uri":"/limit-bbr/"},{"categories":["golang","service","实现"],"content":"限流公式 (cpu \u003e 800 OR (Now - PrevDrop)) \u003c 1s AND (MaxPass * MinRt * windows / 1000) \u003c InFlight MaxPass 表示最近 10s 内，单个 bucket 中最大的请求数 MinRt 表示最近 10s 内，单个 bucket 中最小的响应时间 windows 表示一秒内 bucket 的数量，默认配置中是 10s 100 个bucket，那么 windows 的值为 10 ","date":"2022-11-25","objectID":"/limit-bbr/:0:3","tags":["限流","bbr","高可用"],"title":"系统自适应限流算法-BBR","uri":"/limit-bbr/"},{"categories":["golang","service","实现"],"content":"源码实现 Allow func (l *BBR) Allow(ctx context.Context, opts ...limit.AllowOption) (func(info limit.DoneInfo), error) { allowOpts := limit.DefaultAllowOpts() for _, opt := range opts { opt.Apply(\u0026allowOpts) } if l.shouldDrop() { // 判断是否触发限流 return nil, ecode.LimitExceed } atomic.AddInt64(\u0026l.inFlight, 1) // 增加正在处理请求数 stime := time.Since(initTime) // 记录请求到来的时间 return func(do limit.DoneInfo) { rt := int64((time.Since(initTime) - stime) / time.Millisecond) // 请求处理成功的响应时长 l.rtStat.Add(rt) // 增加 rtStat 响应耗时的统计 atomic.AddInt64(\u0026l.inFlight, -1) // 请求处理成功后, 减少正在处理的请求数 switch do.Op { case limit.Success: l.passStat.Add(1) // 处理成功后增加成功处理请求数的统计 return default: return } }, nil } shouldDrop func (l *BBR) shouldDrop() bool { // 判断目前cpu的使用率是否达到设置的CPU的限制, 默认值800 if l.cpu() \u003c l.conf.CPUThreshold { // 如果上一次舍弃请求的时间是0, 那么说明没有限流的需求, 直接返回 prevDrop, _ := l.prevDrop.Load().(time.Duration) if prevDrop == 0 { return false } // 如果上一次请求的时间与当前的请求时间小于1s, 那么说明有限流的需求 if time.Since(initTime)-prevDrop \u003c= time.Second { if atomic.LoadInt32(\u0026l.prevDropHit) == 0 { atomic.StoreInt32(\u0026l.prevDropHit, 1) } // 增加正在处理的请求的数量 inFlight := atomic.LoadInt64(\u0026l.inFlight) // 判断正在处理的请求数是否达到系统的最大的请求数量 return inFlight \u003e 1 \u0026\u0026 inFlight \u003e l.maxFlight() } // 清空当前的prevDrop l.prevDrop.Store(time.Duration(0)) return false } // 增加正在处理的请求的数量 inFlight := atomic.LoadInt64(\u0026l.inFlight) // 判断正在处理的请求数是否达到系统的最大的请求数量 drop := inFlight \u003e 1 \u0026\u0026 inFlight \u003e l.maxFlight() if drop { prevDrop, _ := l.prevDrop.Load().(time.Duration) // 如果判断达到了最大请求数量, 并且当前有限流需求 if prevDrop != 0 { return drop } l.prevDrop.Store(time.Since(initTime)) } return drop } maxFlight 该函数是核心函数. 其计算公式: MaxPass * MinRt * windows / 1000. maxPASS/minRT 都是基于 metric.RollingCounter 来实现的 // winBucketPerSec: 每秒内的采样数量,其计算方式:int64(time.Second)/(int64(conf.Window)/int64(conf.WinBucket)), conf.Window 默认值 10s, conf.WinBucket 默认值100. // 简化下公式: 1/(10/100) = 10, 所以每秒内的采样数就是10 func (l *BBR) maxFlight() int64 { return int64(math.Floor(float64(l.maxPASS()*l.minRT()*l.winBucketPerSec)/1000.0 + 0.5)) } // 单个采样窗口在一个采样周期中的最大的请求数, 默认的采样窗口是10s, 采样bucket数量100 func (l *BBR) maxPASS() int64 { rawMaxPass := atomic.LoadInt64(\u0026l.rawMaxPASS) if rawMaxPass \u003e 0 \u0026\u0026 l.passStat.Timespan() \u003c 1 { return rawMaxPass } // 遍历100个采样 bucket, 找到采样 bucket 中最大的请求数 rawMaxPass = int64(l.passStat.Reduce(func(iterator metric.Iterator) float64 { var result = 1.0 for i := 1; iterator.Next() \u0026\u0026 i \u003c l.conf.WinBucket; i++ { bucket := iterator.Bucket() count := 0.0 for _, p := range bucket.Points { count += p } result = math.Max(result, count) } return result })) if rawMaxPass == 0 { rawMaxPass = 1 } atomic.StoreInt64(\u0026l.rawMaxPASS, rawMaxPass) return rawMaxPass } // 单个采样窗口中最小的响应时间 func (l *BBR) minRT() int64 { rawMinRT := atomic.LoadInt64(\u0026l.rawMinRt) if rawMinRT \u003e 0 \u0026\u0026 l.rtStat.Timespan() \u003c 1 { return rawMinRT } // 遍历100个采样 bucket, 找到采样 bucket 中最小的响应时间 rawMinRT = int64(math.Ceil(l.rtStat.Reduce(func(iterator metric.Iterator) float64 { var result = math.MaxFloat64 for i := 1; iterator.Next() \u0026\u0026 i \u003c l.conf.WinBucket; i++ { bucket := iterator.Bucket() if len(bucket.Points) == 0 { continue } total := 0.0 for _, p := range bucket.Points { total += p } avg := total / float64(bucket.Count) result = math.Min(result, avg) } return result }))) if rawMinRT \u003c= 0 { rawMinRT = 1 } atomic.StoreInt64(\u0026l.rawMinRt, rawMinRT) return rawMinRT } ","date":"2022-11-25","objectID":"/limit-bbr/:0:4","tags":["限流","bbr","高可用"],"title":"系统自适应限流算法-BBR","uri":"/limit-bbr/"},{"categories":["golang","service","实现"],"content":"使用方式 BBR 限流器可以作为 grpc server 的 UnaryServerInterceptor 实现 // Limit is a server interceptor that detects and rejects overloaded traffic. func (b *RateLimiter) Limit() grpc.UnaryServerInterceptor { return func(ctx context.Context, req interface{}, args *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) { uri := args.FullMethod limiter := b.group.Get(uri) done, err := limiter.Allow(ctx) b.printStats(uri, limiter, err == nil) if err != nil { _metricServerBBR.Inc(uri) return } defer func() { done(limit.DoneInfo{Op: limit.Success}) }() resp, err = handler(ctx, req) return } } 启动 grpc server 的时候可以把这个 UnaryServerInterceptor 添加到 server 的 Interceptor 链中来保护 grpc server。 s.server = grpc.NewServer(opt...) s.health = health.NewServer() s.Use(bbr.New(nil).Limit()) ","date":"2022-11-25","objectID":"/limit-bbr/:0:5","tags":["限流","bbr","高可用"],"title":"系统自适应限流算法-BBR","uri":"/limit-bbr/"},{"categories":["原理","存储","基础"],"content":"Redis v6.0 真正意义上引入了多线程，我们有必要深刻理解这个特性的实现和意义","date":"2022-10-30","objectID":"/redis-io-threads/","tags":["redis","io threads","多线程"],"title":"Redis 多线程的学习和理解","uri":"/redis-io-threads/"},{"categories":["原理","存储","基础"],"content":"在后端开发中面临分布式缓存选型时 Redis 已然成为了头号选手甚至可以说是事实上的标准。Redis 从本质上来讲是一个网络服务器，而对于一个网络服务器来说，网络模型是它的核心。而关于网络模型我们知道 Redis v6.0 版本引入了一个非常重要的特性-多线程 IO。在这篇文章里我们学习一下这个重要的特性，有助于我们更好的额理解 Redis 的底层实现。 ","date":"2022-10-30","objectID":"/redis-io-threads/:0:0","tags":["redis","io threads","多线程"],"title":"Redis 多线程的学习和理解","uri":"/redis-io-threads/"},{"categories":["原理","存储","基础"],"content":"Redis 单线程 这里说的单线程指的是 Redis 的网络模型用单线程实现。单线程实现有以下优点 避免过多的上下文切换开销 多线程调度过程中必然需要在 CPU 之间切换线程上下文 context，而上下文的切换又涉及程序计数器、堆栈指针和程序状态字等一系列的寄存器置换、程序堆栈重置甚至是高速缓存、TLB 快表的汰换，如果是进程内的多线程切换还好一些，因为单一进程内多线程共享进程地址空间，因此线程上下文比之进程上下文要小得多，如果是跨进程调度，则需要切换掉整个进程地址空间。 如果是单线程则可以规避进程内频繁的线程切换开销，因为程序始终运行在进程中单个线程内，没有多线程切换的场景。 可以实现无锁 如果 Redis 选择多线程模型，又因为 Redis 是一个数据库，那么势必涉及到底层数据同步的问题，则必然会引入某些同步机制，比如锁，而我们知道 Redis 不仅仅提供了简单的 key-value 数据结构，还有 list、set 和 hash 等等其他丰富的数据结构，而不同的数据结构对同步访问的加锁粒度又不尽相同，可能会导致在操作数据过程中带来很多加锁解锁的开销，增加程序复杂度的同时还会降低性能。 可维护性高 多线程的引入会使得程序不再保持代码逻辑上的串行性，代码执行的顺序将变成不可预测的，稍不注意就会导致程序出现各种并发编程的问题；其次，多线程模式也使得程序调试更加复杂和麻烦 单线程事件循环 从 Redis 的 v1.0 到 v6.0 版本之前（在 v4.0 版本就已经引入了多线程处理异步任务），Redis 的核心网络模型一直是一个典型的单 Reactor 模型：利用 epoll/select/kqueue 等多路复用技术，在单线程的事件循环中不断去处理事件（客户端请求），最后回写响应数据到客户端。 学习图上几个重要概念 client：客户端对象，Redis 是典型的 CS 架构（Client \u003c—\u003e Server），客户端通过 socket 与服务端建立网络通道然后发送请求命令，服务端执行请求的命令并回复。Redis 使用结构体 client 存储客户端的所有相关信息，包括但不限于封装的套接字连接 – conn，当前选择的数据库指针 -- db，读入缓冲区 – querybuf，写出缓冲区 – buf，写出数据链表 – reply等。 aeApiPoll：I/O 多路复用 API，是基于 epoll_wait/select/kevent 等系统调用的封装，监听等待读写事件触发，然后处理，它是事件循环（Event Loop）中的核心函数，是事件驱动得以运行的基础。 acceptTcpHandler：连接应答处理器，底层使用系统调用 accept 接受来自客户端的新连接，并为新连接注册绑定命令读取处理器，以备后续处理新的客户端 TCP 连接；除了这个处理器，还有对应的 acceptUnixHandler 负责处理 Unix Domain Socket 以及 acceptTLSHandler 负责处理 TLS 加密连接。 readQueryFromClient：命令读取处理器，解析并执行客户端的请求命令。 beforeSleep：事件循环中进入 aeApiPoll 等待事件到来之前会执行的函数，其中包含一些日常的任务，比如把 client-\u003ebuf 或者 client-\u003ereply （后面会解释为什么这里需要两个缓冲区）中的响应写回到客户端，持久化 AOF 缓冲区的数据到磁盘等，相对应的还有一个 afterSleep 函数，在 aeApiPoll 之后执行。 sendReplyToClient：命令回复处理器，当一次事件循环之后写出缓冲区中还有数据残留，则这个处理器会被注册绑定到相应的连接上，等连接触发写就绪事件时，它会将写出缓冲区剩余的数据回写到客户端。 结合上图学习客户端向 Redis 发起请求命令的工作原理 Redis 服务器启动，开启主线程事件循环（Event Loop），注册 acceptTcpHandler 连接应答处理器到用户配置的监听端口对应的文件描述符，等待新连接到来； 客户端和服务端建立网络连接； acceptTcpHandler 被调用，主线程使用 AE 的 API 将 readQueryFromClient 命令读取处理器绑定到新连接对应的文件描述符上，并初始化一个 client 绑定这个客户端连接； 客户端发送请求命令，触发读就绪事件，主线程调用 readQueryFromClient 通过 socket 读取客户端发送过来的命令存入 client-\u003equerybuf 读入缓冲区； 接着调用 processInputBuffer，在其中使用 processInlineBuffer 或者 processMultibulkBuffer 根据 Redis 协议解析命令，最后调用 processCommand 执行命令； 根据请求命令的类型（SET, GET, DEL, EXEC 等），分配相应的命令执行器去执行，最后调用 addReply 函数族的一系列函数将响应数据写入到对应 client 的写出缓冲区：client-\u003ebuf 或者 client-\u003ereply ，client-\u003ebuf 是首选的写出缓冲区，固定大小 16KB，一般来说可以缓冲足够多的响应数据，但是如果客户端在时间窗口内需要响应的数据非常大，那么则会自动切换到 client-\u003ereply 链表上去，使用链表理论上能够保存无限大的数据（受限于机器的物理内存），最后把 client 添加进一个 LIFO 队列 clients_pending_write； 在事件循环（Event Loop）中，主线程执行 beforeSleep –\u003e handleClientsWithPendingWrites，遍历 clients_pending_write 队列，调用 writeToClient 把 client 的写出缓冲区里的数据回写到客户端，如果写出缓冲区还有数据遗留，则注册 sendReplyToClient 命令回复处理器到该连接的写就绪事件，等待客户端可写时在事件循环中再继续回写残余的响应数据。 ","date":"2022-10-30","objectID":"/redis-io-threads/:0:1","tags":["redis","io threads","多线程"],"title":"Redis 多线程的学习和理解","uri":"/redis-io-threads/"},{"categories":["原理","存储","基础"],"content":"Redis 多线程 CPU 通常不会成为性能瓶颈，瓶颈往往是内存和网络，因此单线程足够了。那么为什么现在 Redis 又要引入多线程呢？很简单，就是 Redis 的网络 I/O 瓶颈已经越来越明显了。互联网业务系统所要处理的线上流量越来越大，Redis 的单线程模式会导致系统消耗很多 CPU 时间在网络 I/O 上从而降低吞吐量，要提升 Redis 的性能有两个方向： 优化网络 I/O 模块 提高机器内存读写的速度 6.0 版本之后，Redis 正式在核心网络模型中引入了多线程，即 I/O threading Redis 多线程网络模型的总体设计如下图所示 Redis 服务器启动，开启主线程事件循环（Event Loop），注册 acceptTcpHandler 连接应答处理器到用户配置的监听端口对应的文件描述符，等待新连接到来； 客户端和服务端建立网络连接； acceptTcpHandler 被调用，主线程使用 AE 的 API 将 readQueryFromClient 命令读取处理器绑定到新连接对应的文件描述符上，并初始化一个 client 绑定这个客户端连接； 客户端发送请求命令，触发读就绪事件，服务端主线程不会通过 socket 去读取客户端的请求命令，而是先将 client 放入一个 LIFO 队列 clients_pending_read； 在事件循环（Event Loop）中，主线程执行 beforeSleep –\u003ehandleClientsWithPendingReadsUsingThreads，利用 Round-Robin 轮询负载均衡策略，把 clients_pending_read队列中的连接均匀地分配给 I/O 线程各自的本地 FIFO 任务队列 io_threads_list[id] 和主线程自己，I/O 线程通过 socket 读取客户端的请求命令，存入 client-\u003equerybuf 并解析第一个命令，但不执行命令，主线程忙轮询，等待所有 I/O 线程完成读取任务； 主线程和所有 I/O 线程都完成了读取任务，主线程结束忙轮询，遍历 clients_pending_read 队列，执行所有客户端连接的请求命令，先调用 processCommandAndResetClient 执行第一条已经解析好的命令，然后调用 processInputBuffer 解析并执行客户端连接的所有命令，在其中使用 processInlineBuffer 或者 processMultibulkBuffer 根据 Redis 协议解析命令，最后调用 processCommand 执行命令； 根据请求命令的类型（SET, GET, DEL, EXEC 等），分配相应的命令执行器去执行，最后调用 addReply 函数族的一系列函数将响应数据写入到对应 client 的写出缓冲区：client-\u003ebuf 或者 client-\u003ereply ，client-\u003ebuf 是首选的写出缓冲区，固定大小 16KB，一般来说可以缓冲足够多的响应数据，但是如果客户端在时间窗口内需要响应的数据非常大，那么则会自动切换到 client-\u003ereply 链表上去，使用链表理论上能够保存无限大的数据（受限于机器的物理内存），最后把 client 添加进一个 LIFO 队列 clients_pending_write； 在事件循环（Event Loop）中，主线程执行 beforeSleep –\u003e handleClientsWithPendingWritesUsingThreads，利用 Round-Robin 轮询负载均衡策略，把 clients_pending_write 队列中的连接均匀地分配给 I/O 线程各自的本地 FIFO 任务队列 io_threads_list[id] 和主线程自己，I/O 线程通过调用 writeToClient 把 client 的写出缓冲区里的数据回写到客户端，主线程忙轮询，等待所有 I/O 线程完成写出任务； 主线程和所有 I/O 线程都完成了写出任务， 主线程结束忙轮询，遍历 clients_pending_write 队列，如果 client 的写出缓冲区还有数据遗留，则注册 sendReplyToClient 到该连接的写就绪事件，等待客户端可写时在事件循环中再继续回写残余的响应数据。 这里大部分逻辑和之前的单线程模型是一致的，变动的地方仅仅是把读取客户端请求命令和回写响应数据的逻辑异步化了，交给 I/O 线程去完成。这里需要特别注意的一点是 I/O 线程仅仅是读取和解析客户端命令而不会真正去执行命令，客户端命令的执行最终还是要回到主线程上完成。 我们把上述过程再通过流程图描述如下 ","date":"2022-10-30","objectID":"/redis-io-threads/:0:2","tags":["redis","io threads","多线程"],"title":"Redis 多线程的学习和理解","uri":"/redis-io-threads/"},{"categories":["原理","存储","基础"],"content":"benchmark 单线程示例 benchmark 如下 root@be919f395fa3:/data# redis-benchmark -t get,set -c 32 -n 500000 ====== SET ====== 500000 requests completed in 21.29 seconds 32 parallel clients 3 bytes payload keep alive: 1 host configuration \"save\": 3600 1 300 100 60 10000 host configuration \"appendonly\": no multi-thread: no Summary: throughput summary: 23483.00 requests per second 开启 io-threads 配置，多线程示例 benchmark 如下 root@be919f395fa3:/data# redis-benchmark -t get,set -c 160 --threads 4 -n 500000 ====== SET ====== 500000 requests completed in 19.43 seconds 160 parallel clients 3 bytes payload keep alive: 1 host configuration \"save\": 3600 1 300 100 60 10000 host configuration \"appendonly\": no multi-thread: yes threads: 4 Summary: throughput summary: 25736.05 requests per second 测试结果如图所示，上图是 set 测试结果，下图是 get 测试结果 GET/SET 分别提升 30% 多，并发连接小于 128 时性能差距不大。 总的来说 Redis 多线程网络模型在高并发场景下确实带来了性能提升(理论上高并发下能提升1倍 QPS)，这个方案是作者兼顾了性能和可维护性下的设计。其中 I/O 线程任务仅仅是通过 socket 读取客户端请求命令并解析，却没有真正去执行命令，所有客户端命令最后还需要回到主线程去执行，因此对多核的利用率并不算高，而且每次主线程都必须在分配完任务之后忙轮询等待所有 I/O 线程完成任务之后才能继续执行其他逻辑。在单机场景下我们确实可以利用这个特性来提升 Redis 性能，但是在高可用和高并发场景下选择 Redis-Cluster 我认为更合适。 ","date":"2022-10-30","objectID":"/redis-io-threads/:0:3","tags":["redis","io threads","多线程"],"title":"Redis 多线程的学习和理解","uri":"/redis-io-threads/"},{"categories":["cloud native","service","实践"],"content":"Dapr 介绍 Dapr 是一个开源项目，由微软发起，下面是来自 Dapr 官方网站的介绍 Dapr 是一个可移植的、事件驱动的运行时，它使任何开发人员能够轻松构建出弹性的、无状态和有状态的应用程序，并可运行在云平台或边缘计算中，它同时也支持多种编程语言和开发框架。 通过阅读官方文档和实践，我理解的 dapr 的定位包含这些： 不依赖于特定的库，包和框架，可以使用任意的编程语言开发业务功能 不依赖于特定的中间件，存储等，并且可以按需替换 不依赖于特定平台，可以在本地、Kubernetes 集群或者其它集成 Dapr 的托管环境中运行应用程序 业务通过标准的 API 提供服务，无需关心底层具体实 如下图所示，概括了 Dapr 的能力和层次架构 服务构建块（building block) 构建块是 dapr 提供的可以通过标准 Http 或者 gRPC API访问的模块化最佳实践。构建块可以使用任何组件组合，例如 actors 构建块和 状态管理 构建块都使用 状态组件。 另一个示例是 Pub/Sub 构建块使用 Pub/Sub 组件。 以下是 Dapr 提供的构建块类型: 组件 （component） 组件是 dapr 中最佳实践的功能实现，每个组件都有接口定义，所有组件都是可插拔的，因此可以将组件换为另一个具有相同接口的组件。 目前 dapr 提供的组件例子如下图，具体可以查看官方文档 ","date":"2022-09-30","objectID":"/dapr-pubsub/:0:1","tags":["dapr","消费订阅"],"title":"Dapr 初探-Pub/Sub 实践","uri":"/dapr-pubsub/"},{"categories":["cloud native","service","实践"],"content":"发布订阅实践 运行过程 发布和订阅 (pub/sub) 使微服务能够使用事件驱动架构的消息相互通信。这种服务模式的好处在这里就不赘述了。我们主要是介绍一下 dapr 如何帮助我们来实现这张服务模式。 Dapr 中的发布/订阅 API 功能 提供与平台无关的 API 来发送和接收消息 提供至少一次消息传递保证 与各种消息代理集成例如 redis,kafka, rabbitmq 等等 服务所使用的消息代理是可插拔的，并被配置为运行时的 Dapr Pub/Sub 组件。 这种方法消除了服务的依赖性，从而使服务可以更加可移植和更灵活变更。 Dapr 中使用 pub/sub 的过程： 服务对 Dapr 发布/订阅构建块 API 进行调用 pub/sub 构建块调用封装特定消息代理的 Dapr pub/sub 组件 为了接收消息，Dapr 订阅 pub/sub 组件，并在消息到达时将消息传递到服务提供的接口 下图是一个 dapr 中 pub/sub 的例子 示例 作为消息接收者的服务实现 package main import ( \"encoding/json\" \"fmt\" \"io/ioutil\" \"log\" \"net/http\" \"os\" \"github.com/gorilla/mux\" ) type JSONObj struct { PubsubName string `json:\"pubsubName\"` Topic string `json:\"topic\"` Route string `json:\"route\"` } type Result struct { Data *Data `json:\"data\"` } type Data struct { Name string `json:\"name\"` Id int `json:\"id\"` } // Register Dapr pub/sub subscriptions func getSub(w http.ResponseWriter, r *http.Request) { jsonData := []JSONObj{ { PubsubName: \"pubsub\", Topic: \"test\", Route: \"test\", }, } jsonBytes, err := json.Marshal(jsonData) if err != nil { log.Fatal(\"Error in reading the result obj\") } _, err = w.Write(jsonBytes) if err != nil { log.Fatal(\"Error in writing the result obj\") } } // Dapr subscription in /dapr/subscribe sets up this route func postMessage(w http.ResponseWriter, r *http.Request) { data, err := ioutil.ReadAll(r.Body) if err != nil { log.Fatal(err) } var result Result fmt.Println(\"raw data: \", string(data)) err = json.Unmarshal(data, \u0026result) if err != nil { log.Fatal(err) } fmt.Println(\"Subscriber received: \", result.Data) obj, err := json.Marshal(data) if err != nil { log.Fatal(\"Error in reading the result obj\") } _, err = w.Write(obj) if err != nil { log.Fatal(\"Error in writing the result obj\") } } func main() { appPort := \"6001\" if value, ok := os.LookupEnv(\"APP_PORT\"); ok { appPort = value } r := mux.NewRouter() r.HandleFunc(\"/dapr/subscribe\", getSub).Methods(\"GET\") // Dapr subscription routes orders topic to this route r.HandleFunc(\"/test\", postMessage).Methods(\"POST\") if err := http.ListenAndServe(\":\"+appPort, r); err != nil { log.Panic(err) } } 启动消费者服务 dapr run --app-port 6001 --app-id message-processor --app-protocol http --dapr-http-port 3501 --components-path ~/.dapr/components -- go run . 发送消息 首先启动一个 dapr sidecar 用于接收消息 dapr run --app-id myapp --dapr-http-port 3500 POST http://localhost:\u003cdaprPort\u003e/v1.0/publish/\u003cpubsubname\u003e/\u003ctopic\u003e[?\u003cmetadata\u003e] 可以用来把消息发给指定的 topic 示例中的消息如下所示 curl --location --request POST 'http://localhost:3500/v1.0/publish/pubsub/test' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"name\": \"lxkaka\", \"id\": 123 }' 当然我们可以在服务中去调用 API 作为生产者。 消费者输出如下： == APP == raw data: {\"data\":{\"id\":123,\"name\":\"lxkaka\"},\"datacontenttype\":\"application/json\",\"id\":\"5b895456-2d57-42bd-855f-d8a6882e406b\",\"pubsubname\":\"pubsub\",\"source\":\"mydapr\",\"specversion\":\"1.0\",\"topic\":\"test\",\"traceid\":\"00-c731a591f41c36b09f966b213fa6b7be-4e6eb22474fc2c21-01\",\"traceparent\":\"00-c731a591f41c36b09f966b213fa6b7be-4e6eb22474fc2c21-01\",\"tracestate\":\"\",\"type\":\"com.dapr.event.sent\"} == APP == Subscriber received: \u0026{lxkaka 123} 消费组 Dapr 会根据 app_id 把 app_id 相同的服务实例作为消费组，同一个消费组里只有一个实例能消费到消息。 示例如下，我们再启动一个消费实例 dapr run --app-port 6002 --app-id message-processor --app-protocol http --dapr-http-port 3502 --components-path ~/.dapr/components -- go run . 发了 5 条消息，其中一个实例收到 2 条，另外一个实例收到 3条输出如下： == APP == Subscriber received: \u0026{lxkaka 1} == APP == raw data: {\"data\":{\"id\":3,\"name\":\"lxkaka\"},\"datacontenttype\":\"application/json\",\"id\":\"9e929d03-6c7e-4440-a4a9-f58dc93e2192\",\"pubsubname\":\"pubsub\",\"source\":\"mydapr\",\"specversion\":\"1.0\",\"topic\":\"test\",\"traceid\":\"00-f03697dcc9fe5e0b0e543402bbdad8ef-db83433662f25934-01\",\"traceparent\":\"00-f03697dcc9fe5e0b0e543402bbdad8ef-db83433662f25934-01\",\"tracestate\":\"\",\"type\":\"com.dapr.event.sent\"} == APP == Subscriber received: \u0026{lxkaka 3} == APP == raw data: {\"data\":{\"id\":5,\"name\":\"lxkaka\"},\"datacontenttype\":\"application/json\",\"id\":\"93922ff8-7ef4-4721-ab55-97fcd5f425d5\",\"pubsubname\":\"pubsub\",\"source\":\"mydapr\",\"specversion\":\"1.0\",\"topic\":\"test\",\"trac","date":"2022-09-30","objectID":"/dapr-pubsub/:0:2","tags":["dapr","消费订阅"],"title":"Dapr 初探-Pub/Sub 实践","uri":"/dapr-pubsub/"},{"categories":["golang","service","实践"],"content":"当我们需要在端上展示用户上传的图片时，由于用户上传的图片尺寸各异，为了良好的视觉效果，我们必须对尺寸各异的图片做处理。在这篇文章里我们介绍一种常用的处理图片方法即图片按比例缩小。这种方法需要实现两个目的一是压缩图片，二是统一图片比例。下面是在 Golang 中的实现方式。 ","date":"2022-08-30","objectID":"/image-process/:0:0","tags":["image","draw"],"title":"Golang 中实用的图片处理方法","uri":"/image-process/"},{"categories":["golang","service","实践"],"content":"基础操作 图片缩放 在 Golang 里进行图片缩放我们可以直接使用这个库 https://github.com/nfnt/resize 这个库提供了多种图片缩放的算法，包括以下 NearestNeighbor: Nearest-neighbor interpolation Bilinear: Bilinear interpolation Bicubic: Bicubic interpolation MitchellNetravali: Mitchell-Netravali interpolation Lanczos2: Lanczos resampling with a=2 Lanczos3: Lanczos resampling with a=3 关于几种图像缩放中的插值算法可以看这篇文档。 基本使用方法 // 第一个参数为缩放后的宽度 // 第二个参数为缩放后的高度 // 第三个参数为待缩放图片 //第四个参数为使用哪种插值算法 m := resize.Resize(1000, 0, img, resize.Lanczos3) 图片绘制 在 Golang 里我们经常使用官方库 image/draw 来绘制图片。 两个重要方法 func Draw(dst Image, r image.Rectangle, src image.Image, sp image.Point, op Op) func DrawMask(dst Image, r image.Rectangle, src image.Image, sp image.Point, mask image.Image, mp image.Point, op Op) // dst 绘图的背景图 // r 背景图的绘图区域; // src 要绘制的图 // sp src绘制的开始点，一般是 src 图的左上角 // op 包括 draw.Over 和 draw.Src // op 对于没有 mask 的情况，draw.Src 和 draw.Over 一样， 意思就是 src 图片直接盖在 dst 图片上，dst 被盖住的部分是不可见的 关于 draw 的参数理解可以参考这篇文章。 ","date":"2022-08-30","objectID":"/image-process/:0:1","tags":["image","draw"],"title":"Golang 中实用的图片处理方法","uri":"/image-process/"},{"categories":["golang","service","实践"],"content":"图片按比例缩小 有了上述的背景知识后再来实现图片按特定比例缩放就容易多了。下面的代码实现是把任意一张图片按照 16:9 的比例缩小，并且对比例失调的部分用白边补齐。 // ResizeImage 图片按 16:9 缩小，白边补齐 func ResizeImage(file io.Reader) (out io.Reader, err error) { // file 是原图 buffer, err := ioutil.ReadAll(file) if err != nil { return } // 检查图片类型 fileType := http.DetectContentType(buffer) var img image.Image if fileType == \"image/png\" { // decode jpeg into image.Image img, err = png.Decode(bytes.NewReader(buffer)) } else if fileType == \"image/jpeg\" { img, err = jpeg.Decode(bytes.NewReader(buffer)) } else { err = errors.New(\"invalid file type\") return } if err != nil { return } // 原图的尺寸 width := img.Bounds().Dx() height := img.Bounds().Dy() var ( required = 16 / 9 ration = width / height newW, newH, backW, backH int ) // 如果原图尺寸超过 1280 * 720 则缩小；不超过则保持原尺寸只进行白边补齐 if ration \u003e required { newH = 0 if width \u003e 1280 { newW = 1280 } else { newW = width } backW = newW backH = backW * 9 / 16 } else { newW = 0 if newH \u003e 720 { newH = 720 } else { newH = height } backH = newH backW = backH * 16 / 9 } // 图片缩小 m := resize.Resize(uint(newW), uint(newH), img, resize.NearestNeighbor) // 生成背景图 newImg := image.NewRGBA(image.Rect(0, 0, backW, backH)) // 背景图变成白色 c := color.RGBA{R: 0xff, G: 0xff, B: 0xff, A: 0xff} for x := 0; x \u003c backW; x++ { for y := 0; y \u003c backH; y++ { newImg.Set(x, y, c) } } // 在背景图上绘制缩小后的图片，实现补齐白边 if ration \u003e required { draw.Draw(newImg, image.Rectangle{Min: image.Point{Y: (backH - m.Bounds().Dy()) / 2}, Max: image.Point{X: backW, Y: (backH + m.Bounds().Dy()) / 2}}, m, m.Bounds().Min, draw.Src) } else { draw.Draw(newImg, image.Rectangle{Min: image.Point{X: (backW - m.Bounds().Dx()) / 2}, Max: image.Point{X: (backW + m.Bounds().Dx()) / 2, Y: backH}}, m, m.Bounds().Min, draw.Src) } // 编码生成图片 outBuffer := new(bytes.Buffer) if err = jpeg.Encode(outBuffer, newImg, nil); err != nil { return } out = outBuffer return } 效果一, 3788 * 832 图片缩小成 1280 * 720 (深色模式会对比的很清楚) 效果二, 512 * 406 图片按照 16:9 比例补齐 希望这篇文章中在 Golang 里对图片的操作方法能给大家带来参考和提升。 ","date":"2022-08-30","objectID":"/image-process/:0:2","tags":["image","draw"],"title":"Golang 中实用的图片处理方法","uri":"/image-process/"},{"categories":null,"content":" ldsink 程序员 \u0026\u0026 动漫宅 \u0026\u0026 萝莉控 Lirian Su wubba lubba dub dub ","date":"2022-07-29","objectID":"/friend/:0:0","tags":null,"title":"Friends","uri":"/friend/"},{"categories":["infra","monitor","实践"],"content":"Python 应用的 metrics 如何上报给 Promethues","date":"2022-07-29","objectID":"/py-metrics/","tags":["python","metrics","promethues"],"title":"Python 应用接入 Prometheus 监控","uri":"/py-metrics/"},{"categories":["infra","monitor","实践"],"content":"在这篇文章 我们介绍了 Prmetheus metrics 的基本类型和在 Golang 中的简单应用。我们知道在一个通过 Http 协议提供在线服务的应用中可以通过接口 /metrics 来 expose metrics，然后 promethues 可以通过定时 pull 的方式来抓取指标，这样可以实现服务的监控。那在一个不是通过 Http 接口来提供服务的应用我们如何做应用监控呢？比如在一个只从中间件中消费消息的 Python 应用怎么样通过 Promethues 实现应用监控就是在这篇文章中要介绍的内容。 收集 Metrics 的方式 Pull Prometheus 获取数据的方式只有拉取(PULL)，即 Prometheus 会以固定频率去请求每个 target 所提供的 http url来获取数据。这就需要每个服务端点提供 http 的接口来获取实时的数据。就对应我们上面提到的通过提供接口 /metrics。 Push 并不是所有的应用都能提供 http 接口，为了兼容这种场景 Promethues 也通过增加组件 PushGateway 实现了应用主动推送的方式。 PushGateway 本质上就是一个代理服务，独立部署。它没有数据抓取功能，只能被动的等待数据推送。应用把数据推送到 PushGateway 后，Prometheus 再从 PushGateway抓取。 推送监控指标 我们看推送的方式如何实现，以监控任务中某一阶段的耗时这样一个例子实践 首先我们依赖 Promethues 的 Python 官方客户端。 示例代码如下 #!/usr/bin/env python3 # -*- coding: utf-8 -*- import random import threading import time from prometheus_client import CollectorRegistry, Histogram, push_to_gateway class Moni: registry = CollectorRegistry() h = Histogram('job_duration_seconds', 'duration of job', labelnames=['job_name'], buckets=[1, 2, 3, 10, 20, 50, 100], registry=registry) @classmethod def push(cls): push_to_gateway('http://localhost:9091', job='moni', registry=cls.registry) @classmethod def start(cls): def run(): while True: cls.push() time.sleep(5) t = threading.Thread(target=run) t.daemon = True t.start() def job_run(): st = time.time() time.sleep(random.uniform(0, 10)) duration = time.time() - st Moni.h.labels(job_name=\"test_job\").observe(duration) print('模拟 job 运行: ', duration) def task(): for i in range(10): print('模拟任务开始运行...') job_run() time.sleep(10) if __name__ == '__main__': Moni.start() task() 运行上面示例代码得到如下结果 /Users/lxkaka/bili/rubick-toys/venv/bin/python3 /Users/lxkaka/bili/rubick-worker/test_metrics.py 模拟任务开始运行... 模拟 job 运行: 0.1491992473602295 模拟任务开始运行... 模拟 job 运行: 9.469422817230225 模拟任务开始运行... 模拟 job 运行: 3.0821011066436768 模拟任务开始运行... 模拟 job 运行: 4.389142990112305 模拟任务开始运行... 模拟 job 运行: 6.255290985107422 模拟任务开始运行... 模拟 job 运行: 8.34602689743042 模拟任务开始运行... 模拟 job 运行: 0.5739247798919678 模拟任务开始运行... 模拟 job 运行: 5.111149072647095 模拟任务开始运行... 模拟 job 运行: 2.314656972885132 模拟任务开始运行... 模拟 job 运行: 3.758761405944824 Process finished with exit code 0 下面的图展示了在 PushGateway 中收集到的结果，可以看到 metrics 和上面输出完全一致 此外 prometheus_client 提供了更方便的使用方式–装饰器，可以更优雅的实现指标监控，以上面的示例为例修改如下： h_label = Moni.h.labels(job_name='test_job') @h_label.time() def job_run(): time.sleep(random.uniform(0, 10)) 其他指标类型的装饰器比如 from prometheus_client import Counter, Gauge c = Counter('my_failures', 'Description of counter') @c.count_exceptions() def f(): pass g = Gauge('my_inprogress_requests', 'Description of gauge') @g.track_inprogress() def f(): pass 拉取监控指标 在任务类型的应用中虽然没有现成的 Http server，但我们可以在任务进程中起一个子线程去启动一个 Http server, 这个 server 提供接口给 Promethues 拉取指标。 在 prometheus_client 已经提供了 start_http_server 方法来启动一个后台 Http Server。 使用示例如下 #!/usr/bin/env python3 # -*- coding: utf-8 -*- # !/usr/bin/env python3 # -*- coding: utf-8 -*- import random import time from prometheus_client import Histogram, start_http_server class Moni: h = Histogram('job_duration_seconds', 'duration of job', labelnames=['job_name'], buckets=[1, 2, 3, 10, 20, 50, 100]) @classmethod def start(cls): start_http_server(8000) h_label = Moni.h.labels(job_name='test_job') @h_label.time() def job_run(): time.sleep(random.uniform(0, 10)) def task(): for i in range(10): print('模拟任务开始运行...') job_run() time.sleep(100) if __name__ == '__main__': Moni.start() task() 容器的 health check 配置上此端口，提高监控的可用性。 查看接口，输出结果如下 # HELP job_duration_seconds duration of job # TYPE job_duration_seconds histogram job_duration_seconds_bucket{job_name=\"test_job\",le=\"1.0\"} 2.0 job_duration_seconds_bucket{job_name=\"test_job\",le=\"2.0\"} 2.0 job_duration_seconds_bucket{job_name=\"test_job\",le=\"3.0\"} 2.0 job_duration_seconds_bucket{job_name=\"test_job\",le=\"10.0\"} 10.0 job_duration_seconds_bucket{job_name=\"test_job\",le=\"20.0\"} 10.0 job_duration_seconds_bucket{job_name=\"test_job\",le=\"50.0\"} 10.0 job_duration_seconds_bucket{job_name=\"test_job\",le=\"100.0\"} 10.0 job_duration_seconds_bucket{job_name=\"test_job\",le=\"+","date":"2022-07-29","objectID":"/py-metrics/:0:0","tags":["python","metrics","promethues"],"title":"Python 应用接入 Prometheus 监控","uri":"/py-metrics/"},{"categories":["实践","系统","infra"],"content":"amis是一种后台管理系统的低代码解决方案，文章介绍了方案的实施过工程","date":"2022-06-25","objectID":"/amis-practice/","tags":["amis","管理后台","低代码"],"title":"高效搭建后台系统的解决方案-amis","uri":"/amis-practice/"},{"categories":["实践","系统","infra"],"content":"最近需要搭建一个管理后台来给产品和运营同学使用。管理后台简单来说就是信息管理的工具需要提供增删改查的功能。从细节上来说这样的一个后台一般都会包含以下 可以对数据做筛选 有按钮可以刷新数据 编辑单行数据 批量修改和删除 查询某列 按某列排序 隐藏某列 开启整页内容拖拽排序 表格有分页（页数还能同步到地址栏） 有数据汇总 支持导出 Excel 表头有提示文字 鼠标移动到「平台」那列的内容时还有放大镜符号，可以展开查看更多 如果这些功能全部实现至少需要一个熟悉前端的同学花几天时间。但往往这些页面和交互都是通用的，那么有没有一种更高效的方式甚至不需要开发代码就能实现这样的页面？ 答案是有，我们可以看一下 amis。 ","date":"2022-06-25","objectID":"/amis-practice/:0:0","tags":["amis","管理后台","低代码"],"title":"高效搭建后台系统的解决方案-amis","uri":"/amis-practice/"},{"categories":["实践","系统","infra"],"content":"amis 的优势 提供完整的界面解决方案：其它 UI 框架必须使用 JavaScript 来组装业务逻辑，而 amis 只需 JSON 配置就能完成完整功能开发，包括数据获取、表单提交及验证等功能，做出来的页面不需要经过二次开发就能直接上线； 大量内置组件（120+），一站式解决：其它 UI 框架大部分都只有最通用的组件，如果遇到一些稍微不常用的组件就得自己找第三方，而这些第三方组件往往在展现和交互上不一致，整合起来效果不好，而 amis 则内置大量组件，包括了富文本编辑器、代码编辑器、diff、条件组合、实时日志等业务组件，绝大部分中后台页面开发只需要了解 amis 就足够了； 支持扩展：除了低代码模式，还可以通过 自定义组件 来扩充组件，实际上 amis 可以当成普通 UI 库来使用，实现 90% 低代码，10% 代码开发的混合模式，既提升了效率，又不失灵活性； 容器支持无限级嵌套：可以通过嵌套来满足各种布局及展现需求； 经历了长时间的实战考验：amis 在百度内部得到了广泛使用，在 6 年多的时间里创建了 5 万页面，从内容审核到机器管理，从数据分析到模型训练，amis 满足了各种各样的页面需求，最复杂的页面有超过 1 万行 JSON 配置。 ","date":"2022-06-25","objectID":"/amis-practice/:0:1","tags":["amis","管理后台","低代码"],"title":"高效搭建后台系统的解决方案-amis","uri":"/amis-practice/"},{"categories":["实践","系统","infra"],"content":"amis 实践 如果一个不太熟悉前端的同学看了官方文档可能还会有点懵，甚至还是不知道怎么用 amis 搭建页面。下面我们就介绍一种更直白的页面搭建方法。通过 amis 的官方文档我们知道最基本的一点通过 JSON 配置就能实现页面，所以首页我们需要一个基础应用或者说一个工程来容纳 JSON 配置。 模板应用 clone https://github.com/aisuda/amis-admin # 安装依赖 npm i # 打开服务 npm start 我们可以基于这样一个模板应用来搭建自己的页面, 如下图所示 每个子页面的具体 json 放到了 pages 目录，所以子页面 json 编辑就从 pages 里找到对应的 json 文件就可以了。 例如列表实例/新增 页面对应的 json 如下 { \"type\": \"page\", \"title\": \"新增\", \"remark\": null, \"toolbar\": [ { \"type\": \"button\", \"actionType\": \"link\", \"link\": \"/crud/list\", \"label\": \"返回列表\" } ], \"body\": [ { \"title\": \"\", \"type\": \"form\", \"redirect\": \"/crud/list\", \"name\": \"sample-edit-form\", \"api\": \"https://3xsw4ap8wah59.cfc-execute.bj.baidubce.com/api/amis-mock/sample\", \"controls\": [ { \"type\": \"text\", \"name\": \"engine\", \"label\": \"Engine\", \"required\": true, \"inline\": false, \"description\": \"\", \"descriptionClassName\": \"help-block\", \"placeholder\": \"\", \"addOn\": null }, { \"type\": \"divider\" }, { \"type\": \"text\", \"name\": \"browser\", \"label\": \"Browser\", \"required\": true }, { \"type\": \"divider\" }, { \"type\": \"text\", \"name\": \"platform\", \"label\": \"Platform(s)\", \"required\": true }, { \"type\": \"divider\" }, { \"type\": \"text\", \"name\": \"version\", \"label\": \"Engine version\" }, { \"type\": \"divider\" }, { \"type\": \"text\", \"name\": \"grade\", \"label\": \"CSS grade\" } ] } ] } 我们至此明白了页面的搭建就是开发上面这样的 json 文件，但是写这样的 json 还是费时费力还不直观。在开始我们也提到很多页面包含的功能都是通用的的，只是不同组件的组合。所以如果有一个工具能让我们直接拖拽各种组件就能生成 json 岂不是很爽。 可视化编辑器 clone https://github.com/aisuda/amis-editor-demo # 按照依赖 npm i # 开始编译 npm run dev # 打开服务 npm start 本地编辑器如下所示 下面我们以一个简单的例子来说明，我们实现一个列表页包含编辑和删除。 新增页面 在编辑器右上角点击新增页面 如下图所示我们创建一个列表页，使用增删改查组件 编辑字段 这里不同字段格式可以选用不同的组件来展示，比如日期，图片，视频等等有有相应的组件可以选择。 接入API 在 amis 里邀请 api 返回的数据格式必须是 { \"status\": 0, \"msg\": \"\", \"data\": { ...其他字段 } } 我们需要根据 api 实际的返回格式做一下适配 if (payload.status === 0) { payload.data.items = payload.data.story_examples ? payload.data.story_examples : [] } return payload 预览 预览可以实时看到页面的效果。 copy JSON 如果页面搭建完毕，可以从这里复制生成的 json 到实际的项目中。 以上就是使用 amis 搭建管理后台页面的实践介绍，可以看到整体学习成本并不高，对没有编程能力的同学来说同样适用。amis 以及编辑器还有很多很多的细节可以探索，相信在这组工具的帮助下我们都能高效搭建出不错的后台系统。 amis 对应的商业产品是爱速搭，可以高效提供企业级后台解决方案，也是不错的选择。对于有研发能力的公司同样可以在 amis 基础之上打造内部的后台搭建系统，我相信是一个降本增效的有力措施之一。 ","date":"2022-06-25","objectID":"/amis-practice/:0:2","tags":["amis","管理后台","低代码"],"title":"高效搭建后台系统的解决方案-amis","uri":"/amis-practice/"},{"categories":["service","backend","实现"],"content":"服务在线更新文件的方案介绍","date":"2022-05-20","objectID":"/tts-online-update/","tags":["gitlab","方案设计","热更新"],"title":"服务在线更新配置或文件的方案设计","uri":"/tts-online-update/"},{"categories":["service","backend","实现"],"content":"在 TTS 算法服务中我们的词典可能会经常更新，但如果是离线更新，那么势必要重新构建和服务发布。这样的带来的问题一是更新成本高，二是降低了服务稳定性。为了解决上述问题我们需要设计一套在线更新机制。在这篇文章里我们介绍一下我们的实践思路。 ","date":"2022-05-20","objectID":"/tts-online-update/:0:0","tags":["gitlab","方案设计","热更新"],"title":"服务在线更新配置或文件的方案设计","uri":"/tts-online-update/"},{"categories":["service","backend","实现"],"content":"词典管理 词典我们可以理解成是服务的配置文件，但是与一般配置文件不同的是它可能包含多个并且每个尺寸可能还不小，大的能达到 20MB。首先我们不需要词典的更新实时生效，那么我们就可以不采用更新主动推送的机制（这样的开发成本很高), 而我们可以采用主动拉取的方式。这里我们选择了 git 来维护和管理词典，符合我们 pull 的方式，并且使用 git 有以下好处 git 本身就是版本管理工具，能非常方便迭代词典； gitlab 这样的的仓库方便 review 和权限控制； 文件大小和层级都不会成为障碍 ","date":"2022-05-20","objectID":"/tts-online-update/:0:1","tags":["gitlab","方案设计","热更新"],"title":"服务在线更新配置或文件的方案设计","uri":"/tts-online-update/"},{"categories":["service","backend","实现"],"content":"方案 设计流程如下 克隆词典 在服务首次启动的时候，容器里是没有任何词典配置的，这个时我们需要先把词典从 gitlab clone 下来。首先就是 repo 访问权限的问题，这里我们推荐使用 deploy key 访问 repo。 创建项目级别的 deploy key 配置密钥 我们可以在 CI 中通过 Protected Variable 把密钥写入到镜像，然后使得容器能访问 repo #!/bin/bash keydir=/root/.ssh if [ ! -d \"$keydir\" ]; then mkdir -p $keydir fi echo \"$Key\" \u003e /root/.ssh/id_ed25519 chmod 400 /root/.ssh/id_ed25519 ssh-keyscan git.example.com \u003e\u003e ~/.ssh/known_hosts 服务初次启动执行 #!/bin/bash # clone 词典 bash /data/app/rubick_tts/online_update/clone.sh # 启动词典更新进程 python /data/app/rubick_tts/update_lexicon.py \u0026 触发更新 更新过程如下 #!/bin/bash SCRIPTNAME=\"dict\" BRANCH=\"main\" self_update() { cd /data/app/BTTS/checkpoints/tts-lexicon git fetch # 检查特定目录是否有更新 val=$(git diff --name-only origin/$BRANCH | grep $SCRIPTNAME) echo \"$val\" if [ -n \"$val\" ];then echo \"Found a new version, updating...\" git checkout $BRANCH git pull --force echo \"Running the new version...\" # 异步进程号 pid=$(ps -ef | grep \"/data/app/start/__init__.py\" | grep -v grep | awk '{print $2}') if [ -z \"$pid\"];then # 同步进程号 pid=$(ps -ef | grep \"/data/app/grpc_server/server.py\" | grep -v grep | awk '{print $2}') fi # 发送信号 # 触发进程更新 kill -SIGHUP $pid echo \"$pid 更新完成\" exit 1 # 没有内容更新，跳过 else echo \"Already the latest version.\" fi } self_update 监听信号 在上一步我们看到如果有内容更新会发送信号 SIGHUP，为了完成更新，我们需要在服务里监听此信号并进行词典的更新。 # 在服务的入口我们监听 SIGHUP 信号，然后触发词典更新函数 signal.signal(signal.SIGHUP, lexicon_update) # 更新词典 # 这里 CVModel 是一个单例，所以我们的操作就是重新触发 CVModel 类的构造方法 __init__ # __init__ 里会重新加载词典 def lexicon_update(signum, frame): logger.warn('lexicon update') module = importlib.import_module(app_name) module.CVModel(**{'update': update}) 单例更新 def singleton(cls): _instances = {} def get_instance(**kwargs): if cls not in _instances or kwargs.get('update'): _instances[cls] = cls(**kwargs) return _instances[cls] return get_instance @singleton class CVModel: def __init__(self, **kwargs): # 重新加载词典 ... def run() ... 我们在线更新词典就完成了。上面介绍的在线更新方案其实是非常通用的，有类似不通过重启服务实现远程配置或者文件的更新的需求都可以借鉴这样的方案。 ","date":"2022-05-20","objectID":"/tts-online-update/:0:2","tags":["gitlab","方案设计","热更新"],"title":"服务在线更新配置或文件的方案设计","uri":"/tts-online-update/"},{"categories":["service","backend","实现"],"content":"常见消息系统的方案设计","date":"2022-04-22","objectID":"/msg-system/","tags":["消息","方案设计","mysql"],"title":"消息系统方案设计之站内消息和事件提醒","uri":"/msg-system/"},{"categories":["service","backend","实现"],"content":"消息系统可以说是每个应用不可缺少的系统。从功能上来说一个消息系统至少有两大部分组成 消息的存储 消息的通知 消息系统的复杂程度和用户规模和业务规模呈正相关。这里先做站内消息系统的设计和实现。 我们把消息分为三类 系统通知 事件提醒 私信 这里我们先设计当前需要的系统通知和事件提醒。 ","date":"2022-04-22","objectID":"/msg-system/:0:0","tags":["消息","方案设计","mysql"],"title":"消息系统方案设计之站内消息和事件提醒","uri":"/msg-system/"},{"categories":["service","backend","实现"],"content":"系统通知 系统通知一般是由后台管理员创建并且可指定某一类（全体，个人等）用户接收。 基于此设计两张表来存储相关信息 消息内容表 用户通知表 消息内容表设计如下 结构设计 消息内容表 CREATE TABLE `message_info` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID', `msg_id` varchar(32) NOT NULL DEFAULT '' COMMENT '消息ID', `title` varchar(50) NOT NULL DEFAULT '' COMMENT '标题', `content` varchar(500) NOT NULL DEFAULT '' COMMENT '正文', `image` varchar(1024) NOT NULL DEFAULT '' COMMENT '图片地址', `jump_url` varchar(1024) NOT NULL DEFAULT '' COMMENT '跳转链接', `type` tinyint(1) NOT NULL DEFAULT 0 COMMENT '消息类型 1-系统消息 2-创作邀请 100-其他', `sender` bigint(20) unsigned NOT NULL DEFAULT 0 COMMENT '发送者ID', `channel` tinyint(1) NOT NULL DEFAULT 0 COMMENT '发送渠道 1-站内 2-短信 3-推送', `receive_type` tinyint(1) NOT NULL DEFAULT 0 COMMENT '发送渠道 1-白名单 2-黑名单 3-单一用户 100-全部', `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', PRIMARY KEY (`id`), UNIQUE KEY `uk_msg_id` (`msg_id`), KEY `ix_mtime` (`mtime`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COMMENT ='消息内容表'; 用户通知表设计如下 用户通知表 CREATE TABLE `message_notice` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID', `msg_id` varchar(32) NOT NULL DEFAULT '' COMMENT '消息ID', `state` tinyint(1) NOT NULL DEFAULT 0 COMMENT '已读状态 0-未读 1-已读', `mid` bigint(20) unsigned NOT NULL DEFAULT 0 COMMENT '接收者ID', `type` tinyint(1) NOT NULL DEFAULT 0 COMMENT '消息类型 1-系统消息 2-创作邀请 100-其他', `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', PRIMARY KEY (`id`), UNIQUE KEY `uk_mid_msg` (`mid`, `msg_id`), KEY `ix_mtime` (`mtime`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COMMENT ='消息通知表'; 当创建一条系统通知后，数据插入到消息内容表。消息内容包含了发送渠道，根据发送渠道决定后续动作。我们先只针对站内通知做设计，其他渠道后续扩展即可。 如果是站内渠道，在插入消息内容后异步的插入记录到用户通知表。 扩展设计 针对全体用户发送通知时，可以根据一定策略过滤掉活跃度很低的用户 用户量较大，用户通知表可根据用户 id 分表 ","date":"2022-04-22","objectID":"/msg-system/:0:1","tags":["消息","方案设计","mysql"],"title":"消息系统方案设计之站内消息和事件提醒","uri":"/msg-system/"},{"categories":["service","backend","实现"],"content":"事件提醒 当用户点赞你的弹幕，回复了你的评论我们把这样的行为称之为事件。此类用户行为需要触达到对应的用户，就是事件提醒。当我们收到提醒时，还需要从提醒能跳转到事件发生的地方，比如某条弹幕，某条评论。 在我们的业务场景里定义了一类事件邀请 为此设计了事件表如下 事件表 CREATE TABLE `message_event` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键ID', `msg_id` varchar(32) NOT NULL DEFAULT '' COMMENT '消息ID', `event_id` varchar(32) NOT NULL DEFAULT '' COMMENT '事件ID', `type` tinyint(1) NOT NULL DEFAULT 0 COMMENT '事件类型 2-故事邀请', `action` varchar(20) NOT NULL DEFAULT '' COMMENT '动作类型', `receiver` bigint(20) unsigned NOT NULL DEFAULT 0 COMMENT '接收者ID', `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', PRIMARY KEY (`id`), KEY `ix_msg_id` (`msg_id`), KEY `ix_event_receiver` (`event_id`, `receiver`), KEY `ix_mtime` (`mtime`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COMMENT ='消息关联事件表'; 事件的提醒本质上还是消息的通知。所以事件的通知的内容和通知的状态还是依靠上面的两张表来存储。 在用户 A 发起对 用户 B 的邀请首选会往消息内容表插入一条数据记录邀请的具体内容，然后插入一条用户 B 的通知到消息通知表，最后消息和事件的关联关系会在事件表插入一条记录。 消息获取 在上面的设计下，从消息通知表就能取到用户的系统通知和事件提醒未读数量 select msg_id from message_notice where mid = 'xxx' and state = 0 group by type; 通过 msg_id 查到具体的消息内容 事件通知通过 msg_id 查到事件信息 select * from message_info where msg_id in (xxx, xxx) order by mtime desc; select * from message_event where msg_id in (xxx, xxx) order by mtime desc; 扩展设计 用户规模和消息规模比较庞大下用 redis 缓存提高性能 维护用户消息未读数量的缓存比如使用哈希表 如下 HMSET user_id_unread 1 10 2 20 (1, 2 表示消息类型) 维护用户已读消息的最大 id 比如 HMSET user_id_read 1 247 2 189 (1, 2 表示消息类型) 到此站内的系统消息和时间提醒的设计就介绍到这。后续随着消息种类的增加和用户规模的扩大我们再介绍进一步的方案设计。 ","date":"2022-04-22","objectID":"/msg-system/:0:2","tags":["消息","方案设计","mysql"],"title":"消息系统方案设计之站内消息和事件提醒","uri":"/msg-system/"},{"categories":["service","golang","实现"],"content":"在大部分的业务系统中权限控制是少不了的模块，它对这个系统的数据安全起着关键作用。既然权限系统或者模块这么重要且又很普适，当我们搭建一个权限系统是就没必要从头开始写起重复造轮子。在这篇文章里我们介绍一下如果利用开源的框架快速搭建一个权限系统，希望起到抛砖引玉的作用。 在此之前我们得了解目前常用的权限模型。 ","date":"2022-03-25","objectID":"/rbac-casbin/:0:0","tags":["权限系统","RBAC","Casbin"],"title":"利用 Casbin 高效实现基于 RBAC 的权限系统","uri":"/rbac-casbin/"},{"categories":["service","golang","实现"],"content":"权限模型 ACL(Access Control List) 权限直接赋予用户, 即每一个用户和某一项特定的权限绑定，如果是 n 个用户和 m 项权限，我们就需要创建 n*m 条权限记录。这种模式的缺点在于，但用户量达到一定量级的时候，那么就需要对每个用户都进行一次授权操作，那么这个工作量就会相当大。总体来说 ACL 对稍微复杂一点的场景就不太友好。 RBAC(Role-Based Access Control) 相比于 ACL 模型，RBAC 模型在用户与权限之间多了一个元素角色，通过权限关联角色、角色关联用户的方法来间接地赋予用户权限，从而实现用户与权限的解耦。RBAC 是目前权限系统中最为常用的模型。根据场景和需求复杂度不同，RBAC 又有几种变体。 基本的RBAC模型中定义了三个对象和两个关系，三个对象分别是：user，role，permission。一个用户可以属于多个角色，而一个角色下面也可以有多个用户。同样，角色和权限之间也是这种多对多的关系。关系如下图。 User（用户）：每个用户都有唯一的UID识别，并被授予不同的角色 Role（角色）：不同角色具有不同的权限 Permission（权限）：访问权限 用户-角色映射：用户和角色之间的映射关系 角色-权限映射：角色和权限之间的映射 RBAC 的进阶变体这里就不做详细介绍, 我们简要介绍一下区别 RBAC1：在角色中引入了继承的概念。简单理解就是，给角色可以分成几个等级，每个等级权限不同，从而实现更细粒度的权限管理。 RBAC2：建立在RBAC0基础之上，仅是对用户、角色和权限三者之间增加了一些限制。这些限制可以分成两类，即静态职责分离SSD(Static Separation of Duty)和动态职责分离DSD(Dynamic Separation of Duty) RBAC3：RBAC1和RBAC2的合集，所以RBAC3既有角色分层，也包括可以增加各种限制 总之根据 RBAC 模型去做设计基本上能满足我们对权限系统的要求。 ","date":"2022-03-25","objectID":"/rbac-casbin/:0:1","tags":["权限系统","RBAC","Casbin"],"title":"利用 Casbin 高效实现基于 RBAC 的权限系统","uri":"/rbac-casbin/"},{"categories":["service","golang","实现"],"content":"Casbin Casbin 是一个强大的、高效的开源访问控制框架，其权限管理机制支持多种访问控制模型。这里的访问控制模型就包括 RBAC。 casbin的核心是一套基于PERM metamodel(Policy,Effect,Request,Matchers)的 DSL。Casbin 从用这种 DSL 定义的配置文件中读取访问控制模型，作为后续权限验证的基础。更多原理方面的文档大家可以参看上述的文档链接。 ","date":"2022-03-25","objectID":"/rbac-casbin/:0:2","tags":["权限系统","RBAC","Casbin"],"title":"利用 Casbin 高效实现基于 RBAC 的权限系统","uri":"/rbac-casbin/"},{"categories":["service","golang","实现"],"content":"实现 功能 我们的权限控制目标是可以在中间件中，统一对用户进行鉴权，如果没有权限，中止继续访问，返回无权限的状态码，而如果有权限，则放行。 模型 定义 RBAC 模型 # Request definition 即为请求定义，代表了可以传入什么样的参数来确定权限 [request_definition] r = sub, obj, act # Policy definition 代表了规则的组成 [policy_definition] p = sub, obj, act # g 是一个 RBAC系统, _, _表示角色继承关系的前项和后项，即前项继承后项角色的权限 [role_definition] g = _, _ # Policy effect 则表示什么样的规则可以被允许, e = some(where (p.eft == allow)) 这句就表示当前请求中包含的任何一个规则被允许的话，这个权限就会被允许 [policy_effect] e = some(where (p.eft == allow)) # 是策略匹配程序的定义。表示请求与规则是如何起作用的 [matchers] m = g(r.sub, p.sub) \u0026\u0026 r.obj == p.obj \u0026\u0026 r.act == p.act 初始化 Casbin import ( \"strconv\" \"github.com/casbin/casbin/v2\" \"github.com/casbin/casbin/v2/model\" gormadapter \"github.com/casbin/gorm-adapter/v3\" \"gorm.io/gorm\" ) type Permit struct { *casbin.Enforcer } func NewPermit(db *gorm.DB) *Permit { // 使用 mysql 作为 policy 的存储；使用 gormadapter 作为 policy 的管理 api adapter, _ := gormadapter.NewAdapterByDB(db) // 加载 model 配置 m, _ := model.NewModelFromString(CasbinConf) enforcer, _ := casbin.NewEnforcer(m, adapter) enforcer.EnableAutoSave(true) enforcer.EnableAutoNotifyWatcher(true) return \u0026Permit{ Enforcer: enforcer, } } 中间件 func (p *Permit) Permit(action string) bm.HandlerFunc { return func(ctx *bm.Context) { var ( mid int64 err error obj string midStr string isPermit bool ) midI, ok := ctx.Get(\"mid\") if !ok { ctx.JSON(nil, ecode.NoLogin) ctx.Abort() return } // 用户 mid = midI.(int64) // 资源 obj = ctx.Request.Header.Get(\"OBJ-ID\") if obj == \"\" { ctx.JSON(nil, ecode.AppDenied) ctx.Abort() return } midStr = strconv.FormatInt(mid, 10) // 检查用户权限 isPermit, err = p.Enforce(midStr, obj, action) if err != nil { ctx.JSON(nil, err) ctx.Abort() return } if !isPermit { ctx.JSON(nil, ecode.AccessDenied) ctx.Abort() return } } } 常用操作 // 添加角色 _, err = s.permit.AddPolicy(roleName, roleId, action) // 赋予用户角色 _, err := s.permit.AddRoleForUser(user, roleName) // 查看具有某角色的所有用户 res, err := s.permit.GetUsersForRole(roleName) // 移除用户具有的角色 _, err := s.permit.DeleteRoleForUser(user, roleName) // 移除角色，跟角色相关联的用户都被移除 _, err = s.permit.DeleteRole(roleName) 下图是接口权限访问控制的示例，可以看到在请求头带上了资源我们就实现了资源权限访问控制 以上就是我们利用 Casbin 实现了基于 RBAC 的权限控制，可以看到成本非常低。在一个规模不是很大的系统中去做权限设计我相信这是一个高效的选择。 ","date":"2022-03-25","objectID":"/rbac-casbin/:0:3","tags":["权限系统","RBAC","Casbin"],"title":"利用 Casbin 高效实现基于 RBAC 的权限系统","uri":"/rbac-casbin/"},{"categories":["service","grpc","backend"],"content":"在我们的算法服务中，我们通过 gRPC server 对外提供服务。这里算法和一般的服务有一个很大的区别, 算法服务依赖于 GPU, 往往一次请求就能把单卡的性能吃到 80%。为了并发请求进来不互相影响我们往往会把 server 的处理线程池设为 1，这就意味着单个实例的并发数只有 1， 如果同一时刻实例接收到 2 个请求，则有一个请求会在 gRPC server 中排队。这样随之出现的问题就是如果先处理的请求耗时过长，那么排队的请求还来不及处理客户端就因为超时返回了。这种情况对于客户端来说得一直等到超时并且失败，非常不友好；再者客户端已经超时返回了，服务器还在处理已经返回的请求白白浪费资源。 在这篇文章里我分享一下我们的优化思路，希望能帮到有类似场景的同学。 ","date":"2022-02-27","objectID":"/p2c1/:0:0","tags":["负载均衡","服务治理","P2C"],"title":"算法服务稳定性优化之负载控制","uri":"/p2c1/"},{"categories":["service","grpc","backend"],"content":"快速失败 在上面我们提到在限制并发能力的情况下由于 gRPC server 的排队机制可能导致请求已经超时返回而服务器还在处理这个请求。很自然的想法就是我们把队列设置成 1, 同一时刻进来的多个请求只有一个会被处理，其他请求则返回失败。 这样降低了服务器的负载，但客户端的请求还是失败了，并没有从根本上提升用户的体验。 在服务器处理速率和请求速率相当的情况下，如果我们在快速失败的基础上能立即重试并且不让打到之前失败的实例上，这样能大大降低客户端的调用失败率。 所以，关键就是如果让重试的请求不打到失败的实例。这里我们用的负载均衡算法是 P2C。 P2C P2C 算法全称 Pick of 2 choices，相比 WRR，P2C 有着更科学的 LB 策略，它通过随机选择两个节点后在这俩节点里选择优胜者来避免羊群效应，并通过指数加权移动平均算法统计服务端的实时状态，从而做出最优选择。 我们这里不详细介绍 P2C，只是把该算法节点选择的过程说清楚，让大家便于明白重试请求不容易打到失败的实例。 核心逻辑 在多个节点中随机选择两个节点:会随机的选择3次，如果其中一次选择的节点的健康条件满足要求，就中断选择，采用这两个节点 比较 nodeA、nodeB 两个节点，选出负载最低（综合请求响应时间、服务器 cpu 负载、正在处理的请求数据、请求成功率）的节点作为被选中的节点 下图说明了这个选择过程 关键实现 在这里我们介绍过实现自定义 gRPC 负载均衡器的关键就是 实现 google.golang.org/grpc/balancer/base/base.go/PickerBuilder 接口, 这个接口是有服务节点更新的时候会调用接口里的 Build 方法 type PickerBuilder interface { // Build returns a picker that will be used by gRPC to pick a SubConn. Build(info PickerBuildInfo) balancer.Picker } 还要实现 google.golang.org/grpc/balancer/balancer.go/Picker 接口。这个接口主要实现负载均衡，挑选一个节点供请求使用 type Picker interface { Pick(info PickInfo) (PickResult, error) } 注册自己实现的负载均衡器 重点看一下节点筛选逻辑 选择两个节点 // choose two distinct nodes // 总循环 3 次，随机从 subConns []*subConn 中选择两个节点 nodeB 和 nodeA， // 如果满足 node.valid() 的要求，直接返回，不满足的话，返回最后一次的选择的两个节点 func (p *p2cPicker) prePick() (nodeA *subConn, nodeB *subConn) { for i := 0; i \u003c 3; i++ { p.lk.Lock() a := p.r.Intn(len(p.subConns)) b := p.r.Intn(len(p.subConns) - 1) p.lk.Unlock() if b \u003e= a { b = b + 1 } nodeA, nodeB = p.subConns[a], p.subConns[b] if nodeA.valid() || nodeB.valid() { break } } return } 二选一 func (p *p2cPicker) pick(ctx context.Context, opts balancer.PickInfo) (balancer.PickResult, error) { var pc, upc *subConn start := time.Now().UnixNano() if len(p.subConns) \u003c= 0 { return balancer.PickResult{}, balancer.ErrNoSubConnAvailable } else if len(p.subConns) == 1 { pc = p.subConns[0] } else { nodeA, nodeB := p.prePick() // meta.Weight为服务发布者在disocvery中设置的权重 if nodeA.load()*nodeB.health()*nodeB.meta.Weight \u003e nodeB.load()*nodeA.health()*nodeA.meta.Weight { pc, upc = nodeB, nodeA } else { pc, upc = nodeA, nodeB } // 如果选中的节点，在forceGap期间内没有被选中一次，那么强制一次 // 利用强制的机会，来触发成功率、延迟的衰减 // 原子锁conn.pick保证并发安全，放行一次 pick := atomic.LoadInt64(\u0026upc.pick) if start-pick \u003e forceGap \u0026\u0026 atomic.CompareAndSwapInt64(\u0026upc.pick, pick, start) { pc = upc } } log.Warn(\"node inflight: %s, %d \\n\", pc.addr.Addr, atomic.LoadInt64(\u0026pc.inflight)) // 节点未发生切换才更新pick时间 if pc != upc { atomic.StoreInt64(\u0026pc.pick, start) } atomic.AddInt64(\u0026pc.inflight, 1) atomic.AddInt64(\u0026pc.reqs, 1) 错误实例被选择概率大大降低 在下面的这段代码我们可以看到当请求调用发生错误时，节点健康值 success 会变的很小几乎为0 (采用指数加权移动平均算法更新)，这样当请求失败立即重试有极大的概率被路由到空闲的实例上。 if di.Err != nil { if st, ok := status.FromError(di.Err); ok { // only counter the local grpc error, ignore any business error if st.Code() != codes.Unknown \u0026\u0026 st.Code() != codes.OK { success = 0 } } } oldSuc := atomic.LoadUint64(\u0026pc.success) success = uint64(float64(oldSuc)*w + float64(success)*(1.0-w)) atomic.StoreUint64(\u0026pc.success, success) ... 下图是快速失败 + 立即重试策略的测试结果, 图中右半部分可以看到在使用了优化策略的情况下错误率大大降低，并且响应时间没有明显增加。 ","date":"2022-02-27","objectID":"/p2c1/:0:1","tags":["负载均衡","服务治理","P2C"],"title":"算法服务稳定性优化之负载控制","uri":"/p2c1/"},{"categories":["service","grpc","backend"],"content":"优选空闲节点 在 P2C 算法中我们看到在比较节点的负载的时候是下面的方法, 其中 inflight 表示节点正在处理的请求数 func (sc *subConn) load() uint64 { lag := uint64(math.Sqrt(float64(atomic.LoadUint64(\u0026sc.lag))) + 1) load := atomic.LoadUint64(\u0026sc.svrCPU) * lag * uint64(atomic.LoadInt64(\u0026sc.inflight)) if load == 0 { // penalty是初始化没有数据时的惩罚值，默认为1e9 * 250 load = penalty } return load } 既然我们有 inflight 这个指标，那我们选择节点的时候只选择 inflight=1的节点是不是就能完美解决上面的问题。理论上是的，但是实际上服务器就是成本，在满足业务需求的时候我们尽可能降低服务器成本，那么在这种情况下在生产环境很难一直有空闲的节点。所以在成本和性能的 trade off 下，我们使用的策略是在挑选两节点的阶段在 3次循环中优先挑选 inflight=1的节点。如果三次都没有符合条件的节点就退化成普通的 P2C。 如代码所示 func (p *p2cPicker) prePick() (nodeA *subConn, nodeB *subConn) { for i := 0; i \u003c 3; i++ { p.lk.Lock() a := p.r.Intn(len(p.subConns)) b := p.r.Intn(len(p.subConns) - 1) p.lk.Unlock() if b \u003e= a { b = b + 1 } nodeA, nodeB = p.subConns[a], p.subConns[b] if nodeA.valid() || nodeB.valid() { break } } return } func (sc *subConn) valid() bool { return sc.health() \u003e 500 \u0026\u0026 atomic.LoadUint64(\u0026sc.svrCPU) \u003c 900 \u0026\u0026 atomic.LoadInt64(\u0026sc.inflight) \u003c=1 } 这种优化策略，我们把他起名为 P2C1。 下图是我们在快速失败的基础上使用了进一步的优化策略 P2C1, 可以看到图中第三部分错误请求基本没有了。我们的优化起到了良好的效果。 希望这篇文章里的优化思路和方法能给其他有类似限制场景的同学提供参考。 ","date":"2022-02-27","objectID":"/p2c1/:0:2","tags":["负载均衡","服务治理","P2C"],"title":"算法服务稳定性优化之负载控制","uri":"/p2c1/"},{"categories":["service","backend","工程效率"],"content":"当我们对外提供服务的时候调用方经常会问这样一个问题：你们的服务能提供多大的 QPS？ 那么就要求我们在服务上线之前就应该清楚的知道服务的极限 QPS 是多少。大家都知道这个数值的得来就得靠压测。在这里我就给大家介绍一款简单易用并且自带可视化界面的压测工具-Locust。 ","date":"2022-01-29","objectID":"/locust/:0:0","tags":["压测","性能"],"title":"易于使用的性能测试工具-Locust","uri":"/locust/"},{"categories":["service","backend","工程效率"],"content":"Locust 优点 测试场景脚本化 Locust 采用 Python 脚本描述测试场景，对于最常见的HTTP(S)协议的系统，Locust 采用 Python 的 requests 库作为客户端，使得脚本编写大大简化。而对于其它协议类型的系统，Locust也提供了接口，只要我们能采用 Python 编写对应的请求客户端，就能方便地采用Locust实现压力测试。 协程并发机制 采用多线程来模拟多用户时，线程数会随着并发数的增加而增加，而线程之间的切换是需要占用资源的，IO 的阻塞和线程的 sleep 会不可避免的导致并发效率下降。Locust 的并发机制摒弃了进程和线程，采用协程（gevent）的机制。协程避免了系统级资源调度，由此可以大幅提高单机的并发能力。 ","date":"2022-01-29","objectID":"/locust/:0:1","tags":["压测","性能"],"title":"易于使用的性能测试工具-Locust","uri":"/locust/"},{"categories":["service","backend","工程效率"],"content":"Locust 使用 安装 就和其他 Python 包一样，安装非常简单 pip install locust # check 版本 locust -V 使用示例 编写一个 locustfile，定义测试场景 import time from locust import HttpUser, task, between # 这个类是我们用来模拟发起请求的用户 # 此类继承自 HttpUser, HttpUser 类包含的 client 属性就是一个用来发 Http 请求的 HttpSession # 当开始运行测试，locust 会给每个模拟的 user 创建一个此类的实例 class QuickstartUser(HttpUser): # wait_time 定义了模拟用户的请求间隔时间，这里的含义是每个 user 会在一个请求结束后随机等 1-5s wait_time = between(1, 5) # task 装饰架定义了模拟用户的行为，即发送什么样的 http 请求 # 每个 user 都会起一个 greenlet 执行这些方法 @task def hello_world(self): self.client.get(\"/hello\") self.client.get(\"/world\") # 这里的 3 是定义了这个待执行的行为相比上一个具有更高的优先级，或者说这个行为被执行的概率是上一个的 3 倍 @task(3) def view_items(self): for item_id in range(10): self.client.get(f\"/item?id={item_id}\", name=\"/item\") # 在每个 user 执行真的方法之前，定义的初始化操作，比如这里的模拟用户登录，那么后续用户就会保持响应的 session def on_start(self): self.client.post(\"/login\", json={\"username\":\"foo\", \"password\":\"bar\"}) 重点方法 wait_time constant: 请求间隔固定时间 between: 在最大值和最小值之间随机 constant_throughput: 每秒触发固定次数 task constant_pacing: 固定秒数里执行一次 task(其实就是上面指标的倒数) client client 是 HttpSession 的一个实例， 我们可以理解成它就是 Python 包 requests.Session 的扩展版。所以我们完全可以按照使用 requests的方式使用 client。 例如 self.client.post(url=url, data=data_param, headers=headers, catch_response=True) 验证 response 我们经常需要根据接口的实际返回内容来看某一次请求是否成功。下面的例子展示了在 locustfile 中如何定义测试的失败或者成功 with self.client.get(\"/\", catch_response=True) as response: if response.text != \"Success\": response.failure(\"Got wrong response\") elif response.elapsed.total_seconds() \u003e 0.5: response.failure(\"Request took too long\") 请求分组 经常会遇到某一个接口参数是动态的，比如根据 item id 查询 item /item?id=abc\"。 默认情况下测试统计数据里 id 不同则每个请求都单独展示，而这一般不是我们想要到结果，我们往往只关注这一个接口的统计情况，所以需要把这样的请求分组。我们可以通过参数 name进行请求分组 例如 # Statistics for these requests will be grouped under: /blog/?id=[id] for i in range(10): self.client.get(\"/blog?id=%i\" % i, name=\"/blog?id=[id]\") 代码组织 我们经常会压测很多项目，把这些 locustfile 组织到一起是一个很好的实践方式。 下面是一种推荐方式 * Project root * common/ * __init__.py * auth.py * config.py * my_locustfiles/ * api.py * website.py * requirements.txt ","date":"2022-01-29","objectID":"/locust/:0:2","tags":["压测","性能"],"title":"易于使用的性能测试工具-Locust","uri":"/locust/"},{"categories":["service","backend","工程效率"],"content":"完整的示例 #!/usr/bin/env python3 # -*- coding: utf-8 -*- # 用locust import json import logging import time import requests from locust import HttpUser, between, task logHandler = logging.FileHandler('../logs/test.log') logger = logging.getLogger() logger.addHandler(logHandler) logger.setLevel(logging.INFO) class MyUser(HttpUser): wait_time = between(0.3, 0.5) @task def my_task(self): url = \"http://example.co/byp/api/interface/tts/task\" payload = { \"raw_params\": { \"biz_id\": \"tts389693391320943355\", \"voice\": \"db8\", \"method\": 0, \"logid\": \"\", \"text\": [ \"阿Test正经比比\" ], \"format\": \"wav\", \"sample_rate\": 22050, \"volume\": 50, \"speech_rate\": 0, \"pitch_rate\": 0 } } headers = { 'Content-Type': 'application/json' } res = requests.post(url=url, data=json.dumps(payload), headers=headers) task_id = res.json()['data']['task_id'] st = time.time() # logger.info(f'task: {task_id} {time.time()}') time.sleep(10) res_url = 'http://example.co/byp/api/interface/tts/task/result' with self.client.get(url=f'{res_url}?task_id={task_id}', name=\"/result\", catch_response=True) as response: logger.info(f'task res:{task_id} {time.time()-st}') res = response.json() if res['code'] == 0 and res['data']['state'] == 4: response.success() else: response.failure(res.get('data')) logger.error(f'{task_id}') 在上面的这个 locustfile 里我们模拟的场景是用户提交任务立即返回，服务进行异步处理。在 10s 内如果能查询到任务结果我们认为对用户来说都是服务可用的。 压测 $ locust -f locustfile.py [2022-01-26 19:15:18,393] MacBook-Pro-3.local/INFO/locust.main: Starting web interface at http://0.0.0.0:8089 (accepting connections from all network interfaces) [2022-01-26 19:15:18,399] MacBook-Pro-3.local/INFO/locust.main: Starting Locust 1.6.0 [2022-01-26 19:15:37,970] MacBook-Pro-3.local/INFO/locust.runners: Spawning 50 users at the rate 30 users/s (0 users already running)... [2022-01-26 19:15:39,692] MacBook-Pro-3.local/INFO/locust.runners: All users spawned: MyUser: 50 (50 total running) [2022-01-26 19:15:48,024] MacBook-Pro-3.local/INFO/root: task res:ts392026726928003325 10.019779205322266 [2022-01-26 19:15:48,044] MacBook-Pro-3.local/INFO/root: task res:ts392026726961557757 10.017950057983398 压测结果 我们通过多次调整 user 数找到了在不出现失败的情况下稳定的服务 QPS。 其他场景或者需求可以查阅官方文档进一步探索。 ","date":"2022-01-29","objectID":"/locust/:0:3","tags":["压测","性能"],"title":"易于使用的性能测试工具-Locust","uri":"/locust/"},{"categories":["backend","infra"],"content":"一个服务里既有同步逻辑又有异步逻辑是非常常见的事，比如可以通过 Http 接口调用服务或者通过消息队列传递消息来实现同样的服务逻辑，同一套代码逻辑区别只是入口不一样。 在 Golang 服务里我们用 goroutine 非常容易的起两个入口；在 Python 服务里我们可以用多进程也容易实现。 现在我们看另外一种场景，服务同样需要对外提供同步和异步入口，但是服务本身是计算密集型的且非常占用资源，典型的例子比如使用 GPU 的推理服务。首先推理服务启动时需要加载模型，而这一般需要很大的显存可能达到数个 G 甚至超过十个 G。但是我们的单张显卡显存当然是有上限的，就拿 T4 来说显存只有 15个G 可用，所以一个推理服务同时启动同步和异步可能显存就不够；影响更严重的一个问题是即使显存足够同时启动同步和异步服务，但是当同步和异步同时进行计算是，对显卡的计算资源会存在争夺的情况，那么这样计算速度可能会大大降低，严重降低了推理服务的性能。 为了解决上述问题一个很自然的办法就是同步和异步分别部署一套服务。对于一个这样的服务说是没问题的我们多加一个服务节点、一套 CI/CD 配置、一套服务配置、一套应用配置等就完成了。但是有这样需求的服务有多个，并且数目会持续增加，那么分别部署同步和异步的成本和服务维护的成本就急剧增加。 在这篇文章里我就分享下我们的解决办法，如何在一个服务里(一个节点，一套配置)独立运行同步和异步实例。（这里的实例指的是在 K8s 中一个 deployment 里的一个 Pod） ","date":"2021-12-26","objectID":"/service-manage/:0:0","tags":["服务治理","k8s"],"title":"用 StatefulSet 实现同步服务和异步服务的管理","uri":"/service-manage/"},{"categories":["backend","infra"],"content":"精确实例数量 一个服务的总实例数目是通过配置定义的。假如我们配置的服务实例数是 10，然后在这10个实例当中一部分是同步实例，一部分是异步实例。如何保证这些实例数量是按照我们要求分布的呢？ 按比例随机 假如同步和异步的实例数比例是 4:6, 在服务启动脚本里我们可以生成一个 1-10 的随机整数然后跟同步实例数比例比较，如果小于等于 4 则以同步的方式启动实例，否则以异步的方式启动。 示例脚本如下 #!/bin/bash # 同步实例数比例(通过环境变量注入) SYNCS=$SYNC_RATE # 默认启动同步实例 if [ -z \"$SYNCS\" ]; then SYNCS=10 fi Rand=$(( ( RANDOM % 10 ) + 1 )) if [ \"$Rand\" -le \"$SYNCS\" ]; then # 启动grpc server(同步） python /data/app/grpc_server/server.py else # 启动任务进程(异步） python /data/app/start/__init__.py fi 很明显随机启动无法准确的控制实例数量，尤其是总实例数目较少的时候实例分布偏差较大。 StatefulSet 我们知道在 K8s 中除了常用的服务形式 Deployment 还有一种比较常用的服务形式是 StatefulSet, 后者相比前者来说最显著的一个区别是 StatefulSet 下的 Pod 名字是也有序号的。例如一个 StatefulSet 形式服务名是 web-service 且有 3个实例，那么 3个实例的名字分别是 web-service-0、web-service-1、web-service-2。 我们可以正好利用 StatefulSet 这种 Pod 按编号顺序启动的方式来控制同步和异步实例启动的数量。思路就是比如我们设置好了同步实例的数量，那么在每次实例启动的时候我们可以根据实例的编号和同步实例数做比较，如果编号比同步数小那么就以同步的方式启动实例，反之则以异步的方式启动。 示例脚本如下 #!/bin/bash # pod 编号 POD_NUM=`echo ${POD_NAME} | awk -F'-' '{print $NF}'` # 同步实例数(通过环境变量注入) SYNCS=$SYNC_INSTANCE # 默认同步方式(INSTANCE是期望的总实例数) if [ -z \"$SYNCS\" ]; then SYNCS=$INSTANCE fi if [ \"$POD_NUM\" -lt \"$SYNCS\" ]; then # 启动grpc server(同步） python /data/app/grpc_server/server.py else # 启动任务进程(异步） python /data/app/start/__init__.py fi 所以通过上面的描述我们看到可以利用 StatefulSet 来精确控制同步和异步实例数量。 ","date":"2021-12-26","objectID":"/service-manage/:0:1","tags":["服务治理","k8s"],"title":"用 StatefulSet 实现同步服务和异步服务的管理","uri":"/service-manage/"},{"categories":["backend","infra"],"content":"容器多进程管理 在我们上面的例子中因为服务同时包含了同步实例和异步实例，同步实例有暴露端口的需求而异步实例是没有对外暴露端口的，这样带来的矛盾就是如果服务配置了端口，则我们的基础平台会对异步服务配置的端口健康检查和服务注册，结果就是异步服务必然健康检查失败而启动失败。为了绕过基础平台的功能我们决定对服务不配置暴露端口，同步服务自己实现服务注册的功能。即我们在同步服务中除了启动服务进程之外，再启动一个服务注册管理进程，实现服务注册、服务心跳和服务注销功能。 上面这个情况比较特殊，可能其他同学并不会遇到，我们拿这个例子是为了说明在 K8s 容器中有多个进程该怎么管理。上面说到除了服务进程之外，容器中还存在一个服务注册进程，这个进程必须实现在容器销毁的时候必须向注册中心注销服务实例。 在 K8s 中，Pod 停止时 kubelet 会先给容器中的主进程发 SIGTERM 信号来通知进程进行 shutdown 以实现优雅停止，如果超时进程还未完全停止则会使用 SIGKILL 来强行终止。但是在我们的场景中我们的业务进程是在脚本中启动的，容器的启动入口使用了脚本，所以容器中的主进程并不是我们所希望的业务进程而是 shell 进程，导致业务进程收不到 SIGTERM 信号，自然就无法实现主动注销服务。 我们利用 trap 来实现 trap 捕捉信号 通常trap都在脚本中使用，主要有2种功能： 忽略信号。当运行中的脚本进程接收到某信号时(例如误按了CTRL+C)，可以将其忽略，免得脚本执行到一半就被终止 捕捉到信号后做相应处理，比如清理创建的临时文件 常用信号 Signal Value Comment ───────────────────────────── SIGHUP 1 终止进程，特别是终端退出时，此终端内的进程都将被终止 SIGINT 2 中断进程，几乎等同于sigterm，会尽可能的释放执行clean-up， 释放资源，保存状态等(CTRL+C) SIGQUIT 3 从键盘发出杀死(终止)进程的信号 SIGKILL 9 强制杀死进程，该信号不可被捕捉和忽略，进程收到该信号后不会 执行任何clean-up行为，所以资源不会释放，状态不会保存 SIGTERM 15 杀死(终止)进程，几乎等同于sigint信号，会尽可能的释放执行 clean-up，释放资源，保存状态等 SIGSTOP 19 该信号是不可被捕捉和忽略的进程停止信息，收到信号后会进入stopped状态 SIGTSTP 20 该信号是可被忽略的进程停止信号(CTRL+Z) trap 使用 trap [-lp] [[arg] signal_spec ...] -l 打印信号名称以及信号名称对应的数字。 -p 显示与每个信号关联的trap命令。 参数 arg：接收到信号时执行的命令或函数 signal_spec：信号名称或信号名称对应的数字 通过上述介绍之后我们知道给容器多进程传递信号方式为：可以在 shell 中使用 trap 来捕获信号，当收到信号后触发回调函数来将信号通过 kill 传递给业务进程。 示例脚本如下 #!/bin/bash # pod 编号 POD_NUM=`echo ${POD_NAME} | awk -F'-' '{print $NF}'` # 同步实例数 SYNCS=$SYNC_INSTANCE if [ -z \"$SYNCS\" ]; then SYNCS=$INSTANCE fi if [ \"$POD_NUM\" -lt \"$SYNCS\" ]; then # 启动grpc server python /data/app/grpc_server/server.py \u0026 pid1=\"$!\" python /data/app/grpc_server/register.py \u0026 pid2=\"$!\" handle_sigterm() { echo \"[INFO] Received SIGTERM\" kill -SIGTERM $pid1 $pid2 # 传递 SIGTERM 给业务进程 wait $pid1 $pid2 # 等待所有业务进程完全终止 } trap handle_sigterm TERM # 捕获 SIGTERM 信号并回调 handle_sigterm 函数 wait # 等待回调执行完，主进程再退出 else # 启动任务进程 python /data/app/start/__init__.py fi 下图是实现的效果 ","date":"2021-12-26","objectID":"/service-manage/:0:2","tags":["服务治理","k8s"],"title":"用 StatefulSet 实现同步服务和异步服务的管理","uri":"/service-manage/"},{"categories":["原理","backend","infra"],"content":"消息队列是服务端必不可少的组件，其中 Kafka 可以说是数一数二的选择，对于大部分服务端的同学来说 Kafka 也是最熟悉的消息中间件之一。而当我们在生产上遇到 kafka 的使用问题时想要透过现象看到问题的本质，从而找到解决问题的办法。这就要求对 kafka 的设计和实现有这较为深刻的认识。在这篇文章里我们就以生产实际的例子来展开讨论 Kafka 在消费端中的一个重要设计 consumer group 的 rebalance。只有理解了 rebalance 我们才能对消息消费过程有着更全面的掌握。 某一天我们收到消费端消费严重落后生产的告警。第一时间相关同学去看了 consumer group 的消费曲线监控，消费速率明显出现异常。下面这张示意图展示了这种情况。 我们能清楚的看到整个消费组在消费异常的时间段内经常出现消费停滞的情况如图上消费速率为 0。为什么消费会卡主呢？同事去看了相关服务的日志看到很多 err kafka data maybe rebalancing。看了这篇文章后消费卡主的问题自然就知道答案了。 ","date":"2021-11-20","objectID":"/kafka-rebalance/:0:0","tags":["kafka","rebalance","消息队列"],"title":"从一个消费慢的例子深入理解 kafka rebalance","uri":"/kafka-rebalance/"},{"categories":["原理","backend","infra"],"content":"重要概念 为了说清楚 rebalance 有必要把最相关的重要概念回顾一下 Consumer Group consumer group 是 kafka 提供的可扩展且具有容错性的消费者机制。既然是一个组，那么组内必然可以有多个消费者或消费者实例(consumer instance)，它们共享一个公共的 ID，即 group ID。组内的所有消费者协调在一起来消费 topic 下的所有分区。总结一下就是以下几个关键点。 consumer group 下可以有一个或多个 consumer instance，consumer instance可以是一个进程，也可以是一个线程 group.id 是一个字符串，唯一标识一个 consumer group consumer group 订阅的 topic 下的每个分区只能分配给某个 group 下的一个 consumer (当然该分区还可以被分配给其他 group) Coordinator Group Coordinator 是一个服务，每个 Broker 在启动的时候都会启动一个该服务。Group Coordinator 的作用是用来存储 Group 的相关 Meta 信息，并将对应 Partition 的 Offset 信息记录到 Kafka 内置 Topic(__consumer_offsets)中。 每个 Group 都会选择一个 Coordinator 来完成自己组内各 Partition 的 Offset 信息，选择的规则如下： 1，计算 Group 对应在__consumer_offsets 上的 Partition 2，根据对应的 Partition 寻找该 Partition 的 leader 所对应的 Broker，该 Broker 上的 Group Coordinator 即就是该 Group 的 Coordinator Partition 计算规则 partition-Id(__consumer_offsets) = Math.abs(groupId.hashCode() % groupMetadataTopicPartitionCount) 其中groupMetadataTopicPartitionCount对应offsets.topic.num.partitions参数值，默认值是50个分区 ","date":"2021-11-20","objectID":"/kafka-rebalance/:0:1","tags":["kafka","rebalance","消息队列"],"title":"从一个消费慢的例子深入理解 kafka rebalance","uri":"/kafka-rebalance/"},{"categories":["原理","backend","infra"],"content":"Rebalance 目的 我们知道 topic 的 partition 已经根据策略分配给了 consumer group 下的各个 consumer。那么当有新的 consumer 加入或者老的 consumer 离开这个 partition 与 consumer 的分配关系就会发生变化，如果这个时候不进行重新调配，就可能出现新 consumer 无 partition 消费或者有 partition 无消费者的情况。那么这个重新调配指的就是 consumer 和 partition 的 rebalance。 consumer默认提供了2种分配策略： range 策略：将单个 topic 的分区按顺序排列，然后把这些分区划分成固定大小的分区段并依次分配给每个 consumer。 round-robin：把 topi c的所有分区按顺序排开，以轮询的方式配给每个 consumer。 下面给一个简单的例子，假设目前某个 consumer group A 有 2个 consumer C1 和 C2，当 C3 加入时，触发了 rebalance 条件，coordinator 会进行 rebalance，根据range 策略重新分配了 partition。 ","date":"2021-11-20","objectID":"/kafka-rebalance/:0:2","tags":["kafka","rebalance","消息队列"],"title":"从一个消费慢的例子深入理解 kafka rebalance","uri":"/kafka-rebalance/"},{"categories":["原理","backend","infra"],"content":"Rebalance 时机 Rebalance 在以下情况会触发 consume group 中的成员个数发生变化。例如有新的 consumer 实例加入该消费组或者离开组 订阅 Topic 的分区数发生变化 取消订阅 Topic 或新增订阅 Topic ","date":"2021-11-20","objectID":"/kafka-rebalance/:0:3","tags":["kafka","rebalance","消息队列"],"title":"从一个消费慢的例子深入理解 kafka rebalance","uri":"/kafka-rebalance/"},{"categories":["原理","backend","infra"],"content":"Rebalance 过程 kafka 中的重要设计也会随着版本的升级而优化。rebalance 也不例外，这里我们介绍的 kafka rebalance 流程以我们的线上版本 1.1.1 为例。 当前 consumer 准备加入 consumer group 或 GroupCoordinator 发生故障转移时，consumer 并不知道 GroupCoordinator 的 host 和 port，所以 consumer 会向 Kafka 集群中的任一 broker 节点发送 FindCoordinatorRequest 请求，收到请求的 broker 节点会返回 ConsumerMetadataResponse 响应，其中就包含了负责管理该 Consumer Group 的 GroupCoordinator 的地址 当 consumer 通过 FindCoordinatorRequest 查找到其 Consumer Group 对应的 GroupCoordinator 之后，就会进入 Join Group 阶段 Consumer 先向 GroupCoordinator 发送 JoinGroupRequest 请求，其中包含 consumer 的相关信息 GroupCoordinator 收到 JoinGroupRequest 后会暂存该 consumer 信息，然后等待全部 consumer 的 JoinGroupRequest 请求。JoinGroupRequest 中的 session.timeout.ms 和 rebalance_timeout_ms（ max.poll.interval.ms）决定了 consumer 如果没有响应过多久会被踢出出组 GroupCoordinator 会根据全部 consumer 的 JoinGroupRequest 请求来确定 Consumer Group 中可用的 consumer，从中选取一个 consumer 成为 Group Leader，同时还会决定 partition 分配策略，最后会将这些信息封装成 JoinGroupResponse 返回给 Group Leader Consumer 每个 consumer 都会收到 JoinGroupResponse 响应，但是只有 Group Leader 收到的 JoinGroupResponse 响应中封装的所有 consumer 信息以及 Group Leader 信息。当其中一个 consumer 确定了自己的 Group Leader后，会根据 consumer 信息、kafka 集群元数据以及 partition 分配策略计算 partition 的分片结果。其他非 Group Leader consumer 收到 JoinResponse 为空响应，也就不会进行任何操作，只是原地等待 接下来，所有 consumer 进入 Synchronizing Group State 阶段，所有 consumer 会向 GroupCoordinator 发送 SyncGroupRequest。其中，Group Leader Consumer 的 SyncGroupRequest 请求包含了 partition 分配结果，普通 consumer 的 SyncGroupRequest 为空请求 GroupCoordinator 接下来会将 partition 分配结果封装成 SyncGroupResponse 返回给所有 consumer, consumer 收到 SyncGroupResponse 后进行解析，就可以明确 partition 与 consumer 的映射关系 后续 consumer 还是会与 GroupCoordinator 保持定期的心跳(heartbeat.interval.ms)。当 rebalance 正在进行中 coordinator 会通过 hearbeat response 告诉 consumers 是否要 rejoin group 触发。即心跳响应中包含 IllegalGeneration 异常 ","date":"2021-11-20","objectID":"/kafka-rebalance/:0:4","tags":["kafka","rebalance","消息队列"],"title":"从一个消费慢的例子深入理解 kafka rebalance","uri":"/kafka-rebalance/"},{"categories":["原理","backend","infra"],"content":"Rebalance 问题 在整个 rebalance 的过程中，所有 partition 都会被回收，consumer 是无法消费任何 partition 的。Join 阶段会等待原先组内存活的成员发送 JoinGroupRequest 过来，如果原先组内的成员因为业务处理一直没有发送请求过来，服务端就会一直等待，直到超时。这个超时时间就是 max.poll.interval.ms 的值，默认是5分钟，因此这种情况下 rebalance 的耗时就会长达5分钟，导致所有消费者都无法进行正常消费，这对生产来说是个很大的问题。 ","date":"2021-11-20","objectID":"/kafka-rebalance/:0:5","tags":["kafka","rebalance","消息队列"],"title":"从一个消费慢的例子深入理解 kafka rebalance","uri":"/kafka-rebalance/"},{"categories":["原理","backend","infra"],"content":"Rebalance 改进 Static Membership 为了减少因为 consumer 短暂不可用造成的 rebalance，kafka 在 2.3 版本中引入了 Static Membership。 Static Membership 优化的核心是： 在 consumer 端增加 group.instance.id 配置（group.instance.id 是 consumer 的唯一标识）。如果 consumer 启动的时候明确指定了 group.instance.id 配置值，consumer 会在 JoinGroup Request 中携带该值，表示该 consumer 为 static member。 为了保证 group.instance.id 的唯一性，我们可以考虑使用 hostname、ip 等。 在 GroupCoordinator 端会记录 group.instance.id → member.id 的映射关系，以及已有的 partition 分配关系。当 GroupCoordinator 收到已知 group.instance.id 的 consumer 的 JoinGroup Request 时，不会进行 rebalance，而是将其原来对应的 partition 分配给它。 Static Membership 可以让 consumer group 只在下面的 4 种情况下进行 rebalance： 有新 consumer 加入 consumer group Group Leader 重新加入 Group 时 consumer 下线时间超过阈值（session.timeout.ms） GroupCoordinator 收到 static member 的 LeaveGroup Request 这样的话，在使用 Static Membership 场景下，只要在 consumer 重新启动的时候，不发送 LeaveGroup Request 且在 session.timeout.ms 时长内重启成功，就不会触发 rebalance。所以，这里推荐设置一个足够 consumer 重启的时长 session.timeout.ms，这样能有效降低因 consumer 短暂不可用导致的 reblance 次数。 Incremental Cooperative Rebalancing 从名字中我们就能看出这个版本的 rebalance 过程两个关键词增量和协作，增量指的是原先版本的 rebalance 被分解成了多次小规模的 rebalance, 协作自然指的是 consumer 之间的关系。 核心思想： consumer 比较新旧两个 partition 分配结果，只停止消费回收（revoke）的 partition，对于两次都分配给自己的 partition，consumer 不需要停止消费 通过多轮的局部 rebalance 来最终实现全局的 rebalance 我们以文章开始的例子来理解一下这个版本的改进 首先 C1 -\u003e {P0, p3} C2 -\u003e {P1} C3 -\u003e {P2} 这是 consumer 和 partition 的分配关系，我们假设 C2 宕机超过了 session.timeout.ms, 此时 GroupCoordinator 会触发第一轮 rebalance 第一轮 Rebalance GroupCoordinator 会在下一轮心跳响应中通知 C1 和 C3 发起第一轮 rebalance C1 和 C3 会将自己当前正在处理的 partition 信息封装到 JoinGroupRequest 中（metadata 字段）发往 GroupCoordinator： C1 发送的 JoinGroupRequest（assigned: P0、P3) C3 发送的 JoinGroupRequest（assigned: P2） 假设 GroupCoordinator 在这里选择 1 作为 Group Leader，GroupCoordinator 会将 partition 目前的分配状态通过 JoinGroupResponse 发送给 C1 C1 发现 P1 并未出现（处于 lost 状态），此时 C1 并不会立即解决当前的不平衡问题，返回的 partition 分配结果不变（同时会携带一个 delay 时间，scheduled.rebalance.max.delay.ms，默认 5 分钟）。GroupCoordinator 会根据 C1 的 SyncGroupRequest，生成 SyncGroupResponse 返回给两个存活的 consumer C1 收到的 SyncGroup Response（delay，assigned: P0、P3，revoked：） C3 收到的 SyncGroup Response（delay，assigned: P2，revoked：） 到此为止，第一轮 rebalance 结束。整个 rebalance 过程中，C1 和 C3 并不会停止消费。 第二轮 Rebalance 在 scheduled.rebalance.max.delay.ms 这个时间段内，C2 故障恢复，重新加入到 consumer group 时，会向 GroupCoordinator 发送 JoinGroup Request，触发第二轮的 rebalance。GroupCoordinator 在下一次心跳响应中会通知 C1 和 C3 参与第二轮 rebalance C1 和 C3 在收到心跳之后，会发送 JoinGroupRequest 参与第二轮 rebalance： C1 发送的 JoinGroupRequest（assigned: P0、P3） C3 发送的 JoinGroupRequest（assigned: P2) 在第二轮 rebalance 中，C1 依旧被选为 Group Leader，它会检查 delay 的时间（scheduled.rebalance.max.delay.ms）是否已经到了，如果没到，则依旧不会立即解决当前的不平衡问题，继续返回目前的分配结果，并且返回的 SyncGroupResponse 中更新了 delay 的剩余时间（remaining delay = delay - pass_time) 到此为止，第二轮 rebalance 结束。整个 rebalance 过程中，C1 和 C3 并不会停止消费。 C1 收到的 SyncGroup Response（remaining delay，assigned: P0、P3，revoked：） C2 收到的 SyncGroup Response（remaining delay，assigned:，revoked：） C3 收到的 SyncGroup Response（remaining delay，assigned: P2，revoked：） 第三轮 Rebalance 当 remaining delay 时间到期之后，consumer 全部重新送 JoinGroupRequest，触发第三轮 rebalance C1 发送的 JoinGroupRequest（assigned: P0、P3） C2 发送的 JoinGroupRequest（assigned: ） C3 发送的 JoinGroupRequest（assigned: P2 在此次 rebalance 中，C1 依旧被选为 Group Leader，它会发现 delay 已经到期了，开始解决不平衡的问题，对 partition 进行重新分配。最新的分配结果最终通过 SyncGroupResponse 返回到各个 consumer： 到此为止，第三轮 rebalance 结束。整个 rebalance 过程中，C1 和 C3 的消费都不会停止 C1 收到的 SyncGroup Response（assigned：P0、P3，revoked：） C2 收到的 SyncGroup Response（assigned：P1，revoked：） C3 收到的 SyncGroup Response（assigned：P2，revoked：） 下面这张图展示了上述的 Rebalance 过程 通过上述我们应该对 Kafka 的 Rebalance 有了比较完整的认识。我们现在来回答文章开始提出的消费卡主问题：消费端拿到了异常的消息，这样的消息业务上处理时间过超过了 max.poll.interval.ms, 从而触发了 rebalance, 在 rebalance 过程中所有消费者都暂停了消费。 为了解决这个问题我们首先优化业务逻辑尽可能提高处理消息速度，对异常消息做特殊处理；然后合理的设置 max.poll.interval.ms cover 住业务的处理时间。 为了尽可能减少 Rebalance 次数我们也要注意设置 session.timeout.ms 和 heartbeat.interval.ms 的值。一种推荐的方案 session.timeout.ms \u003e= 3 * heartbeat.interval.ms, 比如 session.timeout.ms = 6s; heartbeat.interval.ms = 2s。这样 consumer 如果宕机且 6s 之内未恢复， Coordinator 能够较快地定位已经挂掉的 consumer，把它踢出 Gr","date":"2021-11-20","objectID":"/kafka-rebalance/:0:6","tags":["kafka","rebalance","消息队列"],"title":"从一个消费慢的例子深入理解 kafka rebalance","uri":"/kafka-rebalance/"},{"categories":["golang","实现"],"content":"最近在做一个功能的时候需要用到分布式锁，发现我们的公共库里好像没有可以直接拿来用的分布式锁，那就自己实现一个。首先想到方案就是 Redis 分布式锁。一个相对完备的分布式锁应该满足以下几点： 互斥性。互斥是锁的基本特征，同一时刻锁只能被一个协程持有，执行操作。 超时释放。通过超时释放，可以避免死锁，防止不必要的协程等待和资源浪费。 可重入性。一个协程在持有锁的情况可以对其再次请求加锁，防止锁在协程执行完操作之前释放。 高性能和高可用。加锁和释放锁的过程性能开销要尽可能的低，同时也要保证高可用，防止分布式锁意外失效。 由于我们有现成的 redis 集群，所以选择 Redis 分布式锁实现成本是最低的，并且基本能满足以上特点。 这里我们就总结一下 redis 实现分布式锁的几种方式和分别存在的问题 ","date":"2021-10-23","objectID":"/redis-lock/:0:0","tags":["golang","redis","分布式锁"],"title":"Golang 实现的 Redis 分布式锁","uri":"/redis-lock/"},{"categories":["golang","实现"],"content":"实现方式 SETNX + EXPIRE 最基本的做法就是利用 Redis 的 SETNX 指令，该指令只在 key 不存在的情况下，将 key 的值设置为 value，若 key 已经存在，则 SETNX 命令不做任何动作。key 是锁的唯一标识，可以按照业务需要锁定的资源来命名。 在使用 SETNX 拿到锁以后，必须给 key 设置一个过期时间，以保证即使没有被显式释放，在获取锁达到一定时间后也要自动释放，防止资源被长时间独占。由于 SETNX 不支持设置过期时间，所以需要额外的 EXPIRE 指令。 func (d *dao) RedisLock(ctx context.Context, lockKey, lockValue string, ttl int64) (err error) { result, err := redis.String(d.redis.Do(ctx, \"SET\", lockKey, lockValue)) if err != nil || result != \"OK\" { if err == redis.ErrNil { err = nil return } log.Error(\"Error on acquiring lock for %s, %v\", lockKey, err) } // 设置过期时间 if ok, err = redis.Bool(d.redis.Do(ctx, \"EXPIRE\", lockKey, ttl)); err != nil { log.Error(\"Erron on expiring lock for %s, %v\", key, err) return } return } 这样实现的分布式锁仍然存在一个严重的问题，由于 SETNX 和 EXPIRE 这两个操作是非原子性的，如果程序在执行 SETNX 和 EXPIRE 之间发生异常，SETNX 执行成功，但 EXPIRE 没有执行，导致锁不会过期，其他协程无法正常获取锁。 SET 扩展命令 为了解决 SETNX 和 EXPIRE 两个操作非原子性的问题，可以使用 Redis 的 SET 指令的扩展参数，使得 SETNX 和 EXPIRE 这两个操作可以原子执行，如下面代码所示。 func (d *dao) RedisLock(ctx context.Context, lockKey, lockValue string, ttl int64) (err error) { result, err := redis.String(d.redis.Do(ctx, \"SET\", lockKey, lockValue, \"EX\", ttl, \"NX\")) if err != nil || result != \"OK\" { if err == redis.ErrNil { err = nil return } log.Error(\"Error on acquiring lock for %s, %v\", lockKey, err) } 在这个 SET 指令中： NX 表示只有当 lockKey 对应的 key 值不存在的时候才能 SET 成功。保证了只有第一个请求的协程才能获得锁，而其它协程在锁被释放之前都无法获得锁。 EX ttl 表示这个锁 ttl 秒钟后会自动过期，业务可以根据实际情况设置这个时间的大小。 其中 EX 也可以替换成 PX 可以使锁过期时间精确到毫秒。 但是这种方式仍然不能彻底解决分布式锁超时问题： 锁被提前释放。假如协程程 A 在加锁和释放锁之间的逻辑执行的时间过长（或者协程 A 执行过程中被堵塞），以至于超出了锁的过期时间后进行了释放，但协程 A 在想加锁部分的逻辑还没有执行完，那么这时候协程 B 就可以提前重新获取这把锁，导致代码不能严格的串行执行，而产生意外情况。 锁被误删。假如以上情形中的协程 A 执行完后，它并不知道此时的锁持有者是协程 B，协程 A 会继续执行 DEL 指令来释放锁，如果协程 B 在临界区的逻辑还没有执行完，协程 A 实际上释放了协程 B 的锁。 为了避免以上情况，建议不要在执行时间过长的场景中使用 Redis 分布式锁，同时一个比较安全的做法是在执行 DEL 释放锁之前对锁进行判断，验证当前锁的持有者是否是自己。 具体实现就是在加锁时将 value 设置为一个唯一的随机数(可以使用 UUID 或者生成随机数)，释放锁时先判断随机数是否一致，然后再执行释放操作，确保不会错误地释放其它协程持有的锁，除非是锁过期了被服务器自动释放 Lua脚本实现 Lua 脚本保证原子性，把多个操作在 Redis 中实现成一个操作，也就是单命令操作 使用了 set key value px milliseconds nx value 具有唯一性 加锁时首先判断 key 的 value 是否和之前设置的一致，一致则修改过期时间 代码如下 package redislock import ( \"context\" \"crypto/sha1\" \"encoding/hex\" \"io\" \"math/rand\" \"strconv\" \"sync/atomic\" \"time\" ) const ( letters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\" lockCommand = `if redis.call(\"GET\", KEYS[1]) == ARGV[1] then redis.call(\"SET\", KEYS[1], ARGV[1], \"PX\", ARGV[2]) return \"OK\" else return redis.call(\"SET\", KEYS[1], ARGV[1], \"NX\", \"PX\", ARGV[2]) end` delCommand = `if redis.call(\"GET\", KEYS[1]) == ARGV[1] then return redis.call(\"DEL\", KEYS[1]) else return 0 end` randomLen = 16 tolerance = 500 // milliseconds millisPerSecond = 1000 ) // A RedisLock is a redis lock. // 这里的 redis 是对 redigo 的一层内部封装 type RedisLock struct { store *redis.Redis seconds uint32 key string id string } type Script struct { keyCount int src string hash string } func init() { rand.Seed(time.Now().UnixNano()) } // NewRedisLock returns a RedisLock. func NewRedisLock(store *redis.Redis, key string) *RedisLock { return \u0026RedisLock{ store: store, key: key, id: randomStr(randomLen), } } func NewScript(keyCount int, src string) *Script { h := sha1.New() io.WriteString(h, src) return \u0026Script{keyCount, src, hex.EncodeToString(h.Sum(nil))} } // Acquire acquires the lock. func (rl *RedisLock) Acquire() (bool, error) { var ( seconds = atomic.LoadUint32(\u0026rl.seconds) keyCount = 1 s = NewScript(keyCount, lockCommand) keysAndArgs = []interface{}{rl.key, rl.id, strconv.Itoa(int(seconds)*millisPerSecond + tolerance)} ) conn := rl.store.Conn(context.TODO()) defer conn.Close() resp, err := conn.Do(\"EVAL\", s.args(s.src, keysAndArgs)...) if err == redis.ErrNil { return false, nil } else if err != nil { log.Error(\"Error on acquiring lock for %s, %v\", rl.key, err) return false, err } else if resp == nil { return false, nil } reply, ok := resp.(string) if ok \u0026\u0026 reply == \"OK\" { return true, nil } log.Error(\"Unknown reply when acquiring lock for %s: %v\", rl.key, resp) return false, nil } // Release releases the lock. func (","date":"2021-10-23","objectID":"/redis-lock/:0:1","tags":["golang","redis","分布式锁"],"title":"Golang 实现的 Redis 分布式锁","uri":"/redis-lock/"},{"categories":["实践","monitor"],"content":"在这篇文章中我们介绍了如何利用 Prometheus 监控应用。在后续的工作中随着监控的深入，我们结合自己的经验和官方文档总结了一些 Metrics 的实践。希望这些实践能给大家提供参考。 ","date":"2021-09-25","objectID":"/metrics-best-practice/:0:0","tags":["promethues","grafana","监控"],"title":"使用 Promethues 实现应用监控的一些实践","uri":"/metrics-best-practice/"},{"categories":["实践","monitor"],"content":"确定监控对象 在具体设计 Metrics 之前，首先需要明确需要测量的对象。需要测量的对象应该依据具体的问题背景、需求和需监控的系统本身来确定。 从需求出发 Google 针对大量分布式监控的经验总结出四个监控的黄金指标，这四个指标对于一般性的监控测量对象都具有较好的参考意义。这四个指标分别为： 延迟：服务请求的时间。 通讯量：监控当前系统的流量，用于衡量服务的容量需求。 错误：监控当前系统所有发生的错误请求，衡量当前系统错误发生的速率。 饱和度：衡量当前服务的饱和度。主要强调最能影响服务状态的受限制的资源。例如，如果系统主要受内存影响，那就主要关注系统的内存状态。 以上四种指标，其实是为了满足四个监控需求： 反映用户体验，衡量系统核心å性能。如：在线系统的时延，作业计算系统的作业完成时间等。 反映系统的吞吐量。如：请求数，发出和接收的网络包大小等。 帮助发现和定位故障和问题。如：错误计数、调用失败率等。 反映系统的饱和度和负载。如：系统占用的内存、作业队列的长度等。 除了以上常规需求，还可根据具体的问题场景，为了排除和发现以前出现过或可能出现的问题，确定相应的测量对象。比如，系统需要经常调用的一个库的接口可能耗时较长，或偶有失败，可制定 Metrics 以测量这个接口的时延和失败数。 从需要监控的系统出发 为了满足相应的需求，不同系统需要观测的测量对象也是不同的。在 官方文档 的最佳实践中，将需要监控的应用分为了三类： 线上服务系统（Online-serving systems）：需对请求做即时的响应，请求发起者会等待响应。如 web 服务器。 线下计算系统（Offline processing）：请求发起者不会等待响应，请求的作业通常会耗时较长。如批处理计算框架 Spark 等。 批处理作业（Batch jobs）：这类应用通常为一次性的，不会一直运行，运行完成后便会结束运行。如数据分析的 MapReduce 作业。 对于每一类应用其通常情况下测量的对象是不太一样的。其总结如下： 线上服务系统：主要有请求、出错的数量，请求的时延等。 线下计算系统：最后开始处理作业的时间，目前正在处理作业的数量，发出了多少 items， 作业队列的长度等。 批处理作业：最后成功执行的时刻，每个主要 stage 的执行时间，总的耗时，处理的记录数量等。 除了系统本身，有时还需监控子系统： 使用的库（Libraries）: 调用次数，成功数，出错数，调用的时延。 日志（Logging）：计数每一条写入的日志，从而可找到每条日志发生的频率和时间。 Failures: 错误计数。 线程池：排队的请求数，正在使用的线程数，总线程数，耗时，正在处理的任务数等。 缓存：请求数，命中数，总时延等。 ","date":"2021-09-25","objectID":"/metrics-best-practice/:0:1","tags":["promethues","grafana","监控"],"title":"使用 Promethues 实现应用监控的一些实践","uri":"/metrics-best-practice/"},{"categories":["实践","monitor"],"content":"选择 Vector 选用 Vec 的原则： 数据类型类似但资源类型、收集地点等不同 Vec 内数据单位统一 例子： 不同资源对象的请求延迟 不同地域服务器的请求延迟 不同 http 请求错误的计数 … 此外，官方文档 中建议，对于一个资源对象的不同操作，如 Read/Write、Send/Receive， 应采用不同的 Metric 去记录，而不要放在一个 Metric 里。原因是监控时一般不会对这两者做聚合，而是分别去观测。 不过对于 request 的测量，通常是以 Label 做区分不同的 action。 ","date":"2021-09-25","objectID":"/metrics-best-practice/:0:2","tags":["promethues","grafana","监控"],"title":"使用 Promethues 实现应用监控的一些实践","uri":"/metrics-best-practice/"},{"categories":["实践","monitor"],"content":"确定 Label 常见 Label 的选择有： resource region type … 确定 Label 的一个重要原则是：同一维度 Label 的数据是可平均和可加和的，也即单位要统一。如风扇的风速和电压就不能放在一个 Label 里。 此外，不建议下列做法： my_metric{label=a} 1 my_metric{label=b} 6 my_metric{label=total} 7 即在 Label 中同时统计了分和总的数据，建议采用 PromQL 在服务器端聚合得到总和的结果。或者用另外的 Metric 去测量总的数据。 ","date":"2021-09-25","objectID":"/metrics-best-practice/:0:3","tags":["promethues","grafana","监控"],"title":"使用 Promethues 实现应用监控的一些实践","uri":"/metrics-best-practice/"},{"categories":["实践","monitor"],"content":"命名 Metrics 和 Label 好的命名能够见名知义，因此命名也是良好设计的一环。 Metric 的命名： 需要符合 pattern: a-zA-Z*:* 应该包含一个单词作为前缀，表明这个 Metric 所属的域。如： prometheus_notifications_total process_cpu_seconds_total ipamd_request_latency 应该包含一个单位的单位作为后缀，表明这个 Metric 的单位。如： http_request_duration_seconds node_memory_usage_bytes http_requests_total (for a unit-less accumulating count) 逻辑上与被测量的变量含义相同。 尽量使用基本单位，如 seconds，bytes。而不是 Milliseconds, megabytes。 Label 的命名： 依据选择的维度命名，如： region: shenzhen/guangzhou/beijing owner: user1/user2/user3 stage: extract/transform/load ","date":"2021-09-25","objectID":"/metrics-best-practice/:0:4","tags":["promethues","grafana","监控"],"title":"使用 Promethues 实现应用监控的一些实践","uri":"/metrics-best-practice/"},{"categories":["实践","monitor"],"content":"Buckets 选择 适宜的 buckets 能使 histogram 的百分位数计算更加准确。 理想情况下，桶会使得数据分布呈阶梯状，即各桶区间内数据个数大致相同。 buckets 的设计可遵从如下经验： 需要知道数据的大致分布，若事先不知道可先用默认桶 （{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10}）或 2 倍数桶（{1,2,4,8…}）观察数据分布再调整 buckets。 数据分布较密处桶间隔制定的较窄一些，分布稀疏处可制定的较宽一些。 对于多数时延数据，一般具有长尾的特性，较适宜用指数形式的桶（ExponentialBuckets）。 初始桶上界一般覆盖10%左右的数据，若不关注头部数据也可以让初始上界更大一些。 若为了更准确计算特定百分位数，如90%，可在90%的数据处加密分布桶，即减少桶的间隔。 比如我在监控我们某些任务耗时的时候，就是选根据实际情况估算出大致的 bucket 取值，上线后观察数据和监控再去调整 bucket， 这样经过几次调整应该就能调整到比较合适的 bucket。 ","date":"2021-09-25","objectID":"/metrics-best-practice/:0:5","tags":["promethues","grafana","监控"],"title":"使用 Promethues 实现应用监控的一些实践","uri":"/metrics-best-practice/"},{"categories":["实践","monitor"],"content":"Grafana 使用技巧 查看所有维度 如果你想知道是否还能按其它维度分组，并快速查看还有哪些维度，可采用以下技巧：在 query 的表达式上只保留指标名称，不做任何计算，Legend format 也留空。这样就能显示出原始的 metric 数据。如下图所示 标尺联动 在 Settings 面板中，有一个 Graph Tooltip 设置项，默认使用 Default。 下面将图形展示工具分别调整为 Shared crosshair 和 Shared Tooltip 看看效果。可以看到标尺能联动展示了，方便排查问题时确认 2 个指标的关联性。 将图形展示工具调整为 Shared Tooltip： ","date":"2021-09-25","objectID":"/metrics-best-practice/:0:6","tags":["promethues","grafana","监控"],"title":"使用 Promethues 实现应用监控的一些实践","uri":"/metrics-best-practice/"},{"categories":["实现","原理"],"content":"在利用深度学习给音乐打标签的实践过程中，我们不会把音频的原始信号直接作为模型的直接输入。常见的做法是我们会对音频信号做一系列的处理生成频谱图，而这里的频谱图常常是梅尔频谱或者是 MFCC, 这样处理得到的频谱图就是音频的“快照”。这样的快照才适合作为深度学习模型的输入。在这篇文章里我们就介绍一下这些音频处理提取特征的基本原理。 ","date":"2021-08-21","objectID":"/audio-process/:0:0","tags":["audio","deep learning","librosa"],"title":"深度学习中的常用音频处理方法","uri":"/audio-process/"},{"categories":["实现","原理"],"content":"声音信号 声音是空气压力的变化产生的，声音信号则代表着空气压力随时间的变化。 声音信号通常以规律性的间隔重复，以使每个波具有相同的形状。 高度表示声音的强度，我们称其为振幅。 信号完成一个完整波所花费的时间叫周期，信号在一秒钟内产生的波数称为频率。 频率是周期的倒数，频率单位是赫兹。 我们遇到的大多数声音都可能不遵循这种简单而有规律的周期性模式。 但是可以将不同频率的信号加在一起，以创建具有更复杂重复模式的复合信号。 我们听到的所有声音（包括我们自己的声音）都包含此类波形 上面的波形图展示的是声音信号在时域的表示。与之相对的就是信号在频域的表示，即自变量是频率，即横轴是频率，纵轴是该频率信号的幅度（振幅）。 时域图：表现的是一段音频在一段时间内音量的变化，波形实质上是将各个频率的波形叠加在了一起（波形是由各频率不同幅值和相位的简单正弦波复合叠加得到的。 频谱图：表现的是一段音频在某一时刻各个频率的音量的高低，表示的是一个静态的时间点上各频率正弦波的幅值大小的分布状况 （各个时刻是一样的，即与时间无关） 下图展示了正弦波声音信号的时域到频域的转换 频谱生成 通信相关专业同学都知道信号的频谱就是用傅里叶变换算出来的。而在数字世界中我们必须把连续信号经过采样变成离散信号，所以对离散信号的频谱计算就是我们常说的离散傅里叶变换(DFT)。那为什么后来又有个快速傅里叶变换(FFT) 呢？从名字我们就知道就是 FFT 就是 DFT 的时机复杂度优化版本。 但在实际的音频信号处理中用的都是短时傅里叶变换(STFT, Short Time Fourier Transform)。为什么要用 STFT 呢？ 自然界中几乎所有信号都是非平稳信号(信号的频率特性在任何时间会发生改变)，比如语音信号就是典型的非平稳信号。 通常傅里叶变换只适合处理平稳信号，对于非平稳信号，由于频率特性会随时间变化，为了捕获这一时变特性，我们需要对信号进行时频分析，就包括 STFT。例如在一段时间内，有很多信号先后出现后消失，直接做 DFT 无法判断出不同信号出现的先后顺序，而 STFT 通过每次取出信号中的一小段加窗后做 DFT 来反映信号随时间的变化。 做STFT时每次取出的一段信号，叫做一帧(frame)。每两帧之间的间隔为Hop Size，一般为frame长度的25%-75%。将对每一帧做DFT得到的结果拼接到一起，可以得到spectrogram。增加frame的长度，频域的信息会越准确。 下图非常生动的展示了信号的 STFT 过程。 梅尔频谱(mel spectrum) 我们听到声音频率的方式称为“音调”，这是频率的主观印象。因此，高音调的声音比低音调声音的频率更大。人类不会线性感知频率，我们对低频之间的差异比高频更为敏感。我们可以轻松分辨出 500Hz和 1000Hz 之间的差异，但是即使两对之间的距离相同，我们也很难分辨出 10000Hz 和 10500Hz 之间的差异。 这时，梅尔标度(Mel Scale)被提出，它是Hz的非线性变换，表示人耳对等距音高(pitch)变化的感官，基于频率定义。 梅尔刻度与线性的频率刻度赫兹(Hz)之间可以进行近似的数学换算。一个常用的将 f 赫兹转换为m 梅尔的公式是: m = 2595*log(1 + (f/700)) 其参考点定义是将 1000Hz，且高于人耳听阈值 40 分贝以上的声音信号，定为 1000mel。在频率 500Hz 以上时，人耳每感觉到等量的音高变化，所需要的频率变化随频率增加而愈来愈大。这样的结果是，在赫兹刻度 500Hz 往上的四个八度(一个八度即为两倍的频率)，只对应梅尔刻度上的两个八度。Mel 的名字来源于单词 melody，表示这个刻度是基于音高比较而创造的。 人类对声音振幅的感知就是声音的响度。与频率相似，我们听到的音量增大，一般都是非线性放大，而不是线性的，并且使用分贝表对此进行说明。 在此等级上，0dB 是完全静音。从此处开始，测量单位呈指数增长。10dB 是 0dB 的10倍，20dB是 100倍，30dB是 1000倍。在此规模上，高于 100dB 的声音开始变得让人难以忍受。 我们可以看到，为了以真实的方式处理声音，我们在处理数据的频率和幅度时，必须通过梅尔标度和分贝标度使用对数标度来转换频谱，这样转换后的频谱就称之为梅尔频谱。 梅尔倒谱系数（Mel-Frequency Cepstral Coefficients) 梅尔频谱图对于大多数音视频深度学习应用程序来说效果都是不错的。但是，对于人类语音的问题（如自动语音识别 ASR), MFCC（梅尔频率倒谱系数）的效果有时会更好。我们这里简要介绍一下 MFCC。首先了解一下倒频谱。 即倒频谱（信号）是信号频谱取对数的傅里叶变换后的新频谱（信号），倒频谱方便提取、分析原频谱图上肉眼难以识别的周期性信号，能将原来频谱图上成族的边频带谱线简化为单根谱线，受传感器的测点位置及传输途径的影响小。 MFCC 即就是组成梅尔频率倒谱的系数，广泛被应用于语音识别中。 下图描述了 MFCC 的处理流程 关于傅里叶变换，倒频谱，MFCC 等我们没有深入介绍背后的数学原理和计算。感兴趣的可以搜索相关主题进一步学习。 ","date":"2021-08-21","objectID":"/audio-process/:0:1","tags":["audio","deep learning","librosa"],"title":"深度学习中的常用音频处理方法","uri":"/audio-process/"},{"categories":["实现","原理"],"content":"信号处理 文章的前半部分我们介绍了语音信号的特征提取的一般原理和过程。现在我们看看在工程上我们如何去实现以上步骤，这里我们拿非常受欢迎的语音处理库 Python 中的 librosa 来作为示例。 波形图 import librosa.display import matplotlib.pyplot as plt # 读取音频文件 AUDIO_FILE = '/Users/lxkaka/Desktop/audio/test/周深-小幸运.mp3' samples, sample_rate = librosa.load(AUDIO_FILE, sr=None) librosa.display.waveplot(samples, sr=sample_rate) plt.show() 频谱图 # 短时傅里叶变换，返回一个复数矩阵D(F，T) sgram = librosa.stft(samples) plt.plot(sgram) plt.show() 梅尔频谱图 # 将复数矩阵D(F, T)分离为幅值𝑆和相位𝑃的函数，返回幅值S，相位P sgram_mag, _ = librosa.magphase(sgram) # 计算梅尔频谱 mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sample_rate) # 幅值转dB，将幅度频谱转换为dB标度频谱。也就是对S取对数 s_db = librosa.amplitude_to_db(mel_scale_sgram, ref=np.max) fig, ax = plt.subplots() img = librosa.display.specshow(s_db, sr=sample_rate, x_axis='time', y_axis='mel') plt.colorbar(img, ax=ax, format=\"%+2.f dB\") plt.show() MFCC # 提取MFCC特征 mfcc = librosa.feature.mfcc(samples, sr=sample_rate) # 执行特征缩放，使得每个系数维度具有零均值和单位方差 mfcc = sklearn.preprocessing.scale(mfcc, axis=1) img = librosa.display.specshow(mfcc, sr=sample_rate, x_axis='time') plt.colorbar(img) plt.show() 至此，在深度学习中对音频做预处理的基本原理就介绍完了，其中音频处理可能还包括的频谱图增强没有提及，感兴趣的同学可以继续研究。后续随着我们应用的深入可能会有文章继续介绍我们的一些实践。 ","date":"2021-08-21","objectID":"/audio-process/:0:2","tags":["audio","deep learning","librosa"],"title":"深度学习中的常用音频处理方法","uri":"/audio-process/"},{"categories":["基础","原理"],"content":"接触过深度学习的同学都知道在训练和推理中使用 GPU 能加速，但是相对于 CPU 来说为什么 GPU 能在深度学习中提供更快的处理速度？我把自己学习和总结的成果分享在这篇文章中。 ","date":"2021-07-31","objectID":"/gpu-dl/:0:0","tags":["GPU","deep learning"],"title":"为什么 GPU 能加速深度学习","uri":"/gpu-dl/"},{"categories":["基础","原理"],"content":"CPU 和 GPU CPU 的结构主要包括运算器（ALU, Arithmetic and Logic Unit）、控制单元（CU, Control Unit）、寄存器（Register）、高速缓存器（Cache）和它们之间通讯的数据、控制及状态的总线。 CPU 是基于低延时的设计，简单来说包括：计算单元、控制单元和存储单元，架构可参考下图： CPU 的特点： CPU有强大的ALU（算术运算单元），它可以在很少的时钟周期内完成算术计算 当今的 CPU 可以达到 64bit 双精度。执行双精度浮点源算的加法和乘法只需要1～3个时钟周期（CPU的时钟周期的频率是非常高的，达到1.532～3gigahertz(千兆HZ, 10的9次方)） 大的缓存可以降低延时 保存很多的数据放在缓存里面，当需要访问的这些数据，只要在之前访问过的，如今直接在缓存里面取即可。 复杂的逻辑控制单元 当程序含有多个分支的时候，它通过提供分支预测的能力来降低延时。 数据转发。当一些指令依赖前面的指令结果时，数据转发的逻辑控制单元决定这些指令在 pipeline 中的位置并且尽可能快的转发一个指令的结果给后续的指令。这些动作需要很多的对比电路单元和转发电路单元。 GPU 全称为 Graphics Processing Unit，中文为图形处理器，就如它的名字一样，GPU 最初是用在个人电脑、工作站、游戏机和一些移动设备（如平板电脑、智能手机等）上运行绘图运算工作的微处理器。 GPU 是基于大的吞吐量设计，GPU 简单架构参考下图: GPU 的特点： 有很多的 ALU 和很少的 cache 缓存的目的不是保存后面需要访问的数据的，这点和 CPU 不同，而是为 thread 提高服务的。如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问 dram（因为需要访问的数据保存在 dram 中而不是 cache 里面），获取数据后 cache 会转发这个数据给对应的线程，这个时候是数据转发的角色。但是由于需要访问 dram，自然会带来延时的问题。 GPU 的控制单元（左边黄色区域块）可以把多个的访问合并成少的访问 GPU 的虽然有 dram 延时，却有非常多的 ALU 和非常多的 thread 为了平衡内存延时的问题，GPU 可以充分利用多的 ALU 的特性达到一个非常大的吞吐量的效果。尽可能多的分配 Threads。 CPU 和 GPU 的优势 当代 CPU 的微架构是按照兼顾“指令并行执行”和“数据并行运算”的思路而设计，就是要兼顾程序执行和数据运算的并行性、通用性以及它们的平衡性。 CPU 的微架构偏重于程序执行的效率，不会一味追求某种运算极致速度而牺牲程序执行的效率。 CPU 微架构的设计是面向指令执行高效率而设计的，因而 CP U是计算机中设计最复杂的芯片。和 GPU 相比，CPU 核心的重复设计部分不多，这种复杂性不能仅以晶体管的多寡来衡量，这种复杂性来自于实现：如程序分支预测，推测执行，多重嵌套分支执行，并行执行时候的指令相关性和数据相关性，多核协同处理时候的数据一致性等等复杂逻辑。 GPU 其实是由硬件实现的一组图形函数的集合，这些函数主要用于绘制各种图形所需要的运算。这些和像素，光影处理，3D 坐标变换等相关的运算由 GPU 硬件加速来实现。图形运算的特点是大量同类型数据的密集运算——如图形数据的矩阵运算，GPU 的微架构就是面向适合于矩阵类型的数值计算而设计的，大量重复设计的计算单元，这类计算可以分成众多独立的数值计算——大量数值运算的线程，而且数据之间没有像程序执行的那种逻辑关联性。 GPU 微架构复杂度不高，尽管晶体管的数量不少。从应用的角度看，如何运用好GPU的并行计算能力主要的工作是开发好它的驱动程序。 GPU 驱动程序的优劣很大程度左右了GPU实际性能的发挥。 因此从架构上看 CPU 擅长的是像操作系统、系统软件和通用应用程序这类拥有复杂指令调度、循环、分支、逻辑判断以及执行等的程序任务。它的并行优势是程序执行层面的，程序逻辑的复杂度也限定了程序执行的指令并行性，上百个并行程序执行的线程基本看不到。 GPU 擅长的是图形类的或者是非图形类的高度并行数值计算，GPU 可以容纳上千个没有逻辑关系的数值计算线程，它的优势是无逻辑关系数据的并行计算。 ","date":"2021-07-31","objectID":"/gpu-dl/:0:1","tags":["GPU","deep learning"],"title":"为什么 GPU 能加速深度学习","uri":"/gpu-dl/"},{"categories":["基础","原理"],"content":"深度学习的特征 深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。 深度学习采用的模型为深层神经网络（Deep Neural Networks，DNN）模型，即包含多个隐藏层（Hidden Layer，也称隐含层）的神经网络（Neural Networks，NN）。深度学习利用模型中的隐藏层，通过特征组合的方式，逐层将原始输入转化为浅层特征、中层特征、高层特征直至最终的任务目标。 而在上面的 GPU 的介绍中我们看到 GPU 非常擅长高度并行(embarrassingly parallel)数值计算。在并行计算中，高度并行任务是指将整个任务分割成一组较小的任务以并行计算的任务。 高度并行任务是那些很容易看到一组小任务彼此独立的任务。由于这个原因，神经网络高度并行。我们用神经网络做的许多计算都可以很容易地分解成更小的计算，这样小的计算集就不会相互依赖。这些小的计算我们拿卷积来举例。 卷积 下图的这个示例展示了卷积的计算过程 底部有一个蓝色的输入通道。在输入通道上滑动的底部有一个阴影的卷积滤波器，还有一个绿色的输出通道。 图上蓝色（底部）表示输入通道； 阴影（覆盖在蓝色上）表示3*3的卷积过滤器; 绿色（顶部）表示输出通道。 对于蓝色输入通道上的每个位置，3x3过滤器进行计算，将蓝色输入通道的阴影部分映射到绿色输出通道的相应阴影部分。在动画中，这些计算一个接一个地依次进行。但是，每个计算都是独立于其他计算的，这意味着任何计算都不依赖于任何其他计算的结果。因此，所有这些独立的计算都可以在GPU上并行进行，从而产生整个输出通道。这让我们看到，卷积运算可以通过使用并行编程方法和 GPU 来加速。 矩阵乘法 而在深度学习的卷积过程中少不了矩阵相乘。我们具体看一个矩阵相乘的例子如何在 GPU 中实现。 在C中，一般的实现如下： void matrixMult (int a[N][N], int b[N][N], int c[N][N], int width) { for (int i = 0; i \u003c width; i++) { for (int j = 0; j \u003c width; j++) { int sum = 0; for (int k = 0; k \u003c width; k++) { int m = a[i][k]; int n = b[k][j]; sum += m * n; } c[i][j] = sum; } } } 其中，矩阵width是矩阵A的列数，显然，上面算法的复杂度是O(N^3)。采用GPU编程只需将上面的方法写成kernel function的形式： __global__ void matrixMult (int *a, int *b, int *c, int width) { int k, sum = 0; int col = threadIdx.x + blockDim.x * blockIdx.x; int row = threadIdx.y + blockDim.y * blockIdx.y; if(col \u003c width \u0026\u0026 row \u003c width) { for (k = 0; k \u003c width; k++) ｛ sum += a[row * width + k] * b[k * width + col]; ｝ c[row * width + col] = sum; } } 对比一下C和GPU实现的线程数量和时间复杂度: 线程数量 时间复杂度 CPU 1 N^3 GPU N^2 N ","date":"2021-07-31","objectID":"/gpu-dl/:0:2","tags":["GPU","deep learning"],"title":"为什么 GPU 能加速深度学习","uri":"/gpu-dl/"},{"categories":["基础","原理"],"content":"Nvidia CUDA CUDA（Compute Unified Device Architecture）是NVIDIA公司基于其生产的图形处理器 GPU 开发的一个并行计算平台和编程模型。2007年 Nvidia 发布了CUDA编程模型，软件开发人员从此可以使用CUDA在英伟达的GPU上进行并行编程。 继 CUDA之后，Nvidia 不断丰富其软件技术栈，提供了科学计算所必须的 cuBLAS 线性代数库，cuFFT 快速傅里叶变换库等，当深度学习大潮到来时，英伟达提供了cuDNN深度神经网络加速库，目前常用的 TensorFlow、PyTorch 深度学习框架的底层大多基于 cuDNN 库。这些软件工具库使研发人员专注于自己的研发领域，不用再去花大量时间学习 GPU 底层知识。 GPU 编程可以直接使用 CUDA 的 C/C++ 版本进行编程，也可以使用其他语言包装好的库，比如 Python可使用 Numba 库调用 CUDA。 我们此前提到的模型训练和推理不是单靠 GPU 就能完成，而是需要 CPU + GPU 协同工作才能完成。我们在说 GPU 并行计算时，其实是指的基于 CPU+GPU 的异构计算架构。在异构计算架构中，GPU 与CPU 通过 PCIe 总线连接在一起来协同工作，CPU 所在位置称为为主机端（host），而GPU所在位置称为设备端（device），如下图所示 所以 CUDA 的编程模型就是基于这样的异构系统，工作原理如下: 分配host内存，并进行数据初始化 分配 device 内存，并从 host 将数据拷贝到 device 上, 调用 CUDA 的核函数在 device 上完成指定的运算； 将 device 上的运算结果拷贝到 host 上； 通过上述的介绍我们明白深度学习高度并行计算的特性与 GPU 大量的计算核心提供并行计算和更大的内存带宽完美契合，所以 GPU 成了加速深度学习的不二选择。 ","date":"2021-07-31","objectID":"/gpu-dl/:0:3","tags":["GPU","deep learning"],"title":"为什么 GPU 能加速深度学习","uri":"/gpu-dl/"},{"categories":["存储","原理"],"content":"对以 RocksDB 为代表的 LSM tree设计思想的探究，学习和总结","date":"2021-06-26","objectID":"/rocksdb-lsm/","tags":["RocksDB","LSM tree"],"title":"以 RocksDB 为代表的 LSM tree 的学习和总结","uri":"/rocksdb-lsm/"},{"categories":["存储","原理"],"content":"在公司最近组织的一次 OceanBase 的分享中我再次听到了 LSM-tree。为什么说是再呢？那不得不先介绍一下 RocksDB RocksDB 是由 Facebook 基于 LevelDB 开发的一款提供键值存储与读写功能的 LSM-tree 架构引擎。用户写入的键值对会先写入磁盘上的 WAL (Write Ahead Log)，然后再写入内存中的跳表（SkipList，这部分结构又被称作 MemTable）。LSM-tree 引擎由于将用户的随机修改（插入）转化为了对 WAL 文件的顺序写，因此具有比 B 树类存储引擎更高的写吞吐。 在这篇文章里我们提到过 Pulsar 使用了 bookkeeper 来作为存储组件，其中 RocksDB 是 bookkeeper 的依赖组件之一。国内比较有名的开源存储 TiDB 把 RocksDB 作为了存储引擎。可以看到 RocksDB 的在存储界的地位之高了吧。而 RocksDB 和 OceanBase 都用到了 LSM tree(Log-Structured Merge-tree):一种分层、有序、面向磁盘的数据结构或者说是存储结构。在这篇文章里我们就学习一下 LSM tree 的设计思想。 ","date":"2021-06-26","objectID":"/rocksdb-lsm/:0:0","tags":["RocksDB","LSM tree"],"title":"以 RocksDB 为代表的 LSM tree 的学习和总结","uri":"/rocksdb-lsm/"},{"categories":["存储","原理"],"content":"LSM tree 简介 相比于传统的 in-place updates(原地更新)索引结构比如 B+ tree，LSM tree 将第一次写入都缓存到内存中，并通过后台的 flush 来顺序写入到磁盘中，也就是out-of-palce updates。 LSM tree 这样的实现方式有非常多的优点，包括写性能的提升、较高的空间利用率、简单的并发控制和异常恢复等。所以我们可以看到在大量写的场景下很多著名软件 BigTable, Dynamo, HBase, Cassandra, LevelDB, RocksDB 等都用到了 LSM tree。 今天我们以 RocksDB 中的 LSM tree 实现为目标来学习 LSM tree 的设计思想。 ","date":"2021-06-26","objectID":"/rocksdb-lsm/:0:1","tags":["RocksDB","LSM tree"],"title":"以 RocksDB 为代表的 LSM tree 的学习和总结","uri":"/rocksdb-lsm/"},{"categories":["存储","原理"],"content":"核心组成 下图是 RocksDB 的核心组成, 了解了核心的组成是掌握 LSM tree 设计思想的基础。 MemTable MemTable 是一个内存数据结构，保存了落盘到 SST 文件前的数据。它同时服务于读和写——新的写入总是将数据插入到memtable，读取在查询 SST 文件前总是要查询memtable，因为 memtable 里面的数据总是更新的。一旦一个 memtable 被写满，它会变成不可修改的，并被一个新的 memtable 替换。一个后台线程会把这个memtable 的内容落盘到一个 SST 文件，然后这个 memtable 就可以被销毁了。并且在flush的过程中，会完成数据的压缩。RocksDB 默认实现方式是 SkipList，适用于范围查询和插入。 Immutable Memtable 所有的写操作都是在 memtable 进行，当 memtable 空间不足时，会创建一块新的 memtable 来继续接收写操作，原先的内存将被标识为只读模式，等待被刷入 sst。刷入时机有以下三个条件来确定： write_buffer_size 设置一块 memtable 的容量,一旦写满，就标记为只读，然后创建一块新的。 max_write_buffer_number 设置 memtable 的最大存在数(active 和 immutable 共享)，一旦active memtable 被写满了，并且 memtable 的数量大于max_write_buffer_number, 此时会阻塞写操作。当flush操作比写入慢的时候，会发生这种情况 min_write_buffer_number_to_merge 设置刷入sst之前，最小可合并的 memtable 数，例如，如果设置 2，只有当 immutable memtable 数量达到2的时候，会被刷入 sst，数量为1的时候，则永远不会被刷入。 WAL(write-ahead log) 每次数据被更新时，会同时写入内存表和 WAL，WAL 可用于发生故障后，恢复内存的数据。 在以下情况下会创建一个WAL： 新打开一个DB flush 了一个 column family。一个 WAL 文件只有当所有的列族数据都已经 flush 到SST file 之后才会被删除，或者说，所有的 WAL 中数据都持久化到SST file 之后，才会被删除。归档的WAL文件会 move 到一个单独的目录，后续从磁盘中删除。 SSTable(soretd string table) SSTable 全称是 Sorted String Table，是一个持久化的、有序的、不可更改的 Map 结构，Key 和 Value 都是任意的 Byte 串。 下图是 SSTable 的组成示意图，了解各个 block 能帮助我们快速理解 LSM tree 的读写过程。 DataBlock data block 顺序存储key/value，为了节省存储空间，并不会为每一对key-value对都存储完整的key值，而是存储与上一个key非共享的部分，避免了key重复内容的存储 MetaBlock 为了方便我们理解，我们只说明其中的 filter block 为了加快 sstable 中数据查询的效率，在直接查询 datablock 中的内容之前，首先根据 filter block 中的过滤数据判断指定的 datablock 中是否有需要查询的数据，若判断不存在，则无需对这个 datablock 进行数据查找。(使用的是 Bloom Filter) MetaIndexBlock 对于只有 FilterBlock 的情况下，meta index block 用来存储 filter block 在整个 sstable 中的索引信息。 IndexBlock 与 meta index block 类似，index block 用来存储所有 data block 的相关索引信息。indexblock 包含若干条记录，每一条记录代表一个 data block 的索引信息。 Footer Footer 是定长的，读取 SST文件的时候，就是从文件末尾，固定读取字节数，进而得到了 Footer 信息。 Footer 中的信息，指明了 MetaIndexBlock 和 IndexBlock的位置，进而找到 MetaBlock 和 DataBlock。 ","date":"2021-06-26","objectID":"/rocksdb-lsm/:0:2","tags":["RocksDB","LSM tree"],"title":"以 RocksDB 为代表的 LSM tree 的学习和总结","uri":"/rocksdb-lsm/"},{"categories":["存储","原理"],"content":"读写过程 读 在 MemTable 中查找，无法命中转到下一步； 在 immutable_memtable 中查找，查找不中转到下一步； 在第0层 SSTable 中查找，无法命中转到下一流程； 对于L0 的文件，RocksDB 采用遍历的方法查找，所以为了查找效率 RocksDB 会控制 L0 的文件个数。每个 Memtable 跟 SST 都会有相应的 Bloom Filter 来加快判断 Key 是否可能在其中，当判断 Key 可能在其中时，就会在 Memtable 或者 SST 中进行查找。 在剩余SSTable中查找。对于 L1 层以及 L1 层以上层级的文件，每个 SSTable 没有交叠，可以使用二分查找快速找到 key 所在的 Level 以及 SSTfile。 写 循环检查 DB 状态; 如果当前 memtable 的 size 未达到阈值 write_buffer_size(默认4MB)，则允许写入; 如果 memtable 的 size 已经达到阈值，但 immutable memtable 仍然存在，则等待 compaction 将其 dump 完成； memtable 已经写满，并且 immutable memtable 不存在，则将当前 memetable 置成 immutable memtable，产生新的 memtable 和 log file，主动触发 compaction，允许该次写。 数据先写入 WAL，成功后转到下一步； 数据写入 memetable ","date":"2021-06-26","objectID":"/rocksdb-lsm/:0:3","tags":["RocksDB","LSM tree"],"title":"以 RocksDB 为代表的 LSM tree 的学习和总结","uri":"/rocksdb-lsm/"},{"categories":["存储","原理"],"content":"Compaction 策略 在上面的介绍中我们看到 RocksDB 有非常重要的操作就是 compaction。随着 sstable 的不断写入，系统打开的文件就会越来越多，并且对于同一个 key 积累的数据改变（更新、删除）操作也就越多。由于 sstable 是不可变的，为了减少文件数并及时清理无效数据，就要进行 compaction 操作，将多个 key 区间有重合的sstable 进行合并。 Leveled Compaction 磁盘的文件组合成多个 Level。Level 0 保存了最新的数据，越高 level 的 block 保存了越旧的数据。在 L0,不同的文件可能会出现相同的 key（因此，每次Get()需要遍历L0的每个文件）,但是 L1 和更高 level 的文件中，不同的文件不会出现相同的 key。除 L0 外，每个 Level 的数据按 key 的顺序分为一个个SST file。每个 SST file 内部的 key 都是有序的。 流程总结： 找到 score 最高的 level；(compact 流程的 Compaction Score，不同 level 的计算方法不一样) 根据一定策略从 level 中选择一个 sst 文件进行 compact，L0 的各个 sst 文件之间 key range 【minkey， maxkey】 有重叠，所以可能一次选取多个； 获取 sst 文件的 minkey 和 maxkey; 从 level + 1 中选取出于 (minkey, maxkey) 用重叠的 sst 文件，有重叠的文件则把文件与 level 中的文件进行合并（merge - sort）作为目标文件，没有重叠文件则把原始文件作为目标文件； 对目标文件进行压缩后放入 level + 1 中。 示意图如下 Universal Compaction Univesal Compaction 主要针对 L0。当 L0 中的文件个数多于 level0_file_num_compaction_trigger，则启动 compact。 L0 中所有的 sst 文件都可能存在重叠的 key range，假设所有的 sst 文件组成了文件队列 R1,R2,R3,…,Rn，R1 文件的数据是最新的，R2 其次，Rn 则包含了最老的数据，其 compact 流程如下： 如果空间放大超过 max_size_amplification_percent，则对所有的 sst 进行 compaction（full compaction）； 如果前 size(R1)小于size(R2)在一定比例，默认 1%，则与 R1 与 R2 一起进行 compaction，如果（R1+R2)*(100+ratio)%100\u003cR3，则将 R3 也加入到 compaction 任务中，依次顺序加入 sst 文件； 如果第1和第2种情况都没有 compaction，则强制选择前 N 个文件进行合并。 FIFO Compaction FIFO 顾名思义就是先进先出，这种模式周期性地删除旧数据。在 FIFO 模式下，所有文件都在 L0，当 sst 文件总大小超过阀值 maxtablefiles_size，则删除最老的 sst 文件。对于 FIFO 来说，它的策略非常的简单，所有的 SST 都在 Level 0，如果超过了阈值，就从最老的 SST 开始删除，其实可以看到，这套机制非常适合于存储时序数据。 ","date":"2021-06-26","objectID":"/rocksdb-lsm/:0:4","tags":["RocksDB","LSM tree"],"title":"以 RocksDB 为代表的 LSM tree 的学习和总结","uri":"/rocksdb-lsm/"},{"categories":["存储","原理"],"content":"总结 在这篇文章里主要是对以 RocksDB 为代表的 LSM tree 设计思想进行学习和总结，希望能对大家认识和学习 LSM Tree 及 RocksDB 带来帮助。文章里很多细节比如核心组成部分数据机构的细节；读写流程的具体实现；compaction 的细节都没有展开，有兴趣的同学可以针对具体的点进一步探究。 关于 RocksDB 的问题 读写放大严重 应对突发流量的时候削峰能力不足 压缩率有限 索引效率较低 业界也有对应的改进措施，我们可以去选取对应的 topic 去看看别人的改进方案来拓宽我们的视野。 ","date":"2021-06-26","objectID":"/rocksdb-lsm/:0:5","tags":["RocksDB","LSM tree"],"title":"以 RocksDB 为代表的 LSM tree 的学习和总结","uri":"/rocksdb-lsm/"},{"categories":["k8s","原理"],"content":"Kubelet 如何重启容器的原理分析","date":"2021-05-29","objectID":"/kubelet-hash/","tags":["container","kubelet"],"title":"容器重启原理-Kubelet hash计算","uri":"/kubelet-hash/"},{"categories":["k8s","原理"],"content":"在日常的开发工作中相信使用 Kubernetes 的同学们一定会偶尔收到容器重启的事件告警。由于应用层面的问题导致的容器重启相对容易排查，比如看容器的内存监控我们能确定是不是内存超过配置的 limit; 又或者看是不是应用有 panic 没有 recovery。 一个正常的工作日我们突然连续收到多条容器重启告警，查看报警还是来自不同的应用。按照一般的排查思路先去查看监控，内存没有异常，使用值一直在 limit 之下；然后去看日志也没有找到任何 panic 或者其他错误。仔细一看这几个告警的应用都是来自同一个集群，这个时候猜测大概率和集群有关系，但是这个集群我们还有其他很多应用并没有发生容器重启，所以猜测应该不是集群本身的问题，那是不是和机器有关系呢？然后我把重启过的实例所在的 node ip 都筛选出来发现重启的应用都是集中在某几台机器。在这些节点上我去查看了一下 kubelet进程，发现 kubelet 在容器告警的时间段都重启了进程。在这种情况下基本就找到了容器重启的直接原因–kubelet 重启了。但是我们并没有更新实例，kubelet 重启怎么会把我们的容器重启呢？下面我们就介绍一下根本原因–kubelet计算容器的 hash 值。 我们知道在 Kubernetes 中的节点上运行着 kubelet 进程，这个进程负责当前节点上所有 Pod 的生命周期。在这里我们从源码层面看看 kubelet 怎么实现容器的重启。 ","date":"2021-05-29","objectID":"/kubelet-hash/:0:0","tags":["container","kubelet"],"title":"容器重启原理-Kubelet hash计算","uri":"/kubelet-hash/"},{"categories":["k8s","原理"],"content":"SyncPod 我们首先看 https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/kuberuntime_manager.go 中的 SyncPod 方法, 这个方法就是保证运行中的 Pod 与我们期望的配置时刻保持一致。 通过以下步骤完成 根据从 API Server 获得的 Pod Spec 以及当前 Pod 的 Status 计算所需要执行的 Actions 在需要情况下 Kill 掉当前 Pod 的 sandbox 根据需要（如重启）kill 掉 Pod 内的 containers 根据需要创建 Pod 的 sandbox 启动下一个 init container 启动 Pod 内的 containers func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) { // Step 1: Compute sandbox and container changes. // 计算 pod 的 podContainerChanges := m.computePodActions(pod, podStatus) glog.V(3).Infof(\"computePodActions got %+v for pod %q\", podContainerChanges, format.Pod(pod)) if podContainerChanges.CreateSandbox { ref, err := ref.GetReference(legacyscheme.Scheme, pod) if err != nil { glog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), err) } if podContainerChanges.SandboxID != \"\" { m.recorder.Eventf(ref, v1.EventTypeNormal, events.SandboxChanged, \"Pod sandbox changed, it will be killed and re-created.\") } else { glog.V(4).Infof(\"SyncPod received new pod %q, will create a sandbox for it\", format.Pod(pod)) } } // Step 2: Kill the pod if the sandbox has changed. // sandbox 有更新，需要 kill pod if podContainerChanges.KillPod { ... killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil) result.AddPodSyncResult(killResult) if killResult.Error() != nil { glog.Errorf(\"killPodWithSyncResult failed: %v\", killResult.Error()) return } if podContainerChanges.CreateSandbox { m.purgeInitContainers(pod, podStatus) } } else { // Step 3: kill any running containers in this pod which are not to keep. // kill 掉 pod 中不需要保留的容器 for containerID, containerInfo := range podContainerChanges.ContainersToKill { glog.V(3).Infof(\"Killing unwanted container %q(id=%q) for pod %q\", containerInfo.name, containerID, format.Pod(pod)) killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name) result.AddSyncResult(killContainerResult) if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil { killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error()) glog.Errorf(\"killContainer %q(id=%q) for pod %q failed: %v\", containerInfo.name, containerID, format.Pod(pod), err) return } } } ... // Step 4: Create a sandbox for the pod if necessary. // 按需创建 sandbox podSandboxID := podContainerChanges.SandboxID if podContainerChanges.CreateSandbox { var msg string var err error glog.V(4).Infof(\"Creating sandbox for pod %q\", format.Pod(pod)) createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod)) result.AddSyncResult(createSandboxResult) podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) ... } ... } ... // Step 5: start the init container. // 启动 init 容器 if container := podContainerChanges.NextInitContainerToStart; container != nil { // Start the next init container. startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name) result.AddSyncResult(startContainerResult) ... if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil { startContainerResult.Fail(err, msg) utilruntime.HandleError(fmt.Errorf(\"init container start failed: %v: %s\", err, msg)) return } // Successfully started the container; clear the entry in the failure glog.V(4).Infof(\"Completed init container %q for pod %q\", container.Name, format.Pod(pod)) } // Step 6: start containers in podContainerChanges.ContainersToStart. // 根据 step1 结果启动容器 for _, idx := range podContainerChanges.ContainersToStart { container := \u0026pod.Spec.Containers[idx] startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name) result.AddSyncResult(","date":"2021-05-29","objectID":"/kubelet-hash/:0:1","tags":["container","kubelet"],"title":"容器重启原理-Kubelet hash计算","uri":"/kubelet-hash/"},{"categories":["k8s","原理"],"content":"computePodActions 在上面 SyncPod 方法中我们可以看到 step 1 的 computePodActions 是决定容器是否需要重启的关键调用，我们看看这个方法具体的逻辑 // computePodActions checks whether the pod spec has changed and returns the changes if true. func (m *kubeGenericRuntimeManager) computePodActions(pod *v1.Pod, podStatus *kubecontainer.PodStatus) podActions { glog.V(5).Infof(\"Syncing Pod %q: %+v\", format.Pod(pod), pod) createPodSandbox, attempt, sandboxID := m.podSandboxChanged(pod, podStatus) changes := podActions{ KillPod: createPodSandbox, CreateSandbox: createPodSandbox, SandboxID: sandboxID, Attempt: attempt, ContainersToStart: []int{}, ContainersToKill: make(map[kubecontainer.ContainerID]containerToKillInfo), } // 这里我们省略其他内容，直接看判断容器是否需要重启的核心逻辑 // Number of running containers to keep. keepCount := 0 // check the status of containers. for idx, container := range pod.Spec.Containers { containerStatus := podStatus.FindContainerStatusByName(container.Name) // Call internal container post-stop lifecycle hook for any non-running container so that any // allocated cpus are released immediately. If the container is restarted, cpus will be re-allocated // to it. if containerStatus != nil \u0026\u0026 containerStatus.State != kubecontainer.ContainerStateRunning { if err := m.internalLifecycle.PostStopContainer(containerStatus.ID.ID); err != nil { glog.Errorf(\"internal container post-stop lifecycle hook failed for container %v in pod %v with error %v\", container.Name, pod.Name, err) } } // If container does not exist, or is not running, check whether we // need to restart it. if containerStatus == nil || containerStatus.State != kubecontainer.ContainerStateRunning { if kubecontainer.ShouldContainerBeRestarted(\u0026container, pod, podStatus) { message := fmt.Sprintf(\"Container %+v is dead, but RestartPolicy says that we should restart it.\", container) glog.V(3).Infof(message) changes.ContainersToStart = append(changes.ContainersToStart, idx) } continue } // The container is running, but kill the container if any of the following condition is met. reason := \"\" restart := shouldRestartOnFailure(pod) // 计算容器的期望的 hash 和 当前 hash, 来判断是否需要重启容器 if expectedHash, actualHash, changed := containerChanged(\u0026container, containerStatus); changed { reason = fmt.Sprintf(\"Container spec hash changed (%d vs %d).\", actualHash, expectedHash) // Restart regardless of the restart policy because the container // spec changed. restart = true } else if liveness, found := m.livenessManager.Get(containerStatus.ID); found \u0026\u0026 liveness == proberesults.Failure { // If the container failed the liveness probe, we should kill it. reason = \"Container failed liveness probe.\" } else { // Keep the container. keepCount += 1 continue } // We need to kill the container, but if we also want to restart the // container afterwards, make the intent clear in the message. Also do // not kill the entire pod since we expect container to be running eventually. message := reason // 可以看到如果需要重启容器，则把容器 id 放到待启动 slice 里准备重启 if restart { message = fmt.Sprintf(\"%s. Container will be killed and recreated.\", message) changes.ContainersToStart = append(changes.ContainersToStart, idx) } // 容器信息更新到待 kill 的 map 里 changes.ContainersToKill[containerStatus.ID] = containerToKillInfo{ name: containerStatus.Name, container: \u0026pod.Spec.Containers[idx], message: message, } glog.V(2).Infof(\"Container %q (%q) of pod %s: %s\", container.Name, containerStatus.ID, format.Pod(pod), message) } if keepCount == 0 \u0026\u0026 len(changes.ContainersToStart) == 0 { changes.KillPod = true } return changes } ","date":"2021-05-29","objectID":"/kubelet-hash/:0:2","tags":["container","kubelet"],"title":"容器重启原理-Kubelet hash计算","uri":"/kubelet-hash/"},{"categories":["k8s","原理"],"content":"containerChanged 在上个方法里我们看到 containerChanged的调用决定了容器是否需要重启，接下来我们看看如果计算容器的 hash 值 func containerChanged(container *v1.Container, containerStatus *kubecontainer.ContainerStatus) (uint64, uint64, bool) { expectedHash := kubecontainer.HashContainer(container) return expectedHash, containerStatus.Hash, containerStatus.Hash != expectedHash } 在文件`kubernetes/pkg/kubelet/container/helpers.go` 中提供了计算 hash 的方法 // HashContainer returns the hash of the container. It is used to compare // the running container with its desired spec. func HashContainer(container *v1.Container) uint64 { hash := fnv.New32a() hashutil.DeepHashObject(hash, *container) return uint64(hash.Sum32()) } 通过上述的代码的我们可以清楚的看到只要 v1.Container 这个 struct 里任何一个字段发生改变都会导致期望的容器 hash 值更新。 下面这种图清晰总结了 Kubelet 重启容器的过程，详相信对照下图和上面的代码大家应该能很好的了解 Kubernetes 的容器重启过程。 ","date":"2021-05-29","objectID":"/kubelet-hash/:0:3","tags":["container","kubelet"],"title":"容器重启原理-Kubelet hash计算","uri":"/kubelet-hash/"},{"categories":["golang","infra"],"content":"自研 golang 服务框架使用 es apm 完善服务监控","date":"2021-04-17","objectID":"/golang-apm/","tags":["apm","monitoring","es"],"title":"Elastic APM 补齐服务监控","uri":"/golang-apm/"},{"categories":["golang","infra"],"content":"在之前的文章介绍过服务可观测性。其中 tracing 扩展一下我们可以称之为 APM(Application Performance Monitoring)。在这篇文章里，我会具体介绍我们在 Golang 服务中的 APM 实践。 ","date":"2021-04-17","objectID":"/golang-apm/:0:0","tags":["apm","monitoring","es"],"title":"Elastic APM 补齐服务监控","uri":"/golang-apm/"},{"categories":["golang","infra"],"content":"APM(应用性能监控) 对于大部分应用程序来说性能都是很重要的一个因素，尤其对于比如网站、手机app等直接由用户访问的应用来说更是如此，因为性能较差的应用将会直接影响其用户体验。因此，能对应用进行性能监控变得非常重要，这将帮助我们找到性能瓶颈并优化。 APM 是对应用程序性能和可用性的监视和管理。APM 努力检测和诊断复杂的应用程序性能问题，以维持预期的服务等级。 目前主流的开源链路追踪实现 Jaeger, SkyWalking其实都实现了 APM 的部分目的。但这些方案各有优劣，使用成本也不太相同。今天在这里我们就介绍一下我们的实践 Elastic APM，相比其他方案 Elastic APM 具有支持框架和语音全面；监控数据全面; 查询方便；界面美观等等优点。 ","date":"2021-04-17","objectID":"/golang-apm/:0:1","tags":["apm","monitoring","es"],"title":"Elastic APM 补齐服务监控","uri":"/golang-apm/"},{"categories":["golang","infra"],"content":"Elastic APM Elastic APM是一个 Elastic Stack 的应用性能监控（APM）系统，它能够： 实时的监控软件服务和应用：为传入的请求，数据库查询，对缓存的调用，外部 HTTP 请求等收集有关响应时间的详细性能信息，使得可以轻松快速地找出并解决性能问题。 自动收集未处理的错和异常以及它们的调用栈，让你能快速定位新错误并且跟踪错误出现的频率。 收集机器级别以及特定 agent 的指标（比如 Java JVM 和 Go Runtime 的指标）。 支持分布式追踪：使你能够在一个视图中分析整个服务架构的性能。 支持真实用户监控（Real User Monitoring，RUM）：可捕获用户与客户端（例如Web浏览器）的交互。 ES APM 架构 Elastic APM 由四个组件组成： APM agents：以应用程序库的形式提供，收集程序中的性能监控数据并上报给APM server。 APM Server：从 APM agents 接收数据、进行校验和处理后写入 Elasticsearch 特定的 APM 索引中。虽然 agent 也可以实现为：将数据收集处理后直接上报到ES，不这么做官方给出的理由：使 agent 保持轻量，防止某些安全风险以及提升 Elastic 组件的兼容性。 Elasticsearch：用于存储性能指标数据并提供聚合功能。 Kibana：可视化性能数据并帮助找到性能瓶颈。 数据模型 Elastic APM agent从其检测（instrument）的应用程序中收集不同类型的数据，这些被称为事件，类型包括 span，transaction，错误和指标四种。 Span 包含有关已执行的特定代码路径的信息。它们从活动的开始到结束进行度量，并且可以与其他span具有父/子关系。 事务（Transaction） 是一种特殊的Span（没有父span，只能从中派生出子span，可以理解为“树”这种数据结构的根节点），具有与之关联的其他属性。可以将事务视为服务中最高级别的工作，比如服务中的请求等。 错误(Error)：错误事件包含有关发生的原始异常或有关发生异常时创建的日志的信息。 指标(Metric)：APM agent 自动获取基本的主机级别指标，包括系统和进程级别的 CPU 和内存指标。除此之外还可获取特定于代理的指标，例如 Java agent 中的JVM 指标和 Go agent 中的 Go Runtime 指标。 ","date":"2021-04-17","objectID":"/golang-apm/:0:2","tags":["apm","monitoring","es"],"title":"Elastic APM 补齐服务监控","uri":"/golang-apm/"},{"categories":["golang","infra"],"content":"ES APM 实践 我们的 Golang web 服务使用了自研的微服务框架，为了使用 ES APM。我们做了如下的改造，下面列举出重点部分和部分代码示例 middleware middleware 是在处理 Http Request 之前框架层面对 request 做的统一处理，比如实现 access log 记录；统一鉴权；trace 打点等。为了把 Http Request 纳入 apm，我们按照自己服务框架的中间件规范实现了 es apm middleware。 package middleware import ( \"fmt\" \"net/http\" \"strings\" \"go.elastic.co/apm\" \"go.elastic.co/apm/module/apmhttp\" ) func APM(o ...Option) bm.HandlerFunc { m := \u0026middleware{ tracer: apm.DefaultTracer, } if env.AppID != \"\" { m.tracer.Service.Name = strings.ReplaceAll(env.AppID, \".\", \"_\") } m.tracer.Service.Environment = env.DeployEnv for _, o := range o { o(m) } if m.requestIgnorer == nil { m.requestIgnorer = apmhttp.NewDynamicServerRequestIgnorer(m.tracer) } return m.handle } type middleware struct { tracer *apm.Tracer requestIgnorer apmhttp.RequestIgnorerFunc //setRouteMapOnce sync.Once //routeMap map[string]map[string]routeInfo } func (m *middleware) handle(c *bm.Context) { if !m.tracer.Recording() || m.requestIgnorer(c.Request) { c.Next() return } requestName := c.Request.Method + \" \" + c.Request.URL.Path tx, req := apmhttp.StartTransaction(m.tracer, requestName, c.Request) c.Request = req defer tx.End() body := m.tracer.CaptureHTTPRequestBody(c.Request) c.Context = apm.ContextWithTransaction(c.Context, tx) defer func() { if v := recover(); v != nil { c.AbortWithStatus(http.StatusInternalServerError) e := m.tracer.Recovered(v) e.SetTransaction(tx) setContext(\u0026e.Context, c, body) e.Send() } err := c.Error cerr := ecode.Cause(err) message := cerr.Message() if cerr.Code() == 0 { message = \"ok\" } c.Writer.WriteHeader(http.StatusOK) tx.Result = fmt.Sprintf(\"HTTP %d %s\", cerr.Code(), message) if tx.Sampled() { setContext(\u0026tx.Context, c, body) } if c.Error != nil { e := m.tracer.NewError(c.Error) e.SetTransaction(tx) setContext(\u0026e.Context, c, body) e.Handled = true e.Send() } body.Discard() }() c.Next() } func setContext(ctx *apm.Context, c *bm.Context, body *apm.BodyCapturer) { ctx.SetTag(\"region\", env.Region) ctx.SetTag(\"zone\", env.Zone) ctx.SetTag(\"hostname\", env.Hostname) ctx.SetTag(\"env\", env.DeployEnv) ctx.SetHTTPRequest(c.Request) ctx.SetHTTPRequestBody(body) ctx.SetHTTPStatusCode(ecode.Cause(c.Error).Code()) ctx.SetHTTPResponseHeaders(c.Writer.Header()) } gorm 我们使用 gorm 操作 db, 对 gorm client 改造如下 func NewMySQL(c *Config) (db *gorm.DB) { db, err := apmgorm.Open(\"mysql\", c.DSN) if err != nil { log.Error(\"orm: open error(%v)\", err) panic(err) } db.DB().SetMaxIdleConns(c.Idle) db.DB().SetMaxOpenConns(c.Active) db.DB().SetConnMaxLifetime(time.Duration(c.IdleTimeout)) db.SetLogger(ormLog{}) .... // 省略其他部分 return redis 操作 redis, 我们对客户端改造如下 type Redis struct { Redis *redis.Redis } func (r Redis) Do(ctx context.Context, commandName string, args ...interface{}) (reply interface{}, err error) { conn := r.Redis.Conn(ctx) defer conn.Close() return Do(ctx, conn, commandName, args...) } // Do calls conn.Do(commandName, args...), and also reports the operation as a span to Elastic APM. func Do(ctx context.Context, conn redis.Conn, commandName string, args ...interface{}) (interface{}, error) { spanName := strings.ToUpper(commandName) if spanName == \"\" { spanName = \"(flush pipeline)\" } span, _ := apm.StartSpan(ctx, spanName, \"db.redis\") defer span.End() return conn.Do(commandName, args...) } ... 像 grpc server, grpc client, http client 等的改造可参照官方例子。 ","date":"2021-04-17","objectID":"/golang-apm/:0:3","tags":["apm","monitoring","es"],"title":"Elastic APM 补齐服务监控","uri":"/golang-apm/"},{"categories":["golang","infra"],"content":"ES APM 部署 在上面我们介绍过 ES APM 的组件，要部署一个生产基本可用的 ES APM 至少部署清单应该如下 APM Server 至少两个实例 ElasticSearch 至少3实例组成的集群 Kibana 一个实例 ES APM 整体的部署成本的大头就是 Elasticsearch cluster。 建议以容器的方式部署，降低部署和运维成本。如果在 k8s 中部署，建议 es 实例配置好 pod antiaffinity 并且提前规划好存储容量。如果以 docker-compose 方式部署，每个 es 实例建议部署到单独的宿主机。 下面是各个组件的 docker-compose yaml 配置文件，供大家参考 Elasticsearch 对各种文件混合使用了 NioFs（ 注：非阻塞文件系统）和 MMapFs （ 注：内存映射文件系统）。配置的最大映射数量，以便有足够的虚拟内存可用于 mmapped 文件。可以在宿主机上设置： sysctl -w vm.max_map_count=262144 Elasticsearch cluster version: '3.7' services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:7.11.2 container_name: es01 deploy: resources: limits: cpus: '8' memory: 24000M reservations: cpus: '8' memory: 24000M environment: - node.name=es01 - network.host=0.0.0.0 - network.publish_host=10.1.1.16 - http.port=9205 - cluster.name=bdjs-es-docker-cluster - discovery.seed_hosts=10.1.1.17, 10.1.1.18 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true ulimits: memlock: soft: -1 hard: -1 volumes: - /data/apm/es/data:/usr/share/elasticsearch/data ports: - 9205:9205 - 9300:9300 version: '3.7' services: es02: image: docker.elastic.co/elasticsearch/elasticsearch:7.11.2 container_name: es02 deploy: resources: limits: cpus: '8' memory: 24000M reservations: cpus: '8' memory: 24000M environment: - node.name=es02 - network.host=0.0.0.0 - network.publish_host=10.1.1.17 - http.port=9205 - cluster.name=bdjs-es-docker-cluster - discovery.seed_hosts=10.1.1.16, 10.1.1.18 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true ports: - 9205:9205 - 9300:9300 ulimits: memlock: soft: -1 hard: -1 volumes: - /data/apm/es/data:/usr/share/elasticsearch/data version: '3.7' services: es03: image: docker.elastic.co/elasticsearch/elasticsearch:7.11.2 container_name: es03 deploy: resources: limits: cpus: '8' memory: 24000M reservations: cpus: '8' memory: 24000M environment: - node.name=es03 - network.host=0.0.0.0 - network.publish_host=10.1.1.18 - http.port=9205 - cluster.name=bdjs-es-docker-cluster - discovery.seed_hosts=10.1.1.16,10.1.1.17 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true ports: - 9205:9205 - 9300:9300 ulimits: memlock: soft: -1 hard: -1 volumes: - /data/apm/es/data:/usr/share/elasticsearch/data APM server 和 Kibana version: '3.7' services: kibana: image: docker.elastic.co/kibana/kibana:7.11.2 deploy: resources: limits: cpus: '2' memory: 8000M reservations: cpus: '2' memory: 8000M environment: ELASTICSEARCH_HOSTS: '[\"http://10.1.1.16:9205\",\"http://10.1.1.17:9205\",\"http://10.1.1.18:9205\"]' ports: - 5601:5601 networks: - elastic healthcheck: interval: 10s retries: 20 test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status apm-server: image: docker.elastic.co/apm/apm-server:7.11.2 deploy: resources: limits: cpus: '2' memory: 8000M reservations: cpus: '2' memory: 8000M cap_add: [\"CHOWN\", \"DAC_OVERRIDE\", \"SETGID\", \"SETUID\"] cap_drop: [\"ALL\"] ports: - 8200:8200 networks: - elastic command: \u003e apm-server -e -E apm-server.rum.enabled=true -E setup.kibana.host=kibana:5601 -E setup.template.settings.index.number_of_replicas=0 -E apm-server.kibana.enabled=true -E apm-server.kibana.host=kibana:5601 -E output.elasticsearch.hosts=[\"10.1.1.16:9205\",\"10.1.1.17:9205\",\"10.1.1.18:9205\"] healthcheck: interval: 10s retries: 12 test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/ networks: elastic: driver: bridge ","date":"2021-04-17","objectID":"/golang-apm/:0:4","tags":["apm","monitoring","es"],"title":"Elastic APM 补齐服务监控","uri":"/golang-apm/"},{"categories":["golang","infra"],"content":"APM 结果 下图是示例服务的 transactions 汇总的概览 下图是某 gprc 请求的链路 timeline ","date":"2021-04-17","objectID":"/golang-apm/:0:5","tags":["apm","monitoring","es"],"title":"Elastic APM 补齐服务监控","uri":"/golang-apm/"},{"categories":["golang","工程效率"],"content":"使用 golangci-lint 和 reviewdog 提高代码 review 效率","date":"2021-03-27","objectID":"/golangci/","tags":["gitlab ci","代码检查","review"],"title":"golangci-lint 提效代码 review","uri":"/golangci/"},{"categories":["golang","工程效率"],"content":"在 Golang 项目中为了写出健壮和优雅的代码我们会采用 linter 帮助我们检查有问题或者不规范的代码。Golang 的 linter 非常多，各自检查的范围或者内容不尽相同，如果一个个 linter 去配置和使用非常的低效和麻烦。所以我们一般会使用 golangci-lint 高效的进行代码检查。 golangci-lint 是 linter 的集合器，本身集成了众多的 golang linter(48种)；并行执行检查，速度很快。 项目一般都是多人共同开发，大家应该遵守相同的代码规范,所以 golangci-lint 的检查应该要放到 CI 流程中。随之而来的一个问题是如果检查出了 bad case 不强制修改也是没有意义的。如果我们能把 bad case 通过 comment的形式展现出来不仅能提到强提醒的作用，而且如果不 resolve comment 无法合并代码。这样的方式我认为能帮助团队形成良好的编码和习惯和风格。在这里我就介绍一下我们如何用 golangci-lint + reviewdog 高效的进行代码检查。 在介绍具体的操作步骤之前有必要简要介绍一下一个重要的工具 reviewdog ","date":"2021-03-27","objectID":"/golangci/:0:0","tags":["gitlab ci","代码检查","review"],"title":"golangci-lint 提效代码 review","uri":"/golangci/"},{"categories":["golang","工程效率"],"content":"reviewdog reviewdog 能够自动化的检测 PR（Pull Requests）里面的一些语法和格式的错误，同时提交评论，目前支持各个平台 GitHub、GitLab。总的来说 reviewdog 的工作原理是通过解析 Lint 的输出结果得 然后与代码提交的 Diff 进行比较，从中找出此次变更的问题。与直接通过静态代码扫描进行拦截相比，reviewdog 通过交互式的代码评审方式，能够减小错误检测（false positive）带来的影响。 实践步骤如下 ","date":"2021-03-27","objectID":"/golangci/:0:1","tags":["gitlab ci","代码检查","review"],"title":"golangci-lint 提效代码 review","uri":"/golangci/"},{"categories":["golang","工程效率"],"content":"Image 构建 我们先构建 CI 中需要使用的 image， image 里有 golangci-lint 和 reviewdog FROM golangci/golangci-lint:latest-alpine RUN go get -u github.com/reviewdog/reviewdog/cmd/reviewdog 如果项目里使用了 cgo, golangci-lint 镜像就不能使用 alpine 版本，因缺乏相关的依赖库。 ","date":"2021-03-27","objectID":"/golangci/:0:2","tags":["gitlab ci","代码检查","review"],"title":"golangci-lint 提效代码 review","uri":"/golangci/"},{"categories":["golang","工程效率"],"content":"gitlab CI 配置文件 在 CI 中添加 lint stage，这一步即实现代码检查和自动 comment. stages: - lint before_script: - echo \"before_script\" - go env -w GO111MODULE=on - go env -w GOPROXY=\"http://goproxy.example.co\" golangci-lint: tags: - cv-service image: hub.exapmle.co/compile/bdjs/reviewdog stage: lint script: - git fetch - export GITLAB_API=\"https://git.example.co/api/v4\" - export REVIEWDOG_INSECURE_SKIP_VERIFY=true - reviewdog -reporter=gitlab-mr-discussion # - golangci-lint run | reviewdog -f=golangci-lint -reporter=gitlab-mr-discussion only: - merge_requests allow_failure: true ","date":"2021-03-27","objectID":"/golangci/:0:3","tags":["gitlab ci","代码检查","review"],"title":"golangci-lint 提效代码 review","uri":"/golangci/"},{"categories":["golang","工程效率"],"content":"配置 access token reviewdog 实现自动 comment 需要配置环境变量 REVIEWDOG_GITLAB_API_TOKEN 创建 access token 配置 CI 环境变量(项目级) ","date":"2021-03-27","objectID":"/golangci/:0:4","tags":["gitlab ci","代码检查","review"],"title":"golangci-lint 提效代码 review","uri":"/golangci/"},{"categories":["golang","工程效率"],"content":"lint 配置 我们之前说过 golangci-lint 支持多种 linter, 可以通过配置文件灵活配置各种检查规则。 示例配置文件 .golangci.yml linters: disable-all: true enable: - deadcode - errcheck - gosimple - govet - ineffassign - staticcheck - structcheck - typecheck - unused - varcheck - bodyclose - misspell - structcheck - typecheck - varcheck - gofmt - goimports - unconvert - exportloopref linters-settings: govet: check-shadowing: true check-unreachable: true check-rangeloops: true check-copylocks: true ","date":"2021-03-27","objectID":"/golangci/:0:5","tags":["gitlab ci","代码检查","review"],"title":"golangci-lint 提效代码 review","uri":"/golangci/"},{"categories":["golang","工程效率"],"content":"reviewdog 配置 reviewdog 命令和 comment 格式配置 runner: golint: cmd: golangci-lint run errorformat: - \"%f:%l:%c: %m\" level: warning 经过上述的工作，如果提交 MR 即能触发 lint 和 自动 comment，一切正常的话你会看到下面的效果 ","date":"2021-03-27","objectID":"/golangci/:0:6","tags":["gitlab ci","代码检查","review"],"title":"golangci-lint 提效代码 review","uri":"/golangci/"},{"categories":["golang","工程效率"],"content":"可能遇到的问题 如果遇到以下问题 could not import C (cgo preprocessing failed)]] golangci-lint:alpine 镜像缺乏依赖库, 请使用 golangci-lint:latest 或特定版本 reviewdog: failed to get merge-base commit: exit status 128 需要先 git fetch 获得远端分支的更新 ","date":"2021-03-27","objectID":"/golangci/:0:7","tags":["gitlab ci","代码检查","review"],"title":"golangci-lint 提效代码 review","uri":"/golangci/"},{"categories":["golang","grpc","实现"],"content":"在 grpc 中自定义一致性 hash 负载均衡的实现","date":"2021-02-21","objectID":"/gprc-balancer/","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"在开发过程中我们遇到了需要 session 保持的场景，即同一个用户的请求需要我们路由到后端服务的同一个实例。如果是 http 请求我们可用利用 nginx 的 ip hash 负载均衡策略来实现此目的；如果服务之间的调用我们用的是 gprc， 该怎么实现呢？在这篇文章里我就介绍一下我们如何实现 grpc 中的 session 保持。 在介绍实现 自定义 gprc balancer 之前，我们必须了解一下 grpc 中服务发现和负载均衡的原理。 ","date":"2021-02-21","objectID":"/gprc-balancer/:0:0","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"grpc 负载均衡 下面这张图展示了在 grpc 中实现负载均衡的的两个核心模块 resovler 和 balancer。 ","date":"2021-02-21","objectID":"/gprc-balancer/:1:0","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"Resolver gprc client 通过 server name 和 grpc server 交互式，resolver 负责解析 server name, 通过 server name 从注册中心实时获取当前 server 的地址列表，同步发送给 Balancer ","date":"2021-02-21","objectID":"/gprc-balancer/:1:1","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"Balancer 接收从 Resolver 发送的server 地址列表，建立并维护连接状态；每次当 Client 发起 RPC 调用时，按照一定算法从连接池中选择一个连接进行发起调用 ","date":"2021-02-21","objectID":"/gprc-balancer/:1:2","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"核心模块原理 ","date":"2021-02-21","objectID":"/gprc-balancer/:2:0","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"Resolver 流程 代码 resolver/resolver.go重点定义如下 // scheme://authority/endpoint type Target struct { Scheme string Authority string Endpoint string } // 向grpc注册服务发现实现时，实际上注册的是Builder type Builder interface { // 创建Resolver，当resolver发现服务列表更新，需要通过ClientConn接口通知上层 Build(target Target, cc ClientConn, opts BuildOption) (Resolver, error) Scheme() string } type Resolver interface { // 当有连接被出现异常时，会触发该方法，因为这时候可能是有服务实例挂了，需要立即实现一次服务发现 ResolveNow(ResolveNowOption) Close() } // type ClientConn interface { // 服务列表和服务配置更新回调接口 UpdateState(State) // 服务列表更新通知接口 NewAddress(addresses []Address) // 服务配置更新通知接口 NewServiceConfig(serviceConfig string) } 其中 Builder 接口用来创建 Resolver，我们可以提供自己的服务发现实现，然后将其注册到 grpc 中，其中通过 scheme 来标识，而 Resolver 接口则是提供服务发现功能。当 resover 发现服务列表发生变更时，会通过 ClientConn 回调接口通知上层。 那么注册进来的 resolver 在哪里用到的呢？当创建客户端的时候调用 DialContext 方法创建 ClientConn 的时候回进行如下操作 拦截器处理 各种配置项处理 解析 target 获取 resolver 创建 ccResolverWrapper 创建 clientConn 的时候回根据 target 解析出 scheme，然后根据 scheme 去找已注册对应的 resolver，如果没有找到则使用默认的 resolver。 相关代码可以在 grpc/clientconn.go 中看到。 ","date":"2021-02-21","objectID":"/gprc-balancer/:2:1","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"Balancer 流程 代码 balancer/balancer.go 重点定义如下 // 声明了balancer需要用到的回调接口 type ClientConn interface { // 根据地址创建网络连接 NewSubConn([]resolver.Address, NewSubConnOptions) (SubConn, error) // 移除无效网络连接 RemoveSubConn(SubConn) // 更新Picker，Picker用于在执行rpc调用时执行负载均衡策略，选举一条连接发送请求 UpdateBalancerState(s connectivity.State, p Picker) // 立即触发服务发现 ResolveNow(resolver.ResolveNowOption) Target() string } // 根据当前的连接列表，执行负载均衡策略选举一条连接发送rpc请求 type Picker interface { Pick(ctx context.Context, opts PickOptions) (conn SubConn, done func(DoneInfo), err error) } // Builder用于创建Balancer，注册的时候也是注册builder type Builder interface { Build(cc ClientConn, opts BuildOptions) Balancer Name() string } type Balancer interface { // 当有连接状态变更时，回调 HandleSubConnStateChange(sc SubConn, state connectivity.State) // 当resolver发现新的服务地址列表时调用（有可能地址列表并没有真的更新） HandleResolvedAddrs([]resolver.Address, error) Close() } 当 Resolver 发现新的服务列表时，最终会调用 Balancer 的 HandleResolvedAddrs 方法进行通知；Balancer 通过 ClientConn 的接口创建网络连接，然后根据当前的网络连接连接构造新的 Picker，然后回调 ClientConn.UpdateBalancerState 更新 Picker。当发送 grpc 请求时，会先执行 Picker 的接口，根据具体的负载均衡策略选举一条网络连接，然后发送rpc请求。 ","date":"2021-02-21","objectID":"/gprc-balancer/:2:2","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"一致性 Hash balancer 实现 ","date":"2021-02-21","objectID":"/gprc-balancer/:3:0","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"一致性 Hash 在实现 balancer 之前，先简单介绍一下一致性 Hash 基本原理是 hash ring(hash 环)，即将节点 node 本身也 hash 到环上，通过数据和节点的 hash 相对位置来决定数据归属，因此当有新 node 加入时只有一部分的数据迁移。但事实上，这样的一致性hash导致数据分布不均匀，因为 node 在 hash ring 上的分布不均匀。分布不均匀的问题通过引入虚拟节点来解决，虚拟节点是均匀分布在环上的，数据做两次 match，最终到实际节点上。这样来保证数据分布的均匀性。 我们这里用一致性 Hash 就是为了同一个用户的请求能路由到同一个 server 实例。 type Ketama struct { sync.Mutex hash HashFunc replicas int // 虚拟节点数 keys []int // 构造的 hash ring hashMap map[int]string } 添加节点 在添加节点时，为每个节点创建 replica 个虚拟节点，并计算虚拟节点的 hash 值存入 hash ring，也就是 keys 这个 slice 中，同时把这些虚拟节点的 hash 值与 node 的对应关系保存在 hashMap。最后给 keys 排个序，就像在环上分布，顺时针递增一样。 func (h *Ketama) Add(nodes ...string) { h.Lock() defer h.Unlock() for _, node := range nodes { for i := 0; i \u003c h.replicas; i++ { key := int(h.hash([]byte(Salt + strconv.Itoa(i) + node))) if _, ok := h.hashMap[key]; !ok { h.keys = append(h.keys, key) } h.hashMap[key] = node } } sort.Ints(h.keys) } 查询节点 Get 方法是获取数据对应的节点，相当于负载均衡中源 ip 对应到哪个节点。计算数据的 hash，并在 hash Ring 上二分查找第一个大于 hash 的虚拟节点，也就通过hashMap 找到了对应的真实节点。 func (h *Ketama) Get(key string) (string, bool) { if h.IsEmpty() { return \"\", false } hash := int(h.hash([]byte(key))) h.Lock() defer h.Unlock() idx := sort.Search(len(h.keys), func(i int) bool { return h.keys[i] \u003e= hash }) if idx == len(h.keys) { idx = 0 } str, ok := h.hashMap[h.keys[idx]] return str, ok } ","date":"2021-02-21","objectID":"/gprc-balancer/:3:1","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"balancer 实现 在了解了 grpc 负载均衡的工作原理之后，实现自定义 balancer 需要完成的工作： 实现 PickerBuilder，Build 方法返回 balancer.Picker 实现 balancer.Picker，Pick 方法实现负载均衡算法逻辑 调用 balancer.Registet 注册自定义 Balancer 实现 Build 方法 func (b *consistentHashPickerBuilder) Build(buildInfo base.PickerBuildInfo) balancer.V2Picker { grpclog.Infof(\"consistentHashPicker: newPicker called with buildInfo: %v\", buildInfo) if len(buildInfo.ReadySCs) == 0 { return base.NewErrPickerV2(balancer.ErrNoSubConnAvailable) } // 构造 consistentHashPicker picker := \u0026consistentHashPicker{ subConns: make(map[string]balancer.SubConn), hash: NewKetama(3, nil), // 构造一致性hash consistentHashKey: b.consistentHashKey, // 用于计算hash的key } for sc, conInfo := range buildInfo.ReadySCs { node := conInfo.Address.Addr picker.hash.Add(node) picker.subConns[node] = sc } return picker } 实现 Pick 方法 func (p *consistentHashPicker) Pick(info balancer.PickInfo) (balancer.PickResult, error) { var ret balancer.PickResult key, ok := info.Ctx.Value(p.consistentHashKey).(string) if ok { targetAddr, ok := p.hash.Get(key) // 根据key的hash值挑选出对应的节点 if ok { ret.SubConn = p.subConns[targetAddr] } } return ret, nil } ","date":"2021-02-21","objectID":"/gprc-balancer/:3:2","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["golang","grpc","实现"],"content":"一致性 Hash balancer 使用 func NewClient(cfg *warden.ClientConfig) (rb.ResourceTaskClient, error) { // 初始化balancer balancer.InitConsistentHashBuilder(\"test\") if cfg == nil { cfg = \u0026warden.ClientConfig{} } client := warden.NewClient(cfg) client.UseOpt(grpc.WithBalancerName(balancer.Name)) cc, err := client.Dial(context.Background(), fmt.Sprintf(\"discovery://default/%s\", AppID)) if err!=nil{ panic(err) } return rb.NewResourceTaskClient(cc), err } func (s *Service) GrpcTest(ctx context.Context) (reply *rb.GetTaskResReply, err error){ // 在context中塞入hash key ctx = context.WithValue(ctx, \"test\", strconv.Itoa(rand.Intn(1000))) reply, err = s.gClient.GetTaskRes(ctx, \u0026rb.GetTaskResReq{TaskId: \"ct340984037629763021\"}) return } 我们把选中的节点信息打印出来展示如下, 不同的 key 选取了不同的节点，如果同一个 key 那么请求还是路由到同一个节点。由此实现我们的 session 保持的目的。 INFO 02/20-14:46:32.958 /Users/lxkaka/bili/cv-service/interface/balancer/conhash.go:61 hash map: map[543647748:10.217.28.143:9000 946644225:10.217.27.218:9000 2448604328:10.217.27.218:9000 2521173259:10.217.27.218:9000 3082607229:10.217.28.143:9000 3098647747:10.217.28.143:9000] INFO 02/20-14:46:32.958 /Users/lxkaka/bili/cv-service/interface/balancer/conhash.go:62 hash key:71 INFO 02/20-14:46:32.958 /Users/lxkaka/bili/cv-service/interface/balancer/conhash.go:66 ip addr:10.217.28.143:9000 INFO 02/20-14:48:05.270 /Users/lxkaka/bili/cv-service/interface/balancer/conhash.go:61 hash map: map[543647748:10.217.28.143:9000 946644225:10.217.27.218:9000 2448604328:10.217.27.218:9000 2521173259:10.217.27.218:9000 3082607229:10.217.28.143:9000 3098647747:10.217.28.143:9000] INFO 02/20-14:48:05.270 /Users/lxkaka/bili/cv-service/interface/balancer/conhash.go:62 hash key:705 INFO 02/20-14:48:05.270 /Users/lxkaka/bili/cv-service/interface/balancer/conhash.go:66 ip addr:10.217.27.218:9000 ","date":"2021-02-21","objectID":"/gprc-balancer/:4:0","tags":["grpc","负载均衡","session stickiness"],"title":"gRPC 一致性 Hash balancer 实现","uri":"/gprc-balancer/"},{"categories":["基础","infra"],"content":"docker的关键实现总结和学习","date":"2021-01-23","objectID":"/docker-principle/","tags":["docker","原理","linux"],"title":"Docker 核心原理","uri":"/docker-principle/"},{"categories":["基础","infra"],"content":"对 Docker 的使用大部分都比较熟悉，但是说到 docker 的实现原理很多人还是一知半解。我把在团队内部做的一次 Docker 核心原理分享总结到文章里，以供参考。 ","date":"2021-01-23","objectID":"/docker-principle/:0:0","tags":["docker","原理","linux"],"title":"Docker 核心原理","uri":"/docker-principle/"},{"categories":["基础","infra"],"content":"Docker 的优势 Build once, Run anywhere 上面这句话很精辟的总结了 docker 的优点。我从下面几点具体描述 docker 带给开发者的能力 应用标准化 无论什么语言开发的应用，我们都能用 dockerfile 和构建脚本方便的进行应用构建打包，代码库 + 构建 + registry 统一了 CI/CD 流程，也提升了效率。 环境一致 由于应用和依赖全部构建成镜像，做到了一次构建多次交付，无论是开发，测试还是上线环境都是一致的。大大提高了开发效率 应用隔离 由于通过 docker 部署的应用，容器之间相互隔离，并且能按需分配资源。大大提高了运维效率和资源利用率 ","date":"2021-01-23","objectID":"/docker-principle/:1:0","tags":["docker","原理","linux"],"title":"Docker 核心原理","uri":"/docker-principle/"},{"categories":["基础","infra"],"content":"架构 Docker使用了 C/S 体系架构，Docker 客户端与 Docker 守护进程通信，Docker 守护进程负责构建，运行和分发 Docker 容器。Docker 客户端和守护进程可以在同一个系统上运行，也可以将Docker客户端连接到远程Docker守护进程。 我们日常在命令行的操作 docker build, docker push, docker pull, docke build 等等操作都是客户端通过 rest api 请求与 Docker 守护进程交互。 ","date":"2021-01-23","objectID":"/docker-principle/:2:0","tags":["docker","原理","linux"],"title":"Docker 核心原理","uri":"/docker-principle/"},{"categories":["基础","infra"],"content":"实现原理 下面我们就介绍一下 Docker 在实现隔离，资源控制，文件系统等关键部分所采用的的技术 ","date":"2021-01-23","objectID":"/docker-principle/:3:0","tags":["docker","原理","linux"],"title":"Docker 核心原理","uri":"/docker-principle/"},{"categories":["基础","infra"],"content":"Namespace Linux manaul page 很好的介绍了 namespace 的作用。 namespace提供了一种内核级别隔离系统资源的方法，通过将系统的全局资源放在不同的 namespace 中，来实现资源隔离的目的。不同 namespace 的进程拥有相互隔离的系统资源。 这里指的资源隔离包含以下这些： Mount: 隔离文件系统挂载点 UTS: 隔离主机名和域名信息 IPC: 隔离进程间通信 PID: 隔离进程的 ID Network: 隔离网络资源 User: 隔离用户和用户组 通过下面的 clone 系统调用可以创建新的进程，参数 flags 控制创建的进程所属的 namespace, 多个 flags 可以同时创建多个 namespace。 int clone(int (*child_func)(void *), void *child_stack, int flags, void *arg); 我们知道容器本质上就是隔离的进程，Docker 在创建容器的时候就是使用 namespace 来实现了容器与容器，容器与宿主机的隔离。 每个进程都有一个 /proc/[pid]/ns 的目录，里面保存了该进程所在对应 namespace 的链接, 我们来查看某个容器也就是某一个进程对应的 namespace 文件描述 root@lxkaka-server:~# ls -l /proc/23204/ns total 0 lrwxrwxrwx 1 root root 0 Jan 9 13:51 cgroup -\u003e 'cgroup:[4026531835]' lrwxrwxrwx 1 root root 0 Jan 20 12:11 ipc -\u003e 'ipc:[4026532341]' lrwxrwxrwx 1 root root 0 Jan 20 12:11 mnt -\u003e 'mnt:[4026532339]' lrwxrwxrwx 1 root root 0 Jul 9 2020 net -\u003e 'net:[4026532344]' lrwxrwxrwx 1 root root 0 Jan 20 12:11 pid -\u003e 'pid:[4026532342]' lrwxrwxrwx 1 root root 0 Jan 9 13:51 pid_for_children -\u003e 'pid:[4026532342]' lrwxrwxrwx 1 root root 0 Jan 9 13:51 user -\u003e 'user:[4026531837]' lrwxrwxrwx 1 root root 0 Jan 20 12:11 uts -\u003e 'uts:[4026532340]' 每个文件都是对应 namespace 的文件描述符，方括号里面的值是 namespace 的 inode，如果两个进程所在的 namespace 一样，那么它们列出来的 inode 是一样的。 我们对比一下另外一个宿主机上的进程 root@lxkaka-server:~# ls -l /proc/20/ns total 0 lrwxrwxrwx 1 root root 0 Jan 20 12:19 cgroup -\u003e 'cgroup:[4026531835]' lrwxrwxrwx 1 root root 0 Jan 20 12:19 ipc -\u003e 'ipc:[4026531839]' lrwxrwxrwx 1 root root 0 Jan 20 12:19 mnt -\u003e 'mnt:[4026531840]' lrwxrwxrwx 1 root root 0 Jan 20 12:19 net -\u003e 'net:[4026531993]' lrwxrwxrwx 1 root root 0 Jan 20 12:19 pid -\u003e 'pid:[4026531836]' lrwxrwxrwx 1 root root 0 Jan 20 12:19 pid_for_children -\u003e 'pid:[4026531836]' lrwxrwxrwx 1 root root 0 Jan 20 12:19 user -\u003e 'user:[4026531837]' lrwxrwxrwx 1 root root 0 Jan 20 12:19 uts -\u003e 'uts:[4026531838]' 可以看到进程，文件系统，网络，进程通信，主机名都是不同的 namespace。 网络模式 Docker 虽然可以通过命名空间创建一个隔离的网络环境，但是如果我们的应用需要对外提供服务，不能与外界进行通信是没有意义的。Docker 提供了多种网络模式来实现容器和外部的通信。 我们重点介绍一下 docker 默认的网络模式 bridge 守护进程会创建一对对等虚拟设备接口 veth pair，将其中一个接口设置为容器的 eth0 接口（容器的网卡），另一个接口放置在宿主机的命名空间中，以类似 vethxxx 这样的名字命名，从而将宿主机上的所有容器都连接到这个内部网络上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。 容器访问外部网络 请求通过 veth pair 到达 docker0 处，docker0 网桥开启了IP forwarding功能，将请求发送至宿主机eth0； 宿主机处理请求时，使用 SNAT 规则，将源地址替换成了宿主机 ip，然后把报文转发出去 外部访问容器 docker run -p 时，docker 实际是在 iptables 做了DNAT规则，实现端口转发功能。 # iptables -t nat -L Chain DOCKER (2 references) target prot opt source destination RETURN all -- anywhere anywhere DNAT tcp -- anywhere anywhere tcp dpt:8888 to:172.17.0.4:8888 外部请求访问访问地址为 宿主机ip:8888，网络包到达 eth0; 命中 iptables dnat 规则，把宿主机ip:8888 替换成 容器ip:8888; 宿主机把报文通过 veth pair 传递到容器 eth0 其他网络模式 Host 模式 和宿主机共用一个 Network Namespace。容器将不会虚拟出自己的网卡，配置自己的 IP 等，而是使用宿主机的 IP 和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的 Contanier 模式 这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等 None 模式 使用none模式，Docker 容器拥有自己的 Network Namespace，但是，并不为Docker 容器进行任何网络配置。也就是说，这个 Docker 容器没有网卡、IP、路由等信息。需要我们自己为 Docker 容器添加网卡、配置 IP 等 ","date":"2021-01-23","objectID":"/docker-principle/:3:1","tags":["docker","原理","linux"],"title":"Docker 核心原理","uri":"/docker-principle/"},{"categories":["基础","infra"],"content":"Cgroups 通过 Linux Namespace 为新创建的进程隔离了文件系统、网络并与宿主机器之间的进程相互隔离，但是 Namespace 并不能够为我们提供物理资源上的隔离，比如 CPU 或者内存。所以 Docker 还借助了 Linux Cgroups 来达到上述目的。 CGroup 全称 Linux Control Group， 是 Linux 内核的一个功能，用来限制，控制与分离一个进程组群的资源（如CPU、内存、磁盘输入输出等) 一组按照某种标准划分的进程，其表示了某进程组，Cgroups 中的资源控制都是以控制组为单位实现，一个进程可以加入到某个控制组。而资源的限制是定义在这个组上，简单点说，cgroup 的呈现就是一个目录带一系列的可配置文件。 理解 cgroups 的几个关键字 cgroup 进程组 进程按照某种标准组织成一个控制组，资源的控制定义在这个组上，新加入的进程就继承该组的配置。比如 docker 启动的容器都加入 docker 这个进程组 subsystem 子系统 cgroups 为每种可以控制的资源定义了一个子系统(即资源控制器) root@lxkaka-server:~# lssubsys cpuset # 分配单独的 cpu 节点或者内存节点 cpu,cpuacct # 限制进程的 cpu 使用率;cpu 使用统计 blkio # 限制进程的块设备 io memory # 限制进程的 memory 使用量 devices # 控制进程能够访问某些设备 freezer # 挂起或者恢复 cgroups 中的进程。 net_cls,net_prio # 可以标记 cgroups 中进程的网络数据包，对数据包进行控制 hierarch 层级关系 由一系列控制组以一个树状结构排列而成，hierarch 通过绑定对应的子系统进行资源调度。hierarch 中的 cgroup 节点可以包含零或多个子节点，子节点继承父节点的属性。整个系统可以有多个hierarchy。 配置示例 docker run -d --cpus=0.1 --memory=100MB busybox root@lxkaka-server:/sys/fs/cgroup/cpu/docker/ee43bb16af9947ed9e8498ad42c859318e001365043e4cda410f68b4a0d79378# cat cpu.cfs_quota_us 10000 root@lxkaka-server:/sys/fs/cgroup/memory/docker/ee43bb16af9947ed9e8498ad42c859318e001365043e4cda410f68b4a0d79378# cat memory.limit_in_bytes 104857600 ","date":"2021-01-23","objectID":"/docker-principle/:3:2","tags":["docker","原理","linux"],"title":"Docker 核心原理","uri":"/docker-principle/"},{"categories":["基础","infra"],"content":"文件驱动 Docker 中的每一个镜像都是由一系列只读的层组成的，Dockerfile 中的每一个命令都会在已有的只读层上创建一个新的层。当镜像被 docker run 命令创建时就会在镜像的最上层添加一个可写的层，也就是容器层，所有对于运行时容器的修改其实都是对这个容器读写层的修改。容器和镜像的区别就在于，所有的镜像都是只读的，而每一个容器其实等于镜像加上一个可读写的层，也就是同一个镜像可以对应多个容器。 这种分层的逻辑是什么呢？ 这就是docker 里文件驱动的职责，负责镜像和容器的文件系统组织。 目前 docker 默认的文件驱动是 overlay2, 它是基于 Linux OverlayFS 的，下面这张图映射了 docker 容器的文件层级结构和 OverlayFS 的对应关系。 OverlayFS 是一种堆叠文件系统，它依赖并建立在其它的文件系统之上，不直接参与磁盘空间结构的划分，仅将原来文件系统中不同目录和文件进行 merge。用户看到就是这个 merged 目录。这些被处理的每个目录都被称为层，视图统一的过程则称为联合挂载。多个目录进行层叠，肯定具有上下层关系，OverlayFS 将下层的目录称为lowerdir，上层的目录称为upperdir，被暴露的统一视图目录称为 merged。 容器文件系统层级示例 root@lxkaka-server:~# docker inspect a2b1e73dda7f ... \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay2/77a2b28678d5406b8184e213a725bac51e4bb0132cb2c2c7928c9805bbeb57e3-init/diff:/var/lib/docker/overlay2/2c24c0bcddaf45a0442e2aa1f061e5a10ac0bb327519c4c7ddc11ecb85878bb1/diff:/var/lib/docker/overlay2/4c689dd9bd8baaff479edf0549e7888b754bf9a635d354e4a7f1531bf946f936/diff:/var/lib/docker/overlay2/84b84a32ceb721075356a9f9d4e8f8c3272a3a440f732a639993293a221236cd/diff:/var/lib/docker/overlay2/fd803c843f13047fd21fda7270a8bec065a4ba62310f95aa5524ec3156755e25/diff:/var/lib/docker/overlay2/fe43a13d9aa35f8b6038d26db6fa83bbd9a108fa59e9599e520505e9b028105d/diff:/var/lib/docker/overlay2/ec929233dc85f54a5b13e906dc1ebd20328fefdebfe2a411a06b6551c501928c/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/77a2b28678d5406b8184e213a725bac51e4bb0132cb2c2c7928c9805bbeb57e3/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/77a2b28678d5406b8184e213a725bac51e4bb0132cb2c2c7928c9805bbeb57e3/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/77a2b28678d5406b8184e213a725bac51e4bb0132cb2c2c7928c9805bbeb57e3/work\" }, \"Name\": \"overlay2\" }, ... 我们可以看到 lowerdir 包含了多层，每个镜像层目录中包含了一个文件 link，文件内容则是当前层对应的短标识符；lower 文件指向了其所有的父层，其文件内容则是父层id的短标识符；镜像层的内容则存放在 diff 目录 lowerdir 某层示例 root@lxkaka-server:/var/lib/docker/overlay2/2c24c0bcddaf45a0442e2aa1f061e5a10ac0bb327519c4c7ddc11ecb85878bb1# ls diff link lower work root@lxkaka-server:/var/lib/docker/overlay2/2c24c0bcddaf45a0442e2aa1f061e5a10ac0bb327519c4c7ddc11ecb85878bb1# ls diff link lower work root@lxkaka-server:/var/lib/docker/overlay2/2c24c0bcddaf45a0442e2aa1f061e5a10ac0bb327519c4c7ddc11ecb85878bb1# cat link 5NC3EXCFR4DKUVKSTJQNDVB3SWroot@lxkaka-server:/var/lib/docker/overlay2/2c24c0bcddaf45a0442e2aa1f061e5a10ac0bb327519c4c7ddc11ecb85878bb1# cat lower l/XJFQUCXRFOKWIS2NCRBGZOQ573:l/AOHICFFE7WZQDQIZIGXPZSUVPN:l/4DLC3KZMRJUW4GUIU3PIACENJB:l/55TW2N2I52PI5IQD7U5PCZ5H4O:l/5R5RBP6MZNORLVMAOJHVTWMWRN 查看容器层 root@lxkaka-server:/var/lib/docker/overlay2/77a2b28678d5406b8184e213a725bac51e4bb0132cb2c2c7928c9805bbeb57e3# ls diff root root@lxkaka-server:/var/lib/docker/overlay2/77a2b28678d5406b8184e213a725bac51e4bb0132cb2c2c7928c9805bbeb57e3# ls merged/ bin boot data dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var # 进入到对应的容器 root@a2b1e73dda7f:/# ls bin boot data dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 我们可以看到容器的目录结构和 merged 是一致的。 ","date":"2021-01-23","objectID":"/docker-principle/:3:3","tags":["docker","原理","linux"],"title":"Docker 核心原理","uri":"/docker-principle/"},{"categories":["基础","infra"],"content":"多阶段构建 在文章的最后我们介绍比较实用的优化构建的方法，多阶段构建 下面的 dockerfile 是一个简单的 golang 应用构建过程 FROM golang:1.15.6-alpine3.12 RUN mkdir -p /src WORKDIR /src COPY src/ . RUN go build -o app . EXPOSE 8000 ENTRYPOINT [\"./app\"] 对于这个例子来说我们其实只需要构建后的二进制包，其他文件包括 golang 我们都是不需要的。 我们可以把上述构建成两个阶段，可以大大减小构建后的镜像大小 FROM golang:1.15.6-alpine3.12 as builder RUN mkdir -p /src WORKDIR /src COPY src/ . RUN go build -o app . FROM alpine:3.12 COPY --from=builder /src/ . EXPOSE 8000 ENTRYPOINT [\"./app\"] $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE app-test v2 0fde852a1af1 4 seconds ago 13MB app-test v1 9d528c34a9f9 15 seconds ago 306MB v2 是多阶段构建出来的可以看到相比 v1 体积减小了20多倍。 ","date":"2021-01-23","objectID":"/docker-principle/:4:0","tags":["docker","原理","linux"],"title":"Docker 核心原理","uri":"/docker-principle/"},{"categories":["backend","infra"],"content":"在我们当前业务场景下使用消息中间件是必不可少的，其中 kafka 和 pulsar 是我们消息中间件的首选项。在之前的一篇文章 中我们提到过这两种消息队列，而在我们了解和评价和他们的性能和可用性时我认为底层的存储结构是非常重要的或者说是最重要的的一个因素。所以，在这篇文章里我们对二者的存储结构做一个对比和汇总。 ","date":"2020-12-26","objectID":"/kakfa-pulsar-storage/:0:0","tags":["kafka","message","middleware"],"title":"Kafka 和 Pulsar 存储结构对比","uri":"/kakfa-pulsar-storage/"},{"categories":["backend","infra"],"content":"Kafka 关于 Kafka 的介绍和使用有很多资料可以供大家参考，这里我们不做介绍。 ","date":"2020-12-26","objectID":"/kakfa-pulsar-storage/:1:0","tags":["kafka","message","middleware"],"title":"Kafka 和 Pulsar 存储结构对比","uri":"/kakfa-pulsar-storage/"},{"categories":["backend","infra"],"content":"Kafka 的 Partition Kakfa 的一个 topic 在物理上被分成多个 partition 用以存储消息，各 partition 以目录形式在 leader broker 及其多副本 brokers 上持久化存储。partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 下面是两个实例 topic ，在 kafka 数据目录下的分区存储情况： |--test_topic-0 |--test_topic-1 |--test_topic-2 |--test_log-0 |--tesst_log-1 |--test_log-2 ","date":"2020-12-26","objectID":"/kakfa-pulsar-storage/:1:1","tags":["kafka","message","middleware"],"title":"Kafka 和 Pulsar 存储结构对比","uri":"/kakfa-pulsar-storage/"},{"categories":["backend","infra"],"content":"Partition 的组成 每个 partion(目录)相当于一个巨型文件被平均分配到多个大小相等 segment(段)数据文件中。但每个段 segment file 消息数量不一定相等，这种特性方便 old segment file 快速被删除。 每个 partiton 只需要支持顺序读写就行了，segment 文件生命周期由服务端配置参数决定。 下面这张图展示了 partition 的文件存储 ","date":"2020-12-26","objectID":"/kakfa-pulsar-storage/:1:2","tags":["kafka","message","middleware"],"title":"Kafka 和 Pulsar 存储结构对比","uri":"/kakfa-pulsar-storage/"},{"categories":["backend","infra"],"content":"Segment 存储结构 Partition 中的 segment file 存储结构 segment file组成：由2大部分组成，分别为 index 文件和 data 文件，这两个文件一一对应，成对出现，后缀 “.index” 和 “.log” 分别表示为 segment 索引文件、数据文件. segment 文件命名规则：partion 全局的第一个 segment 从0开始，后续每个s egment 文件名为上一个 segment 文件最后一条消息的 offset 值。数值最大为64位 long 大小，19位数字字符长度，没有数字用0填充。 创建一个 topic 只包含一个 partition，设置每个 segment 大小为 500MB,并启动 producer 向 Kafka broker 写入大量数据,如下图所示 partiton test_topic-0 文件内容 index file 存储元数据，log file 存储消息，index file 中元数据指向对应 log file 中 message 的物理偏移地址。其中以 index file 中元数据3,497为例，依次在 log file 中表示第3个message(在全局partiton表示第368772个message)、以及该消息的物理偏移地址为497。 以下图展示说明 segmen t中 index file\u003c—-\u003elog file对应关系物理结构如下 其中 message 的物理结构如下 ","date":"2020-12-26","objectID":"/kakfa-pulsar-storage/:1:3","tags":["kafka","message","middleware"],"title":"Kafka 和 Pulsar 存储结构对比","uri":"/kakfa-pulsar-storage/"},{"categories":["backend","infra"],"content":"消息查找 例如读取offset=368776的message，需要通过下面2个步骤查找。 查找segment file 上述图2为例，其中 00000000000000000000.index 表示最开始的文件，起始偏移量(offset)为0.第二个文件 00000000000000368769.index 的消息量起始偏移量为368770 = 368769 + 1.同样，第三个文件 00000000000000737337.index 的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset 二分查找文件列表，就可以快速定位到具体文件。 当offset=368776时定位到 00000000000000368769.index|log 通过segment file查找message 通过第一步定位到segment file，当offset=368776时，依次定位到 00000000000000368769.index 的元数据物理位置和00000000000000368769.log 的物理偏移地址，然后再通过 00000000000000368769.log 顺序查找直到offset=368776为止。 index file 采取稀疏索引存储方式，它减少索引文件大小，通过 mmap 可以直接内存操作，稀疏索引为数据文件的每个对应 message 设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。 ","date":"2020-12-26","objectID":"/kakfa-pulsar-storage/:1:4","tags":["kafka","message","middleware"],"title":"Kafka 和 Pulsar 存储结构对比","uri":"/kakfa-pulsar-storage/"},{"categories":["backend","infra"],"content":"Pulsar Pulsar 的底层存储使用的是 Apache Bookkeeper, 所以要了解 pulsar 的存储结构就是理解 bookkeeper 的存储原理。 BookKeeper集群由两大部分组成： 一组独立的存储服务器，成为bookies 一个元数据存储系统，提供服务发现以及元数据管理服务 BookKeeper 架构属于典型的 slave-slave 架构，zk 存储其集群的 meta 信息，这种模式的好处显而易见，server 端变得非常简单，所有节点都是一样的角色和处理逻辑，能够这样设计的主要原因是其副本没有 leader 和 follower 之分。 ","date":"2020-12-26","objectID":"/kakfa-pulsar-storage/:2:0","tags":["kafka","message","middleware"],"title":"Kafka 和 Pulsar 存储结构对比","uri":"/kakfa-pulsar-storage/"},{"categories":["backend","infra"],"content":"Bookkeeper 存储实现 在 bookkeeper 中一个 Log/Stream/Topic 可以由下面的部分组成 Ledger：它是 BK 的一个基本存储单元（本质上还是一种抽象），BK Client 的读写操作也都是以 Ledger 为粒度的； Fragment：BK 的最小分布单元（实际上也是物理上的最小存储单元），也是 Ledger 的组成单位，默认情况下一个 Ledger 会对应的一个 Fragment（一个 Ledger 也可能由多个 Fragment 组成）； Entry：每条日志都是一个 Entry，它代表一个 record，每条 record 都会有一个对应的 entry id； 关于 Fragment，它是 Ledger 的物理组成单元，也是最小的物理存储单元，在以下两种情况下会创建新的 Fragment： 当创建新的 Ledger 时； 当前 Fragment 使用的 Bookies 发生写入错误或超时，系统会在剩下的 Bookie 中新建 Fragment，但这时并不会新建 Ledger，因为 Ledger 的创建和关闭是由 Client 控制的，这里只是新建了 Fragment（需要注意的是：这两个 Fragment 对应的 Ensemble Bookie 已经不一样了，但它们都属于一个 Ledger，这里并不一定是一个 Ensemble Change 操作）。 新建 ledger Ledger 是一组追加有序的记录，它是由 Client 创建的，然后由其进行追加写操作。每个 Ledger 在创建时会被赋予全局唯一的 ID，其他的 Client 可以根据 Ledger ID，对其进行读取操作。创建 Ledger 及 Entry 写入的相关过程如下： Client 在创建 Ledger 的时候，从 Bookie Pool 里面按照指定的数据放置策略挑选出一定数量的 Bookie，构成一个 Ensemble； 每条 Entry 会被并行地发送给 Ensemble 里面的部分 Bookies（每条 Entry 发送多少个 Bookie 是由 Write Quorum size 设置、具体发送哪些 Bookie 是由 Round Robin 算法来计算），并且所有 Entry 的发送以流水线的方式进行，也就是意味着发送第 N + 1 条记录的写请求不需要等待发送第 N 条记录的写请求返回； 对于每条 Entry 的写操作而言，当它收到 Ensemble 里面大多数 Bookie 的确认后（这个由 Ack Quorum size 来设置），Client 认为这条记录已经持久化到这个 Ensemble 中，并且有大多数副本。 这里引入了三个重要的概念，它们也是 BookKeeper 一致性的基础： Ensemble size(E)：Set of Bookies across which a ledger is striped； Write Quorum Size（Qw）：Number of replica； Ack Quorum Size（Qa）：Number of responses needed before client’s write is satisfied。 示意图如下 Bookkeeper 读写分离 bookeeper 采取了读写分离的设计 下图描述了读写分离是如何实现的 写入（writes）、末尾读（trailing reads）和中间读（catch-up reads）这三种常见的 I/O 操作都被隔离到了三种物理上不同的 I/O 子系统中。 所有写入都被顺序地追加到磁盘上的日志文件，再批量提交到硬盘上。 在写操作持久化到磁盘上之后，它们就会放到一个 Memtable 中，再向客户端发回响应。Memtable 中的数据会被异步刷新到交叉存取的索引数据结构中：记录被追加到日志文件中，偏移量则在 ledger 的索引文件中根据记录 ID 索引起来。 最新的数据肯定在 Memtable 中，供末尾读操作使用。中间读会从记录日志文件中获取数据。由于物理隔离的存在，Bookie 节点可以充分利用网络流入带宽和磁盘的顺序写入特性来满足写请求，以及利用网络流出代宽和多个磁盘共同提供的IOPS处理能力来满足读请求，彼此之间不会相互干扰。 我们用来自于 streaml.io 的一张图来总结 Kafka 和 Pulsar 在存储结构的上的区别 ","date":"2020-12-26","objectID":"/kakfa-pulsar-storage/:2:1","tags":["kafka","message","middleware"],"title":"Kafka 和 Pulsar 存储结构对比","uri":"/kakfa-pulsar-storage/"},{"categories":["backend","infra"],"content":"总结 在 pulsar准确说是 bookkeeper 以相较于 kafka 更复杂的存储设计换来了更强的扩展性 从集群扩展性来说 kafka 增加 broker需要重新分配分区，以使得整个集群负载均衡。需要严密观察每个分区的状态，保证迁移操作不会打满网络带宽和磁盘 IO; pulsar 增加 broker 或 bookie 不需要做数据重新分布，新的分区会自动分配到新扩容的 bookie 上 从读写扩展性来说 kafka 没有物理 I/O 隔离，依靠文件系统缓冲； pulsar 可以通过 bookie 挂载不同磁盘实现物理 I/O 隔离，读写分离 ","date":"2020-12-26","objectID":"/kakfa-pulsar-storage/:3:0","tags":["kafka","message","middleware"],"title":"Kafka 和 Pulsar 存储结构对比","uri":"/kakfa-pulsar-storage/"},{"categories":["backend","基础"],"content":"在用 Golang 开发过程中，我们一定能在代码里很多函数或方法都会传递 context, 也会经常遇到这样的报错 context deadline exceeded。你有想过或去探究过 context 到底是什么吗，为什么会遇到上述的报错。在这里我们就分析一下 context 是什么及用途。 首选 Golang 中的 context 值得是 context.Context 接口,Golang 在 1.7 版本中引入标准库的接口。context 主要用来在 goroutine 之间传递上下文信息，包括：取消信号、截止时间、key-value 等。 ","date":"2020-11-28","objectID":"/golang-context/:0:0","tags":["go","context"],"title":"Golang Context 探究","uri":"/golang-context/"},{"categories":["backend","基础"],"content":"Context 定义 Context 接口定义如下 type Context interface { // 返回 context.Context的截止时间（也就是取消时间） Deadline() (deadline time.Time, ok bool) // 返回一个 Channel，这个 Channel 会在当前操作完成或者上下文被取消之后关闭，多次调用 Done 方法会返回同一个 Channel Done() \u003c-chan struct{} // 返回 context.Context 结束的原因 Err() error // 从 context.Context 中获取键对应的值, 该方法可以用来传递请求特定的数据 Value(key interface{}) interface{} } ","date":"2020-11-28","objectID":"/golang-context/:0:1","tags":["go","context"],"title":"Golang Context 探究","uri":"/golang-context/"},{"categories":["backend","基础"],"content":"Context 实现 首先我们思考为什么Golang 中需要 Context。用我们最熟悉的例子 http server 来解释, Go 服务每次都是启动一个新的 goroutine 来处理每一个请求，在这个请求的操作往往会启动新 goroutine 访问数据库或其他服务，如果此时客户端断开了连接，后续的消耗资源的操作就完全没必要了。所以我们需要一个在 goroutine 之间同步取消信号，截止时间的机制或者特定请求数据。而 Context 被设计出来就是帮我们实现这个目标。 |-----------| |-----------| |-----------| | goroutine |-------\u003e | goroutine |-------\u003e | goroutine | |-----------| |-----------| |-----------| 默认 Context context 包中最常用的方法还是 context.Background、context.TODO，这两个方法都会返回预先初始化好的私有变量 background 和 todo。这两个私有变量都是通过 new(emptyCtx) 语句初始化的，它们是指向私有结构体 context.emptyCtx 的指针。 func Background() Context { return background } func TODO() Context { return todo } 通过下面代码可以看出 emptyCtx 就是 Context 的实现，只不过没有实际功能。 我们一般用 context.Background 作为根 context; 在不确定使用哪种 context 时用 context.Todo 一般起到占位的作用 type emptyCtx int func (*emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (*emptyCtx) Done() \u003c-chan struct{} { return nil } func (*emptyCtx) Err() error { return nil } func (*emptyCtx) Value(key interface{}) interface{} { return nil } 取消的实现 如果 context 实现了下面的这个接口，则是能取消的 context。源码中有两个类型实现了 canceler 接口：*cancelCtx 和 *timerCtx type canceler interface { cancel(removeFromParent bool, err error) Done() \u003c-chan struct{} } context.cancelCtx 首先函数 WithCancel 就是我们用来创建可以取消的 context(context.cancelCtx) 的入口。 func WithCancel(parent Context) (ctx Context, cancel CancelFunc) { // 将传入的上下文包装成私有结构体 context.cancelCtx c := newCancelCtx(parent) // 构建父子上下文之间的关联，当父上下文被取消时，子上下文也会被取消 propagateCancel(parent, \u0026c) return \u0026c, func() { c.cancel(true, Canceled) } } 我们重点看 propagateCancel 的实现 func propagateCancel(parent Context, child canceler) { done := parent.Done() // 父 context 是 emptyCtx if done == nil { return // parent is never canceled } select { case \u003c-done: // parent is already canceled child.cancel(false, parent.Err()) return default: } // 找到可以取消的父 context if p, ok := parentCancelCtx(parent); ok { p.mu.Lock() if p.err != nil { // 父节点已经被取消了，子节点也要取消 child.cancel(false, p.err) } else { // 父节点未取消 if p.children == nil { p.children = make(map[canceler]struct{}) } // 与父 context 关联 p.children[child] = struct{}{} } p.mu.Unlock() } else { // 如果没有找到可取消的父 context，新启动一个协程监控父节点或子节点取消信号 atomic.AddInt32(\u0026goroutines, +1) go func() { select { case \u003c-parent.Done(): child.cancel(false, parent.Err()) case \u003c-child.Done(): } }() } context.propagateCancel 的作用是把子 context 和父 context 关联起来，保证在 parent 被取消时，child 也会收到对应的信号，达到父 context 取消，子 context 也能同时取消的目的。 下面重点看一下 context.cancelCtx 的 cancel 方法 func (c *cancelCtx) cancel(removeFromParent bool, err error) { if err == nil { panic(\"context: internal error: missing cancel error\") } c.mu.Lock() if c.err != nil { c.mu.Unlock() return // 已经被其他协程取消 } c.err = err // 关闭 channel，通知其他协程 if c.done == nil { c.done = closedchan } else { close(c.done) } // 遍历它的所有子节点 for child := range c.children { // 递归地取消所有子节点 child.cancel(false, err) } // 将子节点置空 c.children = nil c.mu.Unlock() if removeFromParent { // 从父节点中移除自己 removeChild(c.Context, c) } } context.timerCtx timerCtx 基于 cancelCtx，只是多了一个 time.Timer 和一个 deadline。Timer 会在 deadline 到来时，自动取消 context。 type timerCtx struct { cancelCtx timer *time.Timer // Under cancelCtx.mu. deadline time.Time } 创建 timerCtx 的函数 func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) { return WithDeadline(parent, time.Now().Add(timeout)) } func WithDeadline(parent Context, d time.Time) (Context, CancelFunc) { // 如果父节点 context 的 deadline 早于指定时间。直接构建一个可取消的 context。 // 原因是一旦父节点超时，自动调用 cancel 函数，子节点也会随之取消。 // 所以不用单独处理子节点的计时器时间到了之后，自动调用 cancel 函数 if cur, ok := parent.Deadline(); ok \u0026\u0026 cur.Before(d) { // The current deadline is already sooner than the new one. return WithCancel(parent) } // 创建 timerCtx c := \u0026timerCtx{ cancelCtx: newCancelCtx(parent), deadline: d, } // 与父节点关联 propagateCancel(parent, c) // 计算当前距离 deadline 的时间 dur := time.Until(d) // 直接取消 if dur \u003c= 0 { c.cancel(true, DeadlineExceeded) // deadline has already passed return c, func() { c.cancel(false, Canceled) } } c.mu.Lock() defer c.mu.Unlock() if c.err == nil { //dur 后 timer 会自动","date":"2020-11-28","objectID":"/golang-context/:0:2","tags":["go","context"],"title":"Golang Context 探究","uri":"/golang-context/"},{"categories":["backend","基础"],"content":"Context 使用 context 超时取消 这里我们以一个 Http 服务接口中包含访问其他服务的操作来举例 func main() { http.HandleFunc(\"/test\", testHandler) err := http.ListenAndServe(\"0.0.0.0:8000\", nil) if err != nil { log.Println(err) } } func testHandler(w http.ResponseWriter, r *http.Request) { // 初始化一个超时 context ctx, cancel := context.WithTimeout(context.Background(), 10*time.Millisecond) defer cancel() getMessage(ctx) } func getMessage(ctx context.Context) { // 新建带 context 超时取消的 http 请求实例 req, _ := http.NewRequestWithContext(ctx, http.MethodGet, \"http://127.0.0.1:8008/test\", nil) client := \u0026http.Client{} res, err := client.Do(req) if res != nil { fmt.Printf(\"res: %v\", res.StatusCode) defer res.Body.Close() } if err != nil { fmt.Printf(\"%v\", err) } } 请求服务不超时则 console 输出 res: 200 请求服务超时则 console 输出 Get \"http://127.0.0.1:8008/test\": context deadline exceeded context 传递 trace // 初始化传递 trace 的 context func NewContext(ctx context.Context, t Trace) context.Context { return context.WithValue(ctx, _ctxkey, t) } // 初始化一个 trace 实例 func ServerTrace(ctx context.Context, operationName string) context.Context { t := New(operationName) defer t.Finish(nil) t.SetTitle(operationName) t.SetTag(String(TagSpanKind, \"server\")) return NewContext(ctx, t) } // 从 context 获取 trace func FromContext(ctx context.Context) (t Trace, ok bool) { t, ok = ctx.Value(_ctxkey).(Trace) return } ","date":"2020-11-28","objectID":"/golang-context/:0:3","tags":["go","context"],"title":"Golang Context 探究","uri":"/golang-context/"},{"categories":["backend","infra"],"content":"在 docker 中使用 nvidia gpu 需要的基础环境","date":"2020-10-24","objectID":"/docker-nvidia/","tags":["docker","nvidia","gpu"],"title":"容器中使用 GPU 的基础环境搭建","uri":"/docker-nvidia/"},{"categories":["backend","infra"],"content":"在 Linux 服务器上使用 GPU 跑深度学习的模型很正常不过。如果我们想用 docker 实现同样的需求，就需要做些额外的工作。本质上就是我们要在容器里能看到并且使用宿主机上的显卡。 在这篇文章里我们就介绍一下 docker 使用 GPU 的环境搭建。 ","date":"2020-10-24","objectID":"/docker-nvidia/:0:0","tags":["docker","nvidia","gpu"],"title":"容器中使用 GPU 的基础环境搭建","uri":"/docker-nvidia/"},{"categories":["backend","infra"],"content":"Nvidia 驱动 某些命令以 Ubuntu 作为示例。 首先宿主机上必现安装 Nvidia 驱动。这里推荐从 Nvidia 官网下载脚本安装，安装和卸载都比较方便并且适用于任何 Linux 发行版，包括 CentOS，Ubuntu 等。 NVIDIA Telsa GPU 的 Linux 驱动在安装过程中需要编译 kernel module，系统需提前安装 gcc 和编译 Linux Kernel Module 所依赖的包，例如 kernel-devel-$(uname -r) 等。 安装 gcc 和 kernel-dev(如果没有) sudo apt install gcc kernel-dev -y 访问官网下载 选择操作系统和安装包，并单击【SEARCH】搜寻驱动，选择要下载的驱动版本 ![nvdriver]https://pics.lxkaka.wang/nvdriver.png) 下载对应版本安装脚本 在宿主机上执行 wget https://www.nvidia.com/content/DriverDownload-March2009/confirmation.php?url=/tesla/450.80.02/NVIDIA-Linux-x86_64-450.80.02.run\u0026lang=us\u0026type=Tesla 安装 执行脚本安装 chmod +x NVIDIA-Linux-x86_64-450.80.02.run \u0026\u0026 ./NVIDIA-Linux-x86_64-450.80.02.run 验证 使用如下命令验证是否安装成功 nvidia-smi 如果输出类似下图则驱动安装成功。 ","date":"2020-10-24","objectID":"/docker-nvidia/:0:1","tags":["docker","nvidia","gpu"],"title":"容器中使用 GPU 的基础环境搭建","uri":"/docker-nvidia/"},{"categories":["backend","infra"],"content":"CUDA 驱动 CUDA（Compute Unified Device Architecture）是显卡厂商 NVIDIA 推出的运算平台。CUDA™ 是一种由 NVIDIA 推出的通用并行计算架构，该架构使 GPU 能够解决复杂的计算问题。它包含了 CUDA 指令集架构（ISA）以及 GPU 内部的并行计算引擎。 这里安装的方式和显卡驱动安装类似。 访问官网下载 下载对应版本如下图 配置环境变量 echo 'export PATH=/usr/local/cuda/bin:$PATH' | sudo tee /etc/profile.d/cuda.sh source /etc/profile ","date":"2020-10-24","objectID":"/docker-nvidia/:0:2","tags":["docker","nvidia","gpu"],"title":"容器中使用 GPU 的基础环境搭建","uri":"/docker-nvidia/"},{"categories":["backend","infra"],"content":"nvidia-docker2 docker 的安装这里就不展开了，具体查看官方文档非常详细。 这里我们就直接介绍安装 nvidia-docker2. 既然叫 nvidia-docker2 就有 nvidia-docker1 就是它的 1.0 版本目前已经废弃了，所以注意不要装错。 这里先简单说一下 nvidia-docker2 的原理 nvidia-docker2 的依赖由下几部分组成 libnvidia-container nvidia-container-toolkit nvidia-container-runtime nvidia-container-runtime 是在 runc 基础上多实现了 nvidia-container-runime-hook(现在叫 nvidia-container-toolkit)，该 hook 是在容器启动后（Namespace已创建完成），容器自定义命令(Entrypoint)启动前执行。当检测到 NVIDIA_VISIBLE_DEVICES 环境变量时，会调用 libnvidia-container 挂载 GPU Device 和 CUDA Driver。如果没有检测到 NVIDIA_VISIBLE_DEVICES 就会执行默认的 runc。 下面分两步安装 设置 repository 和 GPG key distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list 安装 sudo apt-get update sudo apt-get install -y nvidia-docker2 sudo systemctl restart docker 验证 执行 docker run --rm --gpus all nvidia/cuda:10.2-base nvidia-smi 如果输出跟直接在宿主机上执行 nvidia-smi一致则说明安装成功。 如果跑的深度学习模型使用的是 tensorflow 可以在容器里执行 import tensorflow as tf tf.contrib.eager.num_gpus() 如果输出了宿主机上的 Nvidia 显卡数量则模型能使用到显卡加速。 如果使用的是 pytorch 可以在容器里执行 import torch torch.cuda.is_available() 如果输出 True 证明环境也成功了，可以使用显卡。 使用示例 使用所有显卡 docker run --rm --gpus all nvidia/cuda nvidia-smi docker run --rm --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all nvidia/cuda nvidia-smi 指明使用哪几张卡 docker run --gpus '\"device=1,2\"' nvidia/cuda nvidia-smi docker run --rm --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=1,2 nvidia/cuda nvidia-smi 到这里在 docker 下使用 Nvidia 显卡加速计算的基础环境搭建就介绍完了。后续我们可以继续研究一下 k8s 下调度 GPU 的实现。 ","date":"2020-10-24","objectID":"/docker-nvidia/:0:3","tags":["docker","nvidia","gpu"],"title":"容器中使用 GPU 的基础环境搭建","uri":"/docker-nvidia/"},{"categories":["backend","service"],"content":"如何使用 promethues 监控业务指标","date":"2020-09-26","objectID":"/app-metrics/","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["backend","service"],"content":"在 Kubernetes 已经成了事实上的容器编排标准之下，微服务的部署变得非常容易。但随着微服务规模的扩大，服务治理带来的挑战也会越来越大。在这样的背景下出现了服务可观测性(observability)的概念。在分布式系统里，系统的故障可能出现在任何节点，怎么能在出了故障的时候快速定位问题和解决问题，甚至是在故障出现之前就能感知到服务系统的异常，把故障扼杀在摇篮里。这就是可观测性的意义所在。 ","date":"2020-09-26","objectID":"/app-metrics/:0:0","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["backend","service"],"content":"可观测性 可观测性是由 logging, metrics, tracing 构建的, 简称为可观测性三支柱。 Lgging，展现的是应用运行而产生的事件或者程序在执行的过程中间产生的一些日志，可以详细解释系统的运行状态，但是存储和查询需要消耗大量的资源。所以往往使用过滤器减少数据量。 Metrics，是一种聚合数值，存储空间很小，可以观察系统的状态和趋势，但对于问题定位缺乏细节展示。这个时候使用等高线指标等多维数据结构来增强对于细节的表现力。例如统计一个服务的 TBS 的正确率、成功率、流量等，这是常见的针对单个指标或者某一个数据库的。 Tracing，面向的是请求，可以轻松分析出请求中异常点，但与 logging 有相同的问题就是资源消耗较大。通常也需要通过采样的方式减少数据量。比如一次请求的范围，也就是从浏览器或者手机端发起的任何一次调用，一个流程化的东西，我们需要轨迹去追踪。 我们在这篇文章讨论的主题就是可观测性中的 metrics。在 k8s 作为基础设施的背景下，我们知道 k8s 本身是个复杂的容器编排系统，它本身的稳定运行至关重要。与之相伴的指标监控系统 Promethues 也已经成为了云原生服务下监控体系的事实标准。相信大家对资源层面比如 CPU, Memory, Network；应用层面比如 Http 请求数，请求耗时等指标的监控都有所了解。那么业务层面的指标又怎么利用 Prometheus 去监控和告警呢？这就是这篇文章的核心内容。 以我们一个业务场景为例，在系统中有多种类型的 task 在运行，并且 task 的运行时间各异，task 本身有各种状态包括待执行、执行中、执行成功、执行失败等。如果想确保系统的稳定运行，我们必须对各个类型的 task 的运行状况s了如指掌。比如当前是否有任务挤压，失败任务是否过多，并且当超过阈值是否告警。 为了解决上述的监控告警问题，我们先得了解一下 Prometheus 的指标类型 ","date":"2020-09-26","objectID":"/app-metrics/:1:0","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["backend","service"],"content":"指标 ","date":"2020-09-26","objectID":"/app-metrics/:2:0","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["backend","service"],"content":"指标定义 在形式上，所有的指标(Metric)都通过如下格式标示： \u003cmetric name\u003e{\u003clabel name\u003e=\u003clabel value\u003e, ...} 指标的名称(metric name)可以反映被监控样本的含义（比如，http_request_total - 表示当前系统接收到的HTTP请求总量）。指标名称只能由ASCII字符、数字、下划线以及冒号组成并必须符合正则表达式[a-zA-Z_:][a-zA-Z0-9_:]。 标签(label)反映了当前样本的特征维度，通过这些维度Prometheus可以对样本数据进行过滤，聚合等。标签的名称只能由ASCII字符、数字以及下划线组成并满足正则表达式[a-zA-Z_][a-zA-Z0-9_]。 ","date":"2020-09-26","objectID":"/app-metrics/:2:1","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["backend","service"],"content":"指标类型 Prometheus定义了4种不同的指标类型(metric type)：Counter（计数器）、Gauge（仪表盘）、Histogram（直方图）、Summary（摘要） Counter Counter类型的指标其工作方式和计数器一样，只增不减（除非系统发生重置）。常见的监控指标，如http_requests_total，node_cpu都是Counter类型的监控指标。 一般在定义Counter类型指标的名称时推荐使用_total作为后缀。 通过 counter 指标我们可以和容易的了解某个事件产生的速率变化。 例如，通过rate()函数获取HTTP请求量的增长率： rate(http_requests_total[5m]) Gauge Gauge类型的指标侧重于反应系统的当前状态。因此这类指标的样本数据可增可减。常见指标如：node_memory_MemFree（主机当前空闲的内容大小）、node_memory_MemAvailable（可用内存大小）都是Gauge类型的监控指标。 通过Gauge指标，我们可以直接查看系统的当前状态 node_memory_MemFree Summary Summary 主用用于统计和分析样本的分布情况。比如某 Http 请求的响应时间大多数都在 100 ms 内，而个别请求的响应时间需要 5s，那么这中情况下统计指标的平均值就不能反映出真实情况。而如果通过 Summary 指标我们能立马看响应时间的9分位数，这样的指标才是有意义的。 例如 # HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 3.98e-05 go_gc_duration_seconds{quantile=\"0.25\"} 5.31e-05 go_gc_duration_seconds{quantile=\"0.5\"} 6.77e-05 go_gc_duration_seconds{quantile=\"0.75\"} 0.0001428 go_gc_duration_seconds{quantile=\"1\"} 0.0008099 go_gc_duration_seconds_sum 0.0114183 go_gc_duration_seconds_count 85 Histogram Histogram 类型的指标同样用于统计和样本分析。与 Summary 类型的指标相似之处在于 Histogram 类型的样本同样会反应当前指标的记录的总数(以_count作为后缀)以及其值的总量（以_sum作为后缀）。不同在于 Histogram 指标直接反应了在不同区间内样本的个数，区间通过标签len进行定义。同时对于Histogram的指标，可以通过 histogram_quantile() 函数计算出其值的分位数。 例如 # HELP prometheus_http_response_size_bytes Histogram of response size for HTTP requests. # TYPE prometheus_http_response_size_bytes histogram prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"100\"} 1 prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"1000\"} 1 prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"10000\"} 1 prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"100000\"} 1 prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"1e+06\"} 1 prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"+Inf\"} 1 prometheus_http_response_size_bytes_sum{handler=\"/\"} 29 prometheus_http_response_size_bytes_count{handler=\"/\"} 1 ","date":"2020-09-26","objectID":"/app-metrics/:2:2","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["backend","service"],"content":"应用指标监控 暴露指标 Prometheus 最常用的方式是通过 pull 去抓取 metrics。所以我们首先在服务通过 /metrics 接口暴露指标，这样 Promethues server 就能通过 http 请求抓取到我们的业务指标。 接口示例 server := gin.New() server.Use(middlewares.AccessLogger(), middlewares.Metric(), gin.Recovery()) server.GET(\"/health\", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"message\": \"ok\", }) }) server.GET(\"/metrics\", Monitor) func Monitor(c *gin.Context) { h := promhttp.Handler() h.ServeHTTP(c.Writer, c.Request) } 定义指标 为了方便理解，这里选取了三种类型和两种业务场景的指标 示例 var ( //HTTPReqDuration metric:http_request_duration_seconds HTTPReqDuration *prometheus.HistogramVec //HTTPReqTotal metric:http_request_total HTTPReqTotal *prometheus.CounterVec // TaskRunning metric:task_running TaskRunning *prometheus.GaugeVec ) func init() { // 监控接口请求耗时 // 指标类型是 Histogram HTTPReqDuration = prometheus.NewHistogramVec(prometheus.HistogramOpts{ Name: \"http_request_duration_seconds\", Help: \"http request latencies in seconds\", Buckets: nil, }, []string{\"method\", \"path\"}) // \"method\"、\"path\" 是 label // 监控接口请求次数 // 指标类型是 Counter HTTPReqTotal = prometheus.NewCounterVec(prometheus.CounterOpts{ Name: \"http_requests_total\", Help: \"total number of http requests\", }, []string{\"method\", \"path\", \"status\"}) // \"method\"、\"path\"、\"status\" 是 label // 监控当前在执行的 task 数量 // 监控类型是 Gauge TaskRunning = prometheus.NewGaugeVec(prometheus.GaugeOpts{ Name: \"task_running\", Help: \"current count of running task\", }, []string{\"type\", \"state\"}) // \"type\"、\"state\" 是 label prometheus.MustRegister( HTTPReqDuration, HTTPReqTotal, TaskRunning, ) } 通过上述的代码我们就定义并且注册了我们的想要监控的指标。 生成指标 示例 start := time.Now() c.Next() duration := float64(time.Since(start)) / float64(time.Second) path := c.Request.URL.Path // 请求数加1 controllers.HTTPReqTotal.With(prometheus.Labels{ \"method\": c.Request.Method, \"path\": path, \"status\": strconv.Itoa(c.Writer.Status()), }).Inc() // 记录本次请求处理时间 controllers.HTTPReqDuration.With(prometheus.Labels{ \"method\": c.Request.Method, \"path\": path, }).Observe(duration) // 模拟新建任务 controllers.TaskRunning.With(prometheus.Labels{ \"type\": shuffle([]string{\"video\", \"audio\"}), \"state\": shuffle([]string{\"process\", \"queue\"}), }).Inc() // 模拟任务完成 controllers.TaskRunning.With(prometheus.Labels{ \"type\": shuffle([]string{\"video\", \"audio\"}), \"state\": shuffle([]string{\"process\", \"queue\"}), }).Dec() 抓取指标 Promethues 抓取 target 配置 # 抓取间隔 scrape_interval: 5s # 目标 scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['prometheus:9090'] - job_name: 'local-service' metrics_path: /metrics static_configs: - targets: ['host.docker.internal:8000'] 在实际应用中静态配置 target 地址不太适用，在 k8s 下 Promethues通过与 Kubernetes API 集成目前主要支持5种服务发现模式，分别是：Node、Service、Pod、Endpoints、Ingress。 指标展示如下图 ","date":"2020-09-26","objectID":"/app-metrics/:3:0","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["monitor","backend","infra"],"content":"如何使用 promethues 监控业务指标","date":"2020-09-26","objectID":"/app-metrics/","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["monitor","backend","infra"],"content":"在 Kubernetes 已经成了事实上的容器编排标准之下，微服务的部署变得非常容易。但随着微服务规模的扩大，服务治理带来的挑战也会越来越大。在这样的背景下出现了服务可观测性(observability)的概念。在分布式系统里，系统的故障可能出现在任何节点，怎么能在出了故障的时候快速定位问题和解决问题，甚至是在故障出现之前就能感知到服务系统的异常，把故障扼杀在摇篮里。这就是可观测性的意义所在。 ","date":"2020-09-26","objectID":"/app-metrics/:0:0","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["monitor","backend","infra"],"content":"可观测性 可观测性是由 logging, metrics, tracing 构建的, 简称为可观测性三支柱。 Lgging，展现的是应用运行而产生的事件或者程序在执行的过程中间产生的一些日志，可以详细解释系统的运行状态，但是存储和查询需要消耗大量的资源。所以往往使用过滤器减少数据量。 Metrics，是一种聚合数值，存储空间很小，可以观察系统的状态和趋势，但对于问题定位缺乏细节展示。这个时候使用等高线指标等多维数据结构来增强对于细节的表现力。例如统计一个服务的 TBS 的正确率、成功率、流量等，这是常见的针对单个指标或者某一个数据库的。 Tracing，面向的是请求，可以轻松分析出请求中异常点，但与 logging 有相同的问题就是资源消耗较大。通常也需要通过采样的方式减少数据量。比如一次请求的范围，也就是从浏览器或者手机端发起的任何一次调用，一个流程化的东西，我们需要轨迹去追踪。 我们在这篇文章讨论的主题就是可观测性中的 metrics。在 k8s 作为基础设施的背景下，我们知道 k8s 本身是个复杂的容器编排系统，它本身的稳定运行至关重要。与之相伴的指标监控系统 Promethues 也已经成为了云原生服务下监控体系的事实标准。相信大家对资源层面比如 CPU, Memory, Network；应用层面比如 Http 请求数，请求耗时等指标的监控都有所了解。那么业务层面的指标又怎么利用 Prometheus 去监控和告警呢？这就是这篇文章的核心内容。 以我们一个业务场景为例，在系统中有多种类型的 task 在运行，并且 task 的运行时间各异，task 本身有各种状态包括待执行、执行中、执行成功、执行失败等。如果想确保系统的稳定运行，我们必须对各个类型的 task 的运行状况s了如指掌。比如当前是否有任务挤压，失败任务是否过多，并且当超过阈值是否告警。 为了解决上述的监控告警问题，我们先得了解一下 Prometheus 的指标类型 ","date":"2020-09-26","objectID":"/app-metrics/:1:0","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["monitor","backend","infra"],"content":"指标 ","date":"2020-09-26","objectID":"/app-metrics/:2:0","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["monitor","backend","infra"],"content":"指标定义 在形式上，所有的指标(Metric)都通过如下格式标示： \u003cmetric name\u003e{\u003clabel name\u003e=\u003clabel value\u003e, ...} 指标的名称(metric name)可以反映被监控样本的含义（比如，http_request_total - 表示当前系统接收到的HTTP请求总量）。指标名称只能由ASCII字符、数字、下划线以及冒号组成并必须符合正则表达式[a-zA-Z_:][a-zA-Z0-9_:]。 标签(label)反映了当前样本的特征维度，通过这些维度Prometheus可以对样本数据进行过滤，聚合等。标签的名称只能由ASCII字符、数字以及下划线组成并满足正则表达式[a-zA-Z_][a-zA-Z0-9_]。 ","date":"2020-09-26","objectID":"/app-metrics/:2:1","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["monitor","backend","infra"],"content":"指标类型 Prometheus定义了4种不同的指标类型(metric type)：Counter（计数器）、Gauge（仪表盘）、Histogram（直方图）、Summary（摘要） Counter Counter类型的指标其工作方式和计数器一样，只增不减（除非系统发生重置）。常见的监控指标，如http_requests_total，node_cpu都是Counter类型的监控指标。 一般在定义Counter类型指标的名称时推荐使用_total作为后缀。 通过 counter 指标我们可以和容易的了解某个事件产生的速率变化。 例如，通过rate()函数获取HTTP请求量的增长率： rate(http_requests_total[5m]) Gauge Gauge类型的指标侧重于反应系统的当前状态。因此这类指标的样本数据可增可减。常见指标如：node_memory_MemFree（主机当前空闲的内容大小）、node_memory_MemAvailable（可用内存大小）都是Gauge类型的监控指标。 通过Gauge指标，我们可以直接查看系统的当前状态 node_memory_MemFree Summary Summary 主用用于统计和分析样本的分布情况。比如某 Http 请求的响应时间大多数都在 100 ms 内，而个别请求的响应时间需要 5s，那么这中情况下统计指标的平均值就不能反映出真实情况。而如果通过 Summary 指标我们能立马看响应时间的9分位数，这样的指标才是有意义的。 例如 # HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles. # TYPE go_gc_duration_seconds summary go_gc_duration_seconds{quantile=\"0\"} 3.98e-05 go_gc_duration_seconds{quantile=\"0.25\"} 5.31e-05 go_gc_duration_seconds{quantile=\"0.5\"} 6.77e-05 go_gc_duration_seconds{quantile=\"0.75\"} 0.0001428 go_gc_duration_seconds{quantile=\"1\"} 0.0008099 go_gc_duration_seconds_sum 0.0114183 go_gc_duration_seconds_count 85 Histogram Histogram 类型的指标同样用于统计和样本分析。与 Summary 类型的指标相似之处在于 Histogram 类型的样本同样会反应当前指标的记录的总数(以_count作为后缀)以及其值的总量（以_sum作为后缀）。不同在于 Histogram 指标直接反应了在不同区间内样本的个数，区间通过标签len进行定义。同时对于Histogram的指标，可以通过 histogram_quantile() 函数计算出其值的分位数。 例如 # HELP prometheus_http_response_size_bytes Histogram of response size for HTTP requests. # TYPE prometheus_http_response_size_bytes histogram prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"100\"} 1 prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"1000\"} 1 prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"10000\"} 1 prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"100000\"} 1 prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"1e+06\"} 1 prometheus_http_response_size_bytes_bucket{handler=\"/\",le=\"+Inf\"} 1 prometheus_http_response_size_bytes_sum{handler=\"/\"} 29 prometheus_http_response_size_bytes_count{handler=\"/\"} 1 ","date":"2020-09-26","objectID":"/app-metrics/:2:2","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["monitor","backend","infra"],"content":"应用指标监控 暴露指标 Prometheus 最常用的方式是通过 pull 去抓取 metrics。所以我们首先在服务通过 /metrics 接口暴露指标，这样 Promethues server 就能通过 http 请求抓取到我们的业务指标。 接口示例 server := gin.New() server.Use(middlewares.AccessLogger(), middlewares.Metric(), gin.Recovery()) server.GET(\"/health\", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"message\": \"ok\", }) }) server.GET(\"/metrics\", Monitor) func Monitor(c *gin.Context) { h := promhttp.Handler() h.ServeHTTP(c.Writer, c.Request) } 定义指标 为了方便理解，这里选取了三种类型和两种业务场景的指标 示例 var ( //HTTPReqDuration metric:http_request_duration_seconds HTTPReqDuration *prometheus.HistogramVec //HTTPReqTotal metric:http_request_total HTTPReqTotal *prometheus.CounterVec // TaskRunning metric:task_running TaskRunning *prometheus.GaugeVec ) func init() { // 监控接口请求耗时 // 指标类型是 Histogram HTTPReqDuration = prometheus.NewHistogramVec(prometheus.HistogramOpts{ Name: \"http_request_duration_seconds\", Help: \"http request latencies in seconds\", Buckets: nil, }, []string{\"method\", \"path\"}) // \"method\"、\"path\" 是 label // 监控接口请求次数 // 指标类型是 Counter HTTPReqTotal = prometheus.NewCounterVec(prometheus.CounterOpts{ Name: \"http_requests_total\", Help: \"total number of http requests\", }, []string{\"method\", \"path\", \"status\"}) // \"method\"、\"path\"、\"status\" 是 label // 监控当前在执行的 task 数量 // 监控类型是 Gauge TaskRunning = prometheus.NewGaugeVec(prometheus.GaugeOpts{ Name: \"task_running\", Help: \"current count of running task\", }, []string{\"type\", \"state\"}) // \"type\"、\"state\" 是 label prometheus.MustRegister( HTTPReqDuration, HTTPReqTotal, TaskRunning, ) } 通过上述的代码我们就定义并且注册了我们的想要监控的指标。 生成指标 示例 start := time.Now() c.Next() duration := float64(time.Since(start)) / float64(time.Second) path := c.Request.URL.Path // 请求数加1 controllers.HTTPReqTotal.With(prometheus.Labels{ \"method\": c.Request.Method, \"path\": path, \"status\": strconv.Itoa(c.Writer.Status()), }).Inc() // 记录本次请求处理时间 controllers.HTTPReqDuration.With(prometheus.Labels{ \"method\": c.Request.Method, \"path\": path, }).Observe(duration) // 模拟新建任务 controllers.TaskRunning.With(prometheus.Labels{ \"type\": shuffle([]string{\"video\", \"audio\"}), \"state\": shuffle([]string{\"process\", \"queue\"}), }).Inc() // 模拟任务完成 controllers.TaskRunning.With(prometheus.Labels{ \"type\": shuffle([]string{\"video\", \"audio\"}), \"state\": shuffle([]string{\"process\", \"queue\"}), }).Dec() 抓取指标 Promethues 抓取 target 配置 # 抓取间隔 scrape_interval: 5s # 目标 scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['prometheus:9090'] - job_name: 'local-service' metrics_path: /metrics static_configs: - targets: ['host.docker.internal:8000'] 在实际应用中静态配置 target 地址不太适用，在 k8s 下 Promethues通过与 Kubernetes API 集成目前主要支持5种服务发现模式，分别是：Node、Service、Pod、Endpoints、Ingress。 指标展示如下图 ","date":"2020-09-26","objectID":"/app-metrics/:3:0","tags":["metrics","监控","prometheus"],"title":"Prometheus 监控业务指标","uri":"/app-metrics/"},{"categories":["backend","service"],"content":"介绍 Python 服务和 Go 服务之间怎么使用 gRPC 通信","date":"2020-08-22","objectID":"/python-grpc/","tags":["gRPC","Python","Go"],"title":"Python 和 Go 之间的 gRPC 交互","uri":"/python-grpc/"},{"categories":["backend","service"],"content":"因为不同的业务场景，异构的业务系统十分常见。异构服务之间的同步通信方式的选择不外乎 HTTP 和 RPC 两种。RPC 方式可以像调用本地方法一样调用远程服务提供的方法，对客户端来说具体实现完全是透明的，服务之间的通信会变得更容易。 对于 RPC 框架的选择，gRPC 当前已经是首选。 ","date":"2020-08-22","objectID":"/python-grpc/:0:0","tags":["gRPC","Python","Go"],"title":"Python 和 Go 之间的 gRPC 交互","uri":"/python-grpc/"},{"categories":["backend","service"],"content":"gRPC 简介 gRPC是一个高性能、通用的开源RPC框架，其由Google主要面向移动应用开发并基于HTTP/2协议标准而设计，基于ProtoBuf(Protocol Buffers)序列化协议开发，且支持众多开发语言。 gRPC具有以下重要特征： 强大的IDL特性 RPC使用ProtoBuf来定义服务，ProtoBuf是由Google开发的一种数据序列化协议，性能出众，得到了广泛的应用。 支持多种语言 支持 C++、Java、Go、Python、Ruby、C#、Node.js、Android Java、Objective-C、PHP等编程语言。 基于 HTTP/2 标准设计 下面这张图来自于官方网站清晰的给我们展示了使用 gRPC 服务之间的交互流程 gRPC使用流程 定义标准的proto文件 生成标准代码 服务端使用生成的代码提供服务 客户端使用生成的代码调用服务 在这里我们以我们实际业务场景 Python 服务和 Go 服务之间的交互来介绍一下 gRPC 的使用。 ","date":"2020-08-22","objectID":"/python-grpc/:1:0","tags":["gRPC","Python","Go"],"title":"Python 和 Go 之间的 gRPC 交互","uri":"/python-grpc/"},{"categories":["backend","service"],"content":"Python gRPC 1. python 环境 这里可以使用 virtualenv 来初始化一个干净的 Python 环境 pip3 install virtualenv # 使用 python3.7 创建虚拟环境 virtualenv --python=python3.7 venv source venv/bin/activate 2. gRPC 依赖 # grpcio 是启动 gRPC 服务的项目依赖 pip install grpcio # gPRC tools 包含 protocol buffer 编译器和用于从 .proto 文件生成服务端和客户端代码的插件 pip install grpcio-tools 3. 定义 proto 文件 syntax = \"proto3\"; import \"google/protobuf/empty.proto\"; // service 关键字定义提供的服务 service MyService { // 定义一个探活方法 rpc Health (.google.protobuf.Empty) returns (.google.protobuf.Empty){ } // 定义一个批量查询 user 的方法 rpc User (UserReq) returns (UserReply){ } } // message 关键字定义交互的数据结构 message UserReq { repeated int32 userIDs= 1; } message UserReply { string message = 1; // repeated 定义一个数组 repeated User data = 2; } message User { string name = 1; int32 age = 2; string email = 3; } 4. 生成代码 # 使用 protoc 和相应的插件可以编译生成对应语言的代码 # -I 指定 import 路径，可以指定多个 -I 参数，编译时按顺序查找，不指定默认当前目录 python -m grpc_tools.protoc -I ./ --python_out=. --grpc_python_out=. ./api.proto 经过上述步骤，我们生成了这样两个文件 api_pb2.py 此文件包含每个 message 生成一个含有静态描述符的模块，，该模块与一个元类（metaclass）在运行时（runtime）被用来创建所需的Python数据访问类 api_pb2_grpc.py 此文件包含生成的 客户端(MyServiceStub)和服务端 (MyServiceServicer)的类。 5. 实现服务端 #!/usr/bin/env python3 # -*- coding: utf-8 -*- import logging from concurrent import futures import grpc from api import api_pb2_grpc, api_pb2 from api.api_pb2_grpc import MyServiceServicer from service import get_users class Service(MyServiceServicer): def Health(self, request, context): return def User(self, request, context): print('start to process request...') res = get_users(request.userIDs) users = [] for u in res: users.append(api_pb2.User(name=u['name'], age=u['age'], email=u['email'])) return api_pb2.UserReply(message='success', data=users) def serve(): print('start grpc server====\u003e') server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) api_pb2_grpc.add_MyServiceServicer_to_server(Service(), server) server.add_insecure_port('[::]:50051') server.start() server.wait_for_termination() if __name__ == '__main__': logging.basicConfig() serve() 到此，如果我们用 Python 客户端当然也能请求到服务端。因为我们这里介绍的是 Go 和 Python 的交互，这里就不 demo 了。 ","date":"2020-08-22","objectID":"/python-grpc/:2:0","tags":["gRPC","Python","Go"],"title":"Python 和 Go 之间的 gRPC 交互","uri":"/python-grpc/"},{"categories":["backend","service"],"content":"Go gRPC 我们这里 Go 服务作为客户端调用 Python 服务，同样需要根据 proto 文件生成代码，进而创建客户端发起 RPC。 1. go gRPC 依赖 # 安装 ptotobuf, 推荐使用 brew brew install protobuf # protoc go 插件安装 go get -u github.com/golang/protobuf/protoc-gen-go # 这里安装在 GOPATH 下的 bin 目录，所以保证这个目录在 $PATH 中 export PATH=\"$PATH:$(go env GOPATH)/bin\" # 代码 gprc 依赖安装 go get -u google.golang.org/grpc 2. 生成 Go pb 代码 protoc -I ./ --go_out=plugins=grpc:./ api.proto 3. 客户端调用 package main import ( \"context\" \"fmt\" \"log\" \"time\" \"ginDemo/api\" \"google.golang.org/grpc\" ) const ( address = \"localhost:50051\" defaultName = \"world\" ) func main() { conn, err := grpc.Dial(address, grpc.WithInsecure(), grpc.WithBlock()) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() c := api.NewMyServiceClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.User(ctx, \u0026api.UserReq{UserIDs: []int32{1, 2}}) if err != nil { log.Fatalf(\"could not greet: %v\", err) } fmt.Printf(\"gprc result: %+v\", r.Data) } ","date":"2020-08-22","objectID":"/python-grpc/:3:0","tags":["gRPC","Python","Go"],"title":"Python 和 Go 之间的 gRPC 交互","uri":"/python-grpc/"},{"categories":["backend","service"],"content":"输出示例 Python 服务端输出示例 Go 客户端输出示例 以 Python 服务和 Go 服务之间的 gRPC 通信就说到这里，希望对想使用 gRPC 的同学带来参考和启示。 ","date":"2020-08-22","objectID":"/python-grpc/:4:0","tags":["gRPC","Python","Go"],"title":"Python 和 Go 之间的 gRPC 交互","uri":"/python-grpc/"},{"categories":["backend","service"],"content":"介绍 Python 服务和 Go 服务之间怎么使用 gRPC 通信","date":"2020-08-22","objectID":"/python-grpc/","tags":["gRPC","Python","Go"],"title":"Python 和 Go 之间的 gRPC 交互","uri":"/python-grpc/"},{"categories":["backend","service"],"content":"因为不同的业务场景，异构的业务系统十分常见。异构服务之间的同步通信方式的选择不外乎 HTTP 和 RPC 两种。RPC 方式可以像调用本地方法一样调用远程服务提供的方法，对客户端来说具体实现完全是透明的，服务之间的通信会变得更容易。 对于 RPC 框架的选择，gRPC 当前已经是首先: gRPC是一个高性能、通用的开源RPC框架，其由Google主要面向移动应用开发并基于HTTP/2协议标准而设计，基于ProtoBuf(Protocol Buffers)序列化协议开发，且支持众多开发语言。 gRPC具有以下重要特征： 强大的IDL特性 RPC使用ProtoBuf来定义服务，ProtoBuf是由Google开发的一种数据序列化协议，性能出众，得到了广泛的应用。 支持多种语言 支持 C++、Java、Go、Python、Ruby、C#、Node.js、Android Java、Objective-C、PHP等编程语言。 基于 HTTP/2 标准设计 下面这张图来自于官方网站清晰的给我们展示了使用 gRPC 服务之间的交互流程 gRPC使用流程 定义标准的proto文件 生成标准代码 服务端使用生成的代码提供服务 客户端使用生成的代码调用服务 在这里我们以我们实际业务场景 Python 服务和 Go 服务之间的交互来介绍一下 gRPC 的使用。 ","date":"2020-08-22","objectID":"/python-grpc/:0:0","tags":["gRPC","Python","Go"],"title":"Python 和 Go 之间的 gRPC 交互","uri":"/python-grpc/"},{"categories":["backend","service"],"content":"Python gRPC python 环境 这里可以使用 virtualenv 来初始化一个干净的 Python 环境 pip3 install virtualenv # 使用 python3.7 创建虚拟环境 virtualenv --python=python3.7 venv source venv/bin/activate gRPC 依赖 # grpcio 是启动 gRPC 服务的项目依赖 pip install grpcio # gPRC tools 包含 protocol buffer 编译器和用于从 .proto 文件生成服务端和客户端代码的插件 pip install grpcio-tools 定义 proto 文件 syntax = \"proto3\"; import \"google/protobuf/empty.proto\"; // service 关键字定义提供的服务 service MyService { // 定义一个探活方法 rpc Health (.google.protobuf.Empty) returns (.google.protobuf.Empty){ } // 定义一个批量查询 user 的方法 rpc User (UserReq) returns (UserReply){ } } // message 关键字定义交互的数据结构 message UserReq { repeated int32 userIDs= 1; } message UserReply { string message = 1; // repeated 定义一个数组 repeated User data = 2; } message User { string name = 1; int32 age = 2; string email = 3; } 生成代码 # 使用 protoc 和相应的插件可以编译生成对应语言的代码 # -I 指定 import 路径，可以指定多个 -I 参数，编译时按顺序查找，不指定默认当前目录 python -m grpc_tools.protoc -I ./ --python_out=. --grpc_python_out=. ./api.proto 经过上述步骤，我们生成了这样两个文件 api_pb2.py 此文件包含每个 message 生成一个含有静态描述符的模块，，该模块与一个元类（metaclass）在运行时（runtime）被用来创建所需的Python数据访问类 api_pb2_grpc.py 此文件包含生成的 客户端(MyServiceStub)和服务端 (MyServiceServicer)的类。 实现服务端 #!/usr/bin/env python3 # -*- coding: utf-8 -*- import logging from concurrent import futures import grpc from api import api_pb2_grpc, api_pb2 from api.api_pb2_grpc import MyServiceServicer from service import get_users class Service(MyServiceServicer): def Health(self, request, context): return def User(self, request, context): print('start to process request...') res = get_users(request.userIDs) users = [] for u in res: users.append(api_pb2.User(name=u['name'], age=u['age'], email=u['email'])) return api_pb2.UserReply(message='success', data=users) def serve(): print('start grpc server====\u003e') server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) api_pb2_grpc.add_MyServiceServicer_to_server(Service(), server) server.add_insecure_port('[::]:50051') server.start() server.wait_for_termination() if __name__ == '__main__': logging.basicConfig() serve() 到此，如果我们用 Python 客户端当然也能请求到服务端。因为我们这里介绍的是 Go 和 Python 的交互，这里就不 demo 了。 ","date":"2020-08-22","objectID":"/python-grpc/:1:0","tags":["gRPC","Python","Go"],"title":"Python 和 Go 之间的 gRPC 交互","uri":"/python-grpc/"},{"categories":["backend","service"],"content":"Go gRPC 我们这里 Go 服务作为客户端调用 Python 服务，同样需要根据 proto 文件生成代码，进而创建客户端发起 RPC。 go gRPC 依赖 # 安装 ptotobuf, 推荐使用 brew brew install protobuf # protoc go 插件安装 go get -u github.com/golang/protobuf/protoc-gen-go # 这里安装在 GOPATH 下的 bin 目录，所以保证这个目录在 $PATH 中 export PATH=\"$PATH:$(go env GOPATH)/bin\" # 代码 gprc 依赖安装 go get -u google.golang.org/grpc 生成 Go pb 代码 protoc -I ./ --go_out=plugins=grpc:./ api.proto 客户端调用 package main import ( \"context\" \"fmt\" \"log\" \"time\" \"ginDemo/api\" \"google.golang.org/grpc\" ) const ( address = \"localhost:50051\" defaultName = \"world\" ) func main() { conn, err := grpc.Dial(address, grpc.WithInsecure(), grpc.WithBlock()) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() c := api.NewMyServiceClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.User(ctx, \u0026api.UserReq{UserIDs: []int32{1, 2}}) if err != nil { log.Fatalf(\"could not greet: %v\", err) } fmt.Printf(\"gprc result: %+v\", r.Data) } Python 服务端输出示例 Go 客户端输出示例 以 Python 服务和 Go 服务之间的 gRPC 通信就说到这里，希望对想使用 gRPC 的同学带来参考和启示。 ","date":"2020-08-22","objectID":"/python-grpc/:2:0","tags":["gRPC","Python","Go"],"title":"Python 和 Go 之间的 gRPC 交互","uri":"/python-grpc/"},{"categories":null,"content":"如果大家有想把博客 Hexo 迁移到 Hugo 的可以参考此文。 Hugo 的优势： 不需要额外的依赖，一个二进制文件 Hugo 就行，这样的话维护成本会降低 编译生成静态文件非常快 ","date":"2020-07-24","objectID":"/hexo2hugo/:0:0","tags":["hugo","hexo"],"title":"Hexo 迁移 Hugo","uri":"/hexo2hugo/"},{"categories":null,"content":"安装 Hugo Hugo 的安装非常简单，mac 我并不推荐使用 brew 安装，实在太慢，即使更换了代理源速度也不快。这里我推荐直接下载二进制文件，简单又快捷。 下载地址 https://github.com/gohugoio/hugo/releases/download/v0.71.1/hugo_extended_0.71.1_macOS-64bit.tar.gz 版本可以自由选择，但推荐下载 extended version, 有些主题需要 extended version 的支持 解压得到二进制文件 tar xzvf hugo_extended_0.71.1_macOS-64bit.tar.gz 创建一个 bin 目录，把二进制执行文件 hugo 放到 bin 目录 把创建的 bin 目录添加到环境变量 PATH 中,例如 export PATH=$PATH:/Users/lxkaka/bin 重新打开 terminal 或者 source .zshrc, 让 PATH 生效 验证安装 hugo version ","date":"2020-07-24","objectID":"/hexo2hugo/:1:0","tags":["hugo","hexo"],"title":"Hexo 迁移 Hugo","uri":"/hexo2hugo/"},{"categories":null,"content":"配置主题 Hugo 支持的主题也非常多，大家可以去 Hugo theme 网站上看到很多主题，挑选自己喜欢的。 新建站点 hugo new site mySite cd mySite 看到由以下文件和目录组成 drwxr-xr-x 3 lxkaka staff 96 7 24 16:20 archetypes -rw-r--r-- 1 lxkaka staff 82 7 24 16:20 config.toml drwxr-xr-x 2 lxkaka staff 64 7 24 16:20 content drwxr-xr-x 2 lxkaka staff 64 7 24 16:20 data drwxr-xr-x 2 lxkaka staff 64 7 24 16:20 layouts drwxr-xr-x 2 lxkaka staff 64 7 24 16:20 static drwxr-xr-x 2 lxkaka staff 64 7 24 16:20 themes 进入的 themes 目录 clone 自己喜欢的主题 复制 clone 的主题目录 exampleSite 下的配置文件夹 config.toml 到站点的的根目录。例如 cp themes/color-your-world/exampleSite/config.toml . 按照需求修改 config.toml ","date":"2020-07-24","objectID":"/hexo2hugo/:2:0","tags":["hugo","hexo"],"title":"Hexo 迁移 Hugo","uri":"/hexo2hugo/"},{"categories":null,"content":"迁移 Hexo 和 Hugo 对 markdown 文件中 Front Matter 的格式定义不同。需要修改所有的文章来适配 Hugo 的 Front matter。 这里推荐一个脚本 https://github.com/wd/hexo2hugo/blob/master/hexo2hugo.py 安装依赖 python3 -m pip install pyyaml python3 -m pip install pytoml 转换文件格式 # src 是原来所有文章的目录；dest 是转换后要放文章的目录 python3 hexotohugo.py --src=~/mySpace/source/_posts/ --dest=./posts/converted --remove-date-from-name --verbose # 转换前文章 front matter 如下 title: 基于 CAS 实现 SSO date: 2019-04-19 16:24:31 tags: - cas - sso --- # 转换后文章 front matter 如下 +++ title = \"基于 CAS 实现 SSO\" date = \"2019-04-19T16:24:31+08:00\" tags = [\"cas\", \"sso\"] description = \"\" +++ url 格式 在 Hexo 一般 url 的格式配置成这样 permalink: :year/:month/:day/:title/, 为了不让以前的 url 实效，我们在 Hugo 的配置文件 config.toml 需要配置如下 [Permalinks] posts = \"/:year/:month/:day/:filename/\" ","date":"2020-07-24","objectID":"/hexo2hugo/:3:0","tags":["hugo","hexo"],"title":"Hexo 迁移 Hugo","uri":"/hexo2hugo/"},{"categories":null,"content":"deploy Hugo 的命令行没有直接支持 deploy, 这里我们用简单脚本可以来代替。 在站点目录下执行 hugo 在 public 目录下生成我们部署所需的全部静态文件，我们只需要把 public 下的静态文件通过 rsync 上传到服务器即可。 这个过程用脚本描述如下 #!/bin/sh USER=my-user HOST=my-server.com DIR=my/directory/to/topologix.fr/ # the directory where your web site files should go hugo \u0026\u0026 rsync -avz --delete public/ ${USER}@${HOST}:~/${DIR} exit 0 执行 bash deploy.sh则完成了部署。 服务器上 Nginx 示例配置如下 server { root /home/lxkaka/hugo; location = / { root /home/lxkaka/hugo; index index.html index.htm; } ... } 至此，迁移 Hugo 的主要过程就是这样，如果大家遇到啥问题欢迎交流。 ","date":"2020-07-24","objectID":"/hexo2hugo/:4:0","tags":["hugo","hexo"],"title":"Hexo 迁移 Hugo","uri":"/hexo2hugo/"},{"categories":null,"content":"教育经历 本科 西安电子科技大学-通信工程 研究生 德累斯顿工业大学-Informationstechnik ","date":"2020-07-22","objectID":"/about/:1:0","tags":null,"title":"Me","uri":"/about/"},{"categories":null,"content":"技能加点 熟悉主流后端技术栈，包括 Web 框架; MySQL; Redis; Kafka 等 从应用设计到发布上线一把梭 对 Cloud Native 下的技术比较感兴趣，有较多的实践经验 后端架构设计有一定的积累 ","date":"2020-07-22","objectID":"/about/:2:0","tags":null,"title":"Me","uri":"/about/"},{"categories":null,"content":"爱好 dota 首当其冲 多年体育运动(篮球，足球)云玩家 电影，音乐是消遣的常备项目 ","date":"2020-07-22","objectID":"/about/:3:0","tags":null,"title":"Me","uri":"/about/"},{"categories":null,"content":"GitOps 相信不少人听说过，这种新的交付模式越来越受到大家的关注，包括我自己在内。今天在这篇文章里我结合我们的实际生产上遇到的痛点，讲讲我们在 GitOps 上的实践，希望能给大家带来一些参考作用。 首先关于 GitOps 的介绍资源还是非常多的，在这里我引用一下大佬们总结的 GitOps 概念。 ","date":"2020-07-20","objectID":"/gitops-argocd/:0:0","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"GitOps 概念 GitOps 的概念最初来源于 Weaveworks 的联合创始人 Alexis 在 2017 年 8 月发表的一篇博客 GitOps - Operations by Pull Request。文章介绍了 Weaveworks 的工程师如何以 Git 作为事实的唯一真实来源，部署、管理和监控基于 Kubernetes 的 SaaS 应用。 GitOps 是一种持续交付的方式。它的核心思想是将应用系统的声明性基础架构和应用程序存放在 Git 版本库中。将 Git 作为交付流水线的核心，每个开发人员都可以提交拉取请求（Pull Request）并使用Gi​​t来加速和简化 Kubernetes的应用程序部署和运维任务。通过使用像Git这样的简单工具，开发人员可以更高效地将注意力集中在创建新功能而不是运维相关任务上（例如，应用系统安装、配置、迁移等）。 ","date":"2020-07-20","objectID":"/gitops-argocd/:1:0","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"GitOps 基本原则 以 Git 作为事实的唯一真实来源 描述系统相关的所有内容：策略，代码，配置和版本控制等，并且将这些内容全部存储在 Git 仓库中，在通过版本库中的内容构建系统的基础架构或者应用程序的时候，如果没有成功，则可以迅速的回滚，并且重新来过。 用 pull 的模式构建 Pipeline 不断地检查 Git 仓库，将任何状态变化都反映到 Kubernetes 集群中，具体说来就是 GitOps 检测到集群状态和 Git 仓库不一致，会从配置仓库中拉取更新后的清单，并将包含新功能的镜像部署到集群里。 关于 GitOps 就简要介绍一下，更多的信息大家可以去关注一下。 ","date":"2020-07-20","objectID":"/gitops-argocd/:1:1","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"工具链 因为我们讲的是 GitOps 的实践，所以依赖的工具或者说组件必须先介绍一下 Git 仓库 我们使用 GitLab 来管理和存储我们的代码和各种配置 Kubernetes 集群 用于部署我们应用程序的底层集群 Kustomize kubernetes 配置管理 ArgoCD ArgoCD 是一个声明式、GitOps 持续交付的 Kubernetes 工具，它的配置和使用分非常简单，并且自带一个简单易用的 Dashboard，更重要的是 ArgoCD 支持 kustomzie、helm、ksonnet 等多种工具 ","date":"2020-07-20","objectID":"/gitops-argocd/:2:0","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"流程 下图展示了我们实践的 GitOps 工作流程 提交代码到对应的 feature branch 代码和并到 dev branch or master branch 触发 GitLab CI 在 CI 中更新应用配置文件 提交新的配置到 GitLab 仓库 ArgoCD 监控到 GitLab 仓库变化，触发同步，拉取镜像发布新的应用版本 ","date":"2020-07-20","objectID":"/gitops-argocd/:3:0","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"安装 ArgoCD ArgoCD 的安装比较简单，我们可以选择用 helm 安装，或者直接 Kubectl apply 配置文件。 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 这里强调一下 ingress 的配置, 转发到 443 端口（80 会被 redirect 到443，造成浏览器多次 redirect） apiVersion: extensions/v1beta1 kind: Ingress metadata: name: argocd-server-http-ingress namespace: argocd annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" spec: rules: - http: paths: - backend: serviceName: argocd-server servicePort: https host: argocd.example.com tls: - hosts: - argocd.example.com secretName: argocd-secret # do not change, this is provided by Argo CD 安装完成后，就可以浏览器访问 ArgoCD dashbaord。默认的用户名为 admin，密码为 server Pod 的名称 ","date":"2020-07-20","objectID":"/gitops-argocd/:3:1","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"创建应用 ","date":"2020-07-20","objectID":"/gitops-argocd/:3:2","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"使用 Kustomize 管理配置 相信大家在使用 Kubernetes 的时候光为了部署应用就要写一大堆的配置文件，尤其是有多个环境，每个环境都要维护这么一套相似度极高的配置文件，既繁琐又容易出错。所以，Kubernetes 社区就开发了 Kustomize 来帮助我们简化和管理这样的配置。 Kustomize 的能力 kustomize 通过 Base \u0026 Overlays 维护不同环境的应用配置 kustomize 使用 patch 方式复用 Base 配置，并在 Overlay 描述与 Base 应用配置的差异部分来实现资源复用 kustomize 管理的都是 Kubernetes 原生 YAML 文件，学习成本低 下面是我们的实践 以一个应用为例首先我们有这样一份 base 位于 Git 仓库 deploy/k8s/base 文件夹下 -rw-r--r-- 1 lxkaka staff 2150 7 16 18:39 deployment.yml -rw-r--r-- 1 lxkaka staff 276 7 16 18:49 ingress.yml -rw-r--r-- 1 lxkaka staff 131 7 16 17:30 kustomization.yml 其中 deployment 里我们其实可以写多个应用，因为 service.yml 不应修改我们这里也合并到了 deployment.ym 里。 在 deploy/k8s/overlays 下分别包括了测试环境，生产环境的配置 以测试环境 kustomization.yml 为例，这里我们有两个应用 web, scheuler 根据环境的不同把需要改动的部分声明成配置文件即可，而不需要再重复的写一遍配置。Ingress 的改动通过编写一个 patch 文件也可以实现。 apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - ../../base patchesStrategicMerge: - web.yml - scheduler.yml patches: - path: ingress_patch.yml target: group: extensions version: v1beta1 kind: Ingress name: airflow namespace: airflow 最后的目录结构是下面的这样 k8s ├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── ingress.yaml └── overlays ├── prod │ ├── web.yaml │ ├── scheuler.yaml │ ├── ingess_patch.yaml │ └── kustomization.yaml └── test ├── web.yaml ├── scheuler.yaml ├── ingess_patch.yaml └── kustomization.yaml ","date":"2020-07-20","objectID":"/gitops-argocd/:3:3","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"CI Pipeline 首先为了突出重点，我们这里简化一下 Pipeline, 只定义了两个 stage, 其中 build 和 deploy 都分别包含了测试和生产环境 stages: - build - deploy build stage 这一步 build 新的 image 并推送到镜像仓库 .build: image: ${IMAGE_REGISYRY}/inf/common/docker:compose stage: build script: - \u003e echo \"$PROJECT start build $ENV:$VERSION =============\u003e\"; IMAGE_BASE=${BASE_DOCKER}/${PROJECT}/${ENV}/code; IMAGE_URL=${BASE_DOCKER}/${PROJECT}/${ENV}/code:${VERSION}; docker build --pull -t ${IMAGE_URL} --build-arg ENV=${ENV} -f deploy/docker/code.Dockerfile .; docker push ${IMAGE_URL}; echo \"build process done ==========\" deploy stage 这里我们重点介绍一下这个 stage, 根据不同的环境，我们会更新应用的 image tag, 并把 image 最新的配置更新到kustomization.yml，然后推送到 Git 仓库。 .deploy: stage: deploy image: ${IMAGE_REGISYRY}/inf/common/:kustomize:v1.1 before_script: - mkdir ~/.ssh - chmod 700 ~/.ssh - echo \"${DEPLOY_KEY}\" \u003e ~/.ssh/id_rsa - chmod 600 ~/.ssh/id_rsa - echo -e \"Host *\\n\\tStrictHostKeyChecking no\\n\\n\" \u003e ~/.ssh/config - git remote set-url origin ${REPOSITORY} - git config user.email ${GITLAB_USER_EMAIL} - git config user.name ${GITLAB_USER_NAME} script: - cd deploy/k8s/overlays/${ENV} - kustomize edit set image ${BASE_DOCKER}/${PROJECT}/${ENV}/code:${VERSION} - git add . - git commit -m '[skip ci]update image tag' - git push origin HEAD:${CI_COMMIT_REF_NAME} -f ","date":"2020-07-20","objectID":"/gitops-argocd/:3:4","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"ArgoCD 创建应用 在 ArgoCD 创建应用，其实就是告诉 ArgoCD 我们要发布什么应用，这个应用的配置去哪读取。既可以通过 Argo CRD 定义 编写 yaml 也可以通过 Web UI 创建应用 示例如下 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: app-test namespace: argocd spec: project: default source: repoURL: 'git@example.com:sync.git' path: deploy/k8s/overlays/test targetRevision: dev destination: server: 'https://kubernetes.default.svc' namespace: airflow syncPolicy: automated: {} 上面定义的 Application 这个资源，就是 Argo CD 用于描述应用的 CRD 对象： name：Argo CD 应用程序的名称 project：应用程序将被配置的项目名称，这是在 Argo CD 中应用程序的一种组织方式 repoURL：源代码的仓库地址 targetRevision：想要使用的 git 分支 path：Kubernetes 配置文件在仓库中的路径 destination：Kubernetes 集群中的目标 ","date":"2020-07-20","objectID":"/gitops-argocd/:3:5","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"流程 demo 在这里我们展示一下一个完整的 GitOps 流程 修改代码，push 到 feature 分支，并把分支合并到 dev 触发 Pipeline 在 ArgoCD 中查看应用同步状态 Pipeline 成功执行后，我们可以看到应用马上开始同步，图中我们可以看到新版本的 pod 已经 running, 老的 pod 正在被删除。我们这里 image tag 使用的是 Pipeline id，可以看到同步后的 image tag 是 177839 和 Pipeline id 一致，证明同步的就是这个版本。 我们的 GitOps 的实践就介绍到这里。希望大家试用，一定能体会到 GitOps 带来的 开发效率的提升 安全性的提升 可靠性的提升 附件 在这里贴一下 kustomize 的 dockerfile 供大家参考 FROM alpine:latest ENV KUSTOMIZE_VERSION v3.8.0 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \u0026\u0026 apk add git curl bash openssh RUN curl -L --output /tmp/kustomize_v3.8.0_linux_amd64.tar.gz https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv3.8.0/kustomize_v3.8.0_linux_amd64.tar.gz \\ \u0026\u0026 echo \"89cbe307506b25aca031ff6dfc9b4da022284ede65452a49e4e5988346f6354e /tmp/kustomize_v3.8.0_linux_amd64.tar.gz\" | sha256sum -c \\ \u0026\u0026 tar xzf /tmp/kustomize_v3.8.0_linux_amd64.tar.gz -C /usr/local/bin \\ \u0026\u0026 chmod +x /usr/local/bin/kustomize ","date":"2020-07-20","objectID":"/gitops-argocd/:3:6","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"GitOps 相信不少人听说过，这种新的交付模式越来越受到大家的关注，包括我自己在内。今天在这篇文章里我结合我们的实际生产上遇到的痛点，讲讲我们在 GitOps 上的实践，希望能给大家带来一些参考作用。 首先关于 GitOps 的介绍资源还是非常多的，在这里我引用一下大佬们总结的 GitOps 概念。 ","date":"2020-07-20","objectID":"/gitops-argocd/:0:0","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"GitOps 概念 GitOps 的概念最初来源于 Weaveworks 的联合创始人 Alexis 在 2017 年 8 月发表的一篇博客 GitOps - Operations by Pull Request。文章介绍了 Weaveworks 的工程师如何以 Git 作为事实的唯一真实来源，部署、管理和监控基于 Kubernetes 的 SaaS 应用。 GitOps 是一种持续交付的方式。它的核心思想是将应用系统的声明性基础架构和应用程序存放在 Git 版本库中。将 Git 作为交付流水线的核心，每个开发人员都可以提交拉取请求（Pull Request）并使用Gi​​t来加速和简化 Kubernetes的应用程序部署和运维任务。通过使用像Git这样的简单工具，开发人员可以更高效地将注意力集中在创建新功能而不是运维相关任务上（例如，应用系统安装、配置、迁移等）。 ","date":"2020-07-20","objectID":"/gitops-argocd/:1:0","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"GitOps 基本原则 以 Git 作为事实的唯一真实来源 描述系统相关的所有内容：策略，代码，配置和版本控制等，并且将这些内容全部存储在 Git 仓库中，在通过版本库中的内容构建系统的基础架构或者应用程序的时候，如果没有成功，则可以迅速的回滚，并且重新来过。 用 pull 的模式构建 Pipeline 不断地检查 Git 仓库，将任何状态变化都反映到 Kubernetes 集群中，具体说来就是 GitOps 检测到集群状态和 Git 仓库不一致，会从配置仓库中拉取更新后的清单，并将包含新功能的镜像部署到集群里。 关于 GitOps 就简要介绍一下，更多的信息大家可以去关注一下。 ","date":"2020-07-20","objectID":"/gitops-argocd/:1:1","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"工具链 因为我们讲的是 GitOps 的实践，所以依赖的工具或者说组件必须先介绍一下 Git 仓库 我们使用 GitLab 来管理和存储我们的代码和各种配置 Kubernetes 集群 用于部署我们应用程序的底层集群 Kustomize kubernetes 配置管理 ArgoCD ArgoCD 是一个声明式、GitOps 持续交付的 Kubernetes 工具，它的配置和使用分非常简单，并且自带一个简单易用的 Dashboard，更重要的是 ArgoCD 支持 kustomzie、helm、ksonnet 等多种工具 ","date":"2020-07-20","objectID":"/gitops-argocd/:2:0","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"流程 下图展示了我们实践的 GitOps 工作流程 提交代码到对应的 feature branch 代码和并到 dev branch or master branch 触发 GitLab CI 在 CI 中更新应用配置文件 提交新的配置到 GitLab 仓库 ArgoCD 监控到 GitLab 仓库变化，触发同步，拉取镜像发布新的应用版本 ","date":"2020-07-20","objectID":"/gitops-argocd/:3:0","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"安装 ArgoCD ArgoCD 的安装比较简单，我们可以选择用 helm 安装，或者直接 Kubectl apply 配置文件。 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 这里强调一下 ingress 的配置, 转发到 443 端口（80 会被 redirect 到443，造成浏览器多次 redirect） apiVersion: extensions/v1beta1 kind: Ingress metadata: name: argocd-server-http-ingress namespace: argocd annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" spec: rules: - http: paths: - backend: serviceName: argocd-server servicePort: https host: argocd.example.com tls: - hosts: - argocd.example.com secretName: argocd-secret # do not change, this is provided by Argo CD 安装完成后，就可以浏览器访问 ArgoCD dashbaord。默认的用户名为 admin，密码为 server Pod 的名称 ","date":"2020-07-20","objectID":"/gitops-argocd/:3:1","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"创建应用 ","date":"2020-07-20","objectID":"/gitops-argocd/:3:2","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"使用 Kustomize 管理配置 相信大家在使用 Kubernetes 的时候光为了部署应用就要写一大堆的配置文件，尤其是有多个环境，每个环境都要维护这么一套相似度极高的配置文件，既繁琐又容易出错。所以，Kubernetes 社区就开发了 Kustomize 来帮助我们简化和管理这样的配置。 Kustomize 的能力 kustomize 通过 Base \u0026 Overlays 维护不同环境的应用配置 kustomize 使用 patch 方式复用 Base 配置，并在 Overlay 描述与 Base 应用配置的差异部分来实现资源复用 kustomize 管理的都是 Kubernetes 原生 YAML 文件，学习成本低 下面是我们的实践 以一个应用为例首先我们有这样一份 base 位于 Git 仓库 deploy/k8s/base 文件夹下 -rw-r--r-- 1 lxkaka staff 2150 7 16 18:39 deployment.yml -rw-r--r-- 1 lxkaka staff 276 7 16 18:49 ingress.yml -rw-r--r-- 1 lxkaka staff 131 7 16 17:30 kustomization.yml 其中 deployment 里我们其实可以写多个应用，因为 service.yml 不应修改我们这里也合并到了 deployment.ym 里。 在 deploy/k8s/overlays 下分别包括了测试环境，生产环境的配置 以测试环境 kustomization.yml 为例，这里我们有两个应用 web, scheuler 根据环境的不同把需要改动的部分声明成配置文件即可，而不需要再重复的写一遍配置。Ingress 的改动通过编写一个 patch 文件也可以实现。 apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - ../../base patchesStrategicMerge: - web.yml - scheduler.yml patches: - path: ingress_patch.yml target: group: extensions version: v1beta1 kind: Ingress name: airflow namespace: airflow 最后的目录结构是下面的这样 k8s ├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── ingress.yaml └── overlays ├── prod │ ├── web.yaml │ ├── scheuler.yaml │ ├── ingess_patch.yaml │ └── kustomization.yaml └── test ├── web.yaml ├── scheuler.yaml ├── ingess_patch.yaml └── kustomization.yaml ","date":"2020-07-20","objectID":"/gitops-argocd/:3:3","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"CI Pipeline 首先为了突出重点，我们这里简化一下 Pipeline, 只定义了两个 stage, 其中 build 和 deploy 都分别包含了测试和生产环境 stages: - build - deploy build stage 这一步 build 新的 image 并推送到镜像仓库 .build: image: ${IMAGE_REGISYRY}/inf/common/docker:compose stage: build script: - \u003e echo \"$PROJECT start build $ENV:$VERSION =============\u003e\"; IMAGE_BASE=${BASE_DOCKER}/${PROJECT}/${ENV}/code; IMAGE_URL=${BASE_DOCKER}/${PROJECT}/${ENV}/code:${VERSION}; docker build --pull -t ${IMAGE_URL} --build-arg ENV=${ENV} -f deploy/docker/code.Dockerfile .; docker push ${IMAGE_URL}; echo \"build process done ==========\" deploy stage 这里我们重点介绍一下这个 stage, 根据不同的环境，我们会更新应用的 image tag, 并把 image 最新的配置更新到kustomization.yml，然后推送到 Git 仓库。 .deploy: stage: deploy image: ${IMAGE_REGISYRY}/inf/common/:kustomize:v1.1 before_script: - mkdir ~/.ssh - chmod 700 ~/.ssh - echo \"${DEPLOY_KEY}\" \u003e ~/.ssh/id_rsa - chmod 600 ~/.ssh/id_rsa - echo -e \"Host *\\n\\tStrictHostKeyChecking no\\n\\n\" \u003e ~/.ssh/config - git remote set-url origin ${REPOSITORY} - git config user.email ${GITLAB_USER_EMAIL} - git config user.name ${GITLAB_USER_NAME} script: - cd deploy/k8s/overlays/${ENV} - kustomize edit set image ${BASE_DOCKER}/${PROJECT}/${ENV}/code:${VERSION} - git add . - git commit -m '[skip ci]update image tag' - git push origin HEAD:${CI_COMMIT_REF_NAME} -f ","date":"2020-07-20","objectID":"/gitops-argocd/:3:4","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"ArgoCD 创建应用 在 ArgoCD 创建应用，其实就是告诉 ArgoCD 我们要发布什么应用，这个应用的配置去哪读取。既可以通过 Argo CRD 定义 编写 yaml 也可以通过 Web UI 创建应用 示例如下 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: app-test namespace: argocd spec: project: default source: repoURL: 'git@example.com:sync.git' path: deploy/k8s/overlays/test targetRevision: dev destination: server: 'https://kubernetes.default.svc' namespace: airflow syncPolicy: automated: {} 上面定义的 Application 这个资源，就是 Argo CD 用于描述应用的 CRD 对象： name：Argo CD 应用程序的名称 project：应用程序将被配置的项目名称，这是在 Argo CD 中应用程序的一种组织方式 repoURL：源代码的仓库地址 targetRevision：想要使用的 git 分支 path：Kubernetes 配置文件在仓库中的路径 destination：Kubernetes 集群中的目标 ","date":"2020-07-20","objectID":"/gitops-argocd/:3:5","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"流程 demo 在这里我们展示一下一个完整的 GitOps 流程 修改代码，push 到 feature 分支，并把分支合并到 dev 触发 Pipeline 在 ArgoCD 中查看应用同步状态 Pipeline 成功执行后，我们可以看到应用马上开始同步，图中我们可以看到新版本的 pod 已经 running, 老的 pod 正在被删除。我们这里 image tag 使用的是 Pipeline id，可以看到同步后的 image tag 是 177839 和 Pipeline id 一致，证明同步的就是这个版本。 我们的 GitOps 的实践就介绍到这里。希望大家试用，一定能体会到 GitOps 带来的 开发效率的提升 安全性的提升 可靠性的提升 附件 在这里贴一下 kustomize 的 dockerfile 供大家参考 FROM alpine:latest ENV KUSTOMIZE_VERSION v3.8.0 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \u0026\u0026 apk add git curl bash openssh RUN curl -L --output /tmp/kustomize_v3.8.0_linux_amd64.tar.gz https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv3.8.0/kustomize_v3.8.0_linux_amd64.tar.gz \\ \u0026\u0026 echo \"89cbe307506b25aca031ff6dfc9b4da022284ede65452a49e4e5988346f6354e /tmp/kustomize_v3.8.0_linux_amd64.tar.gz\" | sha256sum -c \\ \u0026\u0026 tar xzf /tmp/kustomize_v3.8.0_linux_amd64.tar.gz -C /usr/local/bin \\ \u0026\u0026 chmod +x /usr/local/bin/kustomize ","date":"2020-07-20","objectID":"/gitops-argocd/:3:6","tags":["GitOps","ArgoCD","kubernetes"],"title":"Kustomize + Argo CD 优化发布流程","uri":"/gitops-argocd/"},{"categories":null,"content":"在这篇文章里我接着讲述一下数仓数据同步到 ADB 的方案演进。随着数据规模纵向和横向的扩大，把 hive 作为同步的数据源瓶颈越来越明显。首先是单表的数据超过 3000w 后，分段（limit方法）的速度非常缓慢；再者表越来越多 hive 的 IO 压力凸显。在上一篇文末我已经提到了这个问题，当时的设想是把数据源变成 OSS, 这样数据源我们完全可以认为瓶颈不会出现在源头。下面我具体介绍一下方案的设计和实施过程。 ","date":"2020-06-23","objectID":"/orc-adb/:0:0","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"目标改进 首先我们的目标在此前的基础上做出了如下改进 \b延时 数据源头切换成 OSS, 数仓里的表 location 全部在 OSS 的特定 bucket 里并且文件格式是 ORC, 也就说数据源的存储形态就是 OSS bucket 中的一个个的 ORC 文件。一个表可能有多个 ORC 文件组成，所以我们同步任务的粒度可以精细到单个 ORC 文件，多个 ORC 文件创建多个同步任务，这些任务同时同时进行，同步的效率可以达到 ADB 的写入上限。相比之前的方案，同步的效率有了巨大的提升。 容错 同步任务的前置和后置分别抽成 task, 前置 task 负责中间表的清理和创建；后置 task 负责中间表的 rename 和原表的备份。这样的设计提高了容错率，比如当一张表的某一个同步任务失败，在有唯一键的情况下重新执行这个 task 即可；再比如当需要重新同步整张表，重置前置 task 即可重新同步；或者本次同步失败，只需要标记后置 task 为失败即可。 扩展性 另外一个巨大的改进是基础设施的改进。在上一篇文章中我们讲过同步任务的调度框架是 Airflow, 每个 task 最终以消息的形式发送给 celery worker 执行。在 Airflow 1.10 后引入了一种新的 Executor–Kubernetes Executor, 这是我一直想引入的 Executor 终于可以投入到生产使用了。简单说就是每个 task 现在可以以 k8s Pod 的形式调度到 k8s 集群执行，task 执行完成则 Pod 自动被 remove。这类非常驻的任务可以搭配公有云厂商提供的 serverless k8s 集群可以说非常高效。 ","date":"2020-06-23","objectID":"/orc-adb/:1:0","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"同步流程 改进的大致同步流程如下 ","date":"2020-06-23","objectID":"/orc-adb/:2:0","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"方案 ","date":"2020-06-23","objectID":"/orc-adb/:3:0","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"task 生成 根据表的 location 前缀找到所有有效的 ORC 文件，有的表可能因为分区数很多，可能产生了数千甚至上万的小文件，如果每个文件生成一个 task, 小文件对应的 task 同步数据的时间比创建 pod 的时间还短，会十分的低效。所以，这里有两个方法，一是在 hive 层面我们可以使用命令例如 alter table test_xxx partition(product_name='a',measure_name='b') CONCATENATE; 来压缩小文件；二是我们把多个小文件组合成一个 task, 即多个小文件组合成一个 batch, 每个 batch 生成一个 task。 综合来看第二种方式更方便高效。 示例代码如下 result = [] res = oss_bucket.list_objects(oss_prefix) index = 0 acc = 0 batch = [] while res.object_list: for f in res.object_list: if f.size \u003e 0: s = f.size // 1000000 acc += s batch.append(f) if acc \u003e 50: index += 1 result.extend(sync_operator(batch, dag, sync_task, index, memory_base=memory_base)) acc = 0 batch = [] if res.next_marker: res = oss_bucket.list_objects(oss_prefix, marker=res.next_marker) else: break index += 1 result.extend(sync_operator(batch, dag, sync_task, index, memory_base=memory_base)) return result ","date":"2020-06-23","objectID":"/orc-adb/:3:1","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"ORC 解析 ORC 文件的解析的关键就是解析表结构；如何获取一行数据 示例代码如下 # 下载 oss 文件到本地 logger.info(f'start download oss file {file}....') try: # 断点续传 oss2.resumable_download(oss_bucket, file, local_name, num_threads=3) except Exception as e: logger.error(e) download(oss_bucket, local_name) # 解析 orc 文件 with open(local_name, 'rb') as f: data = orc.ORCFile(f) table = data.read() # 表的 colum 数 num_columns = table.num_columns logger.info(table.num_rows) logger.info(num_columns) # 获取多行数据的集合 for i in table.to_batches(): d = i.to_pydict() zip_data = [d[n] for n in table.schema.names] # 这里得到多行数据组成的元祖可以直接 insert 到 ADB 表中 r = zip(*zip_data) # 对于分区表我们解析文件的路径，从路径中获得分区并插入到行中 partition = [] partition_value = [] for path in file_parts: if '=' in path: partition.append(path.split('=')[0]) partition_value.append(path.split('=')[1]) columns = table.schema.names + partition columns = ','.join(columns) 其中数据的写入过程和容错过程与上篇文章所描述的基本一致，这里不再赘述。 ","date":"2020-06-23","objectID":"/orc-adb/:3:2","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"使用 Kubernetes Executor 在 Airflow 的部署上，我们只需要把 Airflow webserver 和 scheduler 通过 deployment 部署到 k8s 集群。 配置 启用 k8s executor 需要基本配置如下 executor = KubernetesExecutor [kubernetes] # The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker to Run worker_container_repository = {IMAGE_BASE} worker_container_tag = {VERSION} worker_container_image_pull_policy = Always # If True (default), worker pods will be deleted upon termination delete_worker_pods = True # Number of Kubernetes Worker Pod creation calls per scheduler loop worker_pods_creation_batch_size = 20 # The Kubernetes namespace where airflow workers should be created. Defaults to ``default`` namespace = default # For docker image already contains DAGs, this is set to ``True``, and the worker will # search for dags in dags_folder, # otherwise use git sync or dags volume claim to mount DAGs # 我们这里把 dag 打包到镜像里，官方推荐使用 git repo 同步的方式，这个需要 git sync 的一些配置，具体可参考官方文档 dags_in_image = True # Use the service account kubernetes gives to pods to connect to kubernetes cluster. # It's intended for clients that expect to be running inside a pod running on kubernetes. # It will raise an exception if called from a process not running in a kubernetes environment. in_cluster = False # When running with in_cluster=False change the default cluster_context or config_file # options to Kubernetes client. Leave blank these to use default behaviour like ``kubectl`` has. # cluster_context = config_file = /root/airflow/k8sconfig Airfow 使用 Kubernetes Executor 我画了下面这张图来说明一下现在的架构 通过浏览器我们可以访问到部署在 k8s 中的 Airflow Web服务，查看 task 的执行情况； Airflow scheduler 负责调用集群（k8s集群可以是本集群或者是其他集群）的 API 根据 Airflow operator 的定义和其他 k8s 配置创建 Pod 执行 task; task 执行完成 Pod 被删除，执行日志上传到 OSS 由不同 batch 生成的 task 的 dag 示意图如下 上面说到的前置 task 即图中的 start, 后者 task 即 finish, 同步任务即 batch_i ","date":"2020-06-23","objectID":"/orc-adb/:3:3","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"存在的问题 到这里 ORC 数据同步方案介绍完了，这个方案也不是最完善的。从示例代码中可以看出来，大量数据同步前都放到内存里了，在执行的同步过程中随着数据量的上升内存的占用会越来越高，如果同步前没有规划好 task 的内存资源(反应到 k8s 即 Pod 的内存占用)可能 task 会因为 OOM 被 kill。资源的优化，如何节省内存是下一步需要思考的问题。 ","date":"2020-06-23","objectID":"/orc-adb/:4:0","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"在这篇文章里我接着讲述一下数仓数据同步到 ADB 的方案演进。随着数据规模纵向和横向的扩大，把 hive 作为同步的数据源瓶颈越来越明显。首先是单表的数据超过 3000w 后，分段（limit方法）的速度非常缓慢；再者表越来越多 hive 的 IO 压力凸显。在上一篇文末我已经提到了这个问题，当时的设想是把数据源变成 OSS, 这样数据源我们完全可以认为瓶颈不会出现在源头。下面我具体介绍一下方案的设计和实施过程。 ","date":"2020-06-23","objectID":"/orc-adb/:0:0","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"目标改进 首先我们的目标在此前的基础上做出了如下改进 \b延时 数据源头切换成 OSS, 数仓里的表 location 全部在 OSS 的特定 bucket 里并且文件格式是 ORC, 也就说数据源的存储形态就是 OSS bucket 中的一个个的 ORC 文件。一个表可能有多个 ORC 文件组成，所以我们同步任务的粒度可以精细到单个 ORC 文件，多个 ORC 文件创建多个同步任务，这些任务同时同时进行，同步的效率可以达到 ADB 的写入上限。相比之前的方案，同步的效率有了巨大的提升。 容错 同步任务的前置和后置分别抽成 task, 前置 task 负责中间表的清理和创建；后置 task 负责中间表的 rename 和原表的备份。这样的设计提高了容错率，比如当一张表的某一个同步任务失败，在有唯一键的情况下重新执行这个 task 即可；再比如当需要重新同步整张表，重置前置 task 即可重新同步；或者本次同步失败，只需要标记后置 task 为失败即可。 扩展性 另外一个巨大的改进是基础设施的改进。在上一篇文章中我们讲过同步任务的调度框架是 Airflow, 每个 task 最终以消息的形式发送给 celery worker 执行。在 Airflow 1.10 后引入了一种新的 Executor–Kubernetes Executor, 这是我一直想引入的 Executor 终于可以投入到生产使用了。简单说就是每个 task 现在可以以 k8s Pod 的形式调度到 k8s 集群执行，task 执行完成则 Pod 自动被 remove。这类非常驻的任务可以搭配公有云厂商提供的 serverless k8s 集群可以说非常高效。 ","date":"2020-06-23","objectID":"/orc-adb/:1:0","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"同步流程 改进的大致同步流程如下 ","date":"2020-06-23","objectID":"/orc-adb/:2:0","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"方案 ","date":"2020-06-23","objectID":"/orc-adb/:3:0","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"task 生成 根据表的 location 前缀找到所有有效的 ORC 文件，有的表可能因为分区数很多，可能产生了数千甚至上万的小文件，如果每个文件生成一个 task, 小文件对应的 task 同步数据的时间比创建 pod 的时间还短，会十分的低效。所以，这里有两个方法，一是在 hive 层面我们可以使用命令例如 alter table test_xxx partition(product_name='a',measure_name='b') CONCATENATE; 来压缩小文件；二是我们把多个小文件组合成一个 task, 即多个小文件组合成一个 batch, 每个 batch 生成一个 task。 综合来看第二种方式更方便高效。 示例代码如下 result = [] res = oss_bucket.list_objects(oss_prefix) index = 0 acc = 0 batch = [] while res.object_list: for f in res.object_list: if f.size \u003e 0: s = f.size // 1000000 acc += s batch.append(f) if acc \u003e 50: index += 1 result.extend(sync_operator(batch, dag, sync_task, index, memory_base=memory_base)) acc = 0 batch = [] if res.next_marker: res = oss_bucket.list_objects(oss_prefix, marker=res.next_marker) else: break index += 1 result.extend(sync_operator(batch, dag, sync_task, index, memory_base=memory_base)) return result ","date":"2020-06-23","objectID":"/orc-adb/:3:1","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"ORC 解析 ORC 文件的解析的关键就是解析表结构；如何获取一行数据 示例代码如下 # 下载 oss 文件到本地 logger.info(f'start download oss file {file}....') try: # 断点续传 oss2.resumable_download(oss_bucket, file, local_name, num_threads=3) except Exception as e: logger.error(e) download(oss_bucket, local_name) # 解析 orc 文件 with open(local_name, 'rb') as f: data = orc.ORCFile(f) table = data.read() # 表的 colum 数 num_columns = table.num_columns logger.info(table.num_rows) logger.info(num_columns) # 获取多行数据的集合 for i in table.to_batches(): d = i.to_pydict() zip_data = [d[n] for n in table.schema.names] # 这里得到多行数据组成的元祖可以直接 insert 到 ADB 表中 r = zip(*zip_data) # 对于分区表我们解析文件的路径，从路径中获得分区并插入到行中 partition = [] partition_value = [] for path in file_parts: if '=' in path: partition.append(path.split('=')[0]) partition_value.append(path.split('=')[1]) columns = table.schema.names + partition columns = ','.join(columns) 其中数据的写入过程和容错过程与上篇文章所描述的基本一致，这里不再赘述。 ","date":"2020-06-23","objectID":"/orc-adb/:3:2","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"使用 Kubernetes Executor 在 Airflow 的部署上，我们只需要把 Airflow webserver 和 scheduler 通过 deployment 部署到 k8s 集群。 配置 启用 k8s executor 需要基本配置如下 executor = KubernetesExecutor [kubernetes] # The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker to Run worker_container_repository = {IMAGE_BASE} worker_container_tag = {VERSION} worker_container_image_pull_policy = Always # If True (default), worker pods will be deleted upon termination delete_worker_pods = True # Number of Kubernetes Worker Pod creation calls per scheduler loop worker_pods_creation_batch_size = 20 # The Kubernetes namespace where airflow workers should be created. Defaults to ``default`` namespace = default # For docker image already contains DAGs, this is set to ``True``, and the worker will # search for dags in dags_folder, # otherwise use git sync or dags volume claim to mount DAGs # 我们这里把 dag 打包到镜像里，官方推荐使用 git repo 同步的方式，这个需要 git sync 的一些配置，具体可参考官方文档 dags_in_image = True # Use the service account kubernetes gives to pods to connect to kubernetes cluster. # It's intended for clients that expect to be running inside a pod running on kubernetes. # It will raise an exception if called from a process not running in a kubernetes environment. in_cluster = False # When running with in_cluster=False change the default cluster_context or config_file # options to Kubernetes client. Leave blank these to use default behaviour like ``kubectl`` has. # cluster_context = config_file = /root/airflow/k8sconfig Airfow 使用 Kubernetes Executor 我画了下面这张图来说明一下现在的架构 通过浏览器我们可以访问到部署在 k8s 中的 Airflow Web服务，查看 task 的执行情况； Airflow scheduler 负责调用集群（k8s集群可以是本集群或者是其他集群）的 API 根据 Airflow operator 的定义和其他 k8s 配置创建 Pod 执行 task; task 执行完成 Pod 被删除，执行日志上传到 OSS 由不同 batch 生成的 task 的 dag 示意图如下 上面说到的前置 task 即图中的 start, 后者 task 即 finish, 同步任务即 batch_i ","date":"2020-06-23","objectID":"/orc-adb/:3:3","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"存在的问题 到这里 ORC 数据同步方案介绍完了，这个方案也不是最完善的。从示例代码中可以看出来，大量数据同步前都放到内存里了，在执行的同步过程中随着数据量的上升内存的占用会越来越高，如果同步前没有规划好 task 的内存资源(反应到 k8s 即 Pod 的内存占用)可能 task 会因为 OOM 被 kill。资源的优化，如何节省内存是下一步需要思考的问题。 ","date":"2020-06-23","objectID":"/orc-adb/:4:0","tags":["airflow","kubernetes executor","orc"],"title":"Hive To AnalyticDB 数据同步方案演进","uri":"/orc-adb/"},{"categories":null,"content":"最近在做同步数仓数据的工作，从设计到实现整个过程其实是挺有意思的。这里记录和分享一下我们的实施方案，也能给有类似需求的同学提供一些参考价值。 ","date":"2020-05-22","objectID":"/airflow-sync/:0:0","tags":["hive","AnalyticDB","数据同步"],"title":"Hive To AnalyticDB 数据同步方案设计","uri":"/airflow-sync/"},{"categories":null,"content":"需求 在调研了阿里云的 AnalyticDB MySQL 后，我们决定把业务系统使用 Presto 查询 Hive 数据的过程替换成业直接查询 ADB(文中 ADB 都指的是 AnalyticDB MySQL)。具体原因这里就不做展开，简单说三点就是第一解耦我们目前业务直接依赖 EMR， 第二是性能上有很好的提升， 第三点用钱买服务省心。 所以简单来说我们需要把数仓 DWS 层的数据同步到 ADB。 ","date":"2020-05-22","objectID":"/airflow-sync/:1:0","tags":["hive","AnalyticDB","数据同步"],"title":"Hive To AnalyticDB 数据同步方案设计","uri":"/airflow-sync/"},{"categories":null,"content":"目标 在明确了需求后，数仓数据同步的任务需要达到以下目标 扩展性高 新建的表或者删除表都不需要改动同步逻辑，或者其他代码 延时低 数仓的数据生产完毕或者更新完毕，相关的表就能开始同步 容错率高 不会出现漏同步，或者重复同步 ","date":"2020-05-22","objectID":"/airflow-sync/:2:0","tags":["hive","AnalyticDB","数据同步"],"title":"Hive To AnalyticDB 数据同步方案设计","uri":"/airflow-sync/"},{"categories":null,"content":"方案设计 为了达到上述的目标，数据同步的主要流程如下 首先我们的同步任务最终执行的形式是 Airflow 中的一个 task，各个不同表的同步任务组成了 Airflow 中的一个 dag。 扩展性 为了达到扩展性的要求，我们把需要同步的 table 和他的依赖 task 以及一些别的信息存入 mysql 表中(表名 sync_task)。上面提到的 dag 就是根据 sync_task 的有效记录动态生成的。所以无论新建同步任务或者删除同步任务，只需要更新 sync_task， 那么 dag 也会动态调整。 dag 示例代码如下 # 读取同步记录 sync_tasks = get_mysql_dataset( mysql_conn_id=\"mysql_metadata\", schema=\"metadata\", sql=f\"select * from sync_task where status = 1\", ) # 生成 dag 里的同步 task for sync_task in sync_tasks: adb_table = sync_task[\"adb_table\"] sync_task_id = sync_task[\"id\"] sync_conditions = sync_task[\"conditions\"] if not sync_conditions or sync_conditions == \"None\": sync_conditions = \"\" HiveRecordToMySqlOperator( task_id=f\"sync_to_adb_{adb_table}_{sync_task_id}\", dag=dag, hive_table=sync_task[\"hive_table\"], mysql_table=sync_task[\"adb_table\"], mysql_conn_id=sync_task[\"mysql_conn_id\"], dml_operator=sync_task[\"dml_operator\"], conditions=sync_conditions, sync_task_record_id=sync_task_id, ) 延时低 为了达到高效的同步，我们做了以下几点来保证 多线程读取和写入 我们知道在 Python 中多线程真正能分配到 CPU 时间片的只有一线程，所以很多情况下多线程没有太多用处。但是我们这个同步数据过程是典型的 IO 密集型任务，如果单线程的话 CPU 大部分时间处于空闲状态，这个时候多线程就有意义了，能显著的提高同步效率。 示例代码如下 with ThreadPoolExecutor(5 * multiprocessing.cpu_count()) as executor: while rows: serialize_rows = [] for row in rows: lst = [] for cell in row: lst.append(self._serialize_cell(cell)) serialize_rows.append(tuple(lst)) placeholders = \",\".join([\"%s\"] * len(serialize_rows[0])) sql = f\"{self.dml_operator} {tmp_mysql_table} {target_fields} VALUES ({placeholders})\" task = executor.submit( self._mysql_run, sql, serialize_rows, mysql, tmp_mysql_table ) all_tasks.append(task) rows = hive_cursor.fetchmany(self.query_every) 数据分片 上面我们提到的多线程只利用了 airflow worker 的一个进程，所以如果我们把数据分片，每一片数据是一个独立的同步任务，那么自然就能生成一个 dag 中的 task, 多个数据分片就同时生成多个同步任务，这些任务能被 airflow scheduler 调度给不同的 worker 或者同一个 worker 的不同进程。 示例代码如下 # 获取同步数据量 # 这里为了不消耗 EMR 的资源，我们去 hive metastore 表中获取待同步表的数据量，而不是使用 select count(*) hive_conn = HiveCliHook(hive_cli_conn_id=\"hive_cli_default\") hive_conn.run_cli(\"ANALYZE TABLE dm.delphi_membership_properties COMPUTE STATISTICS;\") table_rows = get_mysql_dataset( mysql_conn_id=\"hivemeta_db\", schema=\"hivemeta\", sql=\"select PARAM_KEY, PARAM_VALUE from TBLS as tb inner join TABLE_PARAMS as tp on(tb.TBL_ID=tp.TBL_ID) \" \"where tb.TBL_NAME='delphi_membership_properties' and tp.PARAM_KEY='numRows';\" ) if table_rows: table_count = int(table_rows[0][\"PARAM_VALUE\"]) # 这里分片数量可以根据实际的 worker 数和 worker 进程数，动态调整 # 下面只是一个简单的示例 workers = multiprocessing.cpu_count() // 2 step = math.ceil(table_count / workers) conf_ids = get_mysql_dataset( mysql_conn_id=\"mysql_metadata\", schema=\"metadata\", sql=\"select id from adb_sync_task where hive_table='dm.delphi_membership_properties' and status=1\", ) mysql = MySqlHook(mysql_conn_id=\"mysql_metadata\") if conf_ids: conf_ids = [item[\"id\"] for item in conf_ids] # 根据分片数，生成同步记录 for i in range(workers): conditions = f\"limit {i * step},{step}\" if i \u003c len(conf_ids) and conf_ids[i]: sql = f\"update adb_sync_task set conditions='{conditions}' where id={conf_ids[i]}\" else: sql = ( f\"insert into adb_sync_task (dag_id, task_id, hive_table, adb_table, dml_operator, \" f\"conditions, extra_sql) values ('ods_to_delphi_membership_dm','update_membership_properties_trigger_conf',\" f\"'dm.delphi_membership_properties','delphi_membership_properties','REPLACE INTO','{conditions}',\" f\"'primary key (membership_id));')\" ) mysql.run(sql) 调整 dag 执行频率 我们可以根据实际情况调整 dag 的执行频率， 比如可以调整到分钟级别，这样能尽快的跟上同步任务的依赖，能及时触发同步操作。 容错率高 为了保证同步任务的正确执行，我们充分利用了 airflow 中的 task_instace, 每一次 task 的执行都会生成记录被 airflow 写入 mysql。 不漏跑 每次执行同步任务需要先确保依赖已经在当前时间之前成功执行 示例代码如下 # 筛选出同步任务依赖的 dag 最近的执行时间 dep_sql = f\"select end_date from task_instance where dag_id='{dag_id}' and task_id='{task_id}' and state='success' order by end_date desc limit 1\" dep_res = get_mysql_dataset( mysql_conn_id=\"airflow_db\", schema=\"airflow\", sql=dep_sql ) print('dep：', dep_sql, dep_res) # 筛选出同步任务最近成功的执行时间 sync_task_id = f\"sync_to_adb_{self.mysql_table}_{self.sync_task_record_id}\" sql = f\"select end_date from task_instanc","date":"2020-05-22","objectID":"/airflow-sync/:3:0","tags":["hive","AnalyticDB","数据同步"],"title":"Hive To AnalyticDB 数据同步方案设计","uri":"/airflow-sync/"},{"categories":null,"content":"存在的问题 当同步任务特别多且在同一时间段触发了较多的同步操作，那么我们的数据源 hive 和 airflow worker 都可能成为瓶颈，影响到数据同步。airflow worker 我们可以通过限制 task concurrency 和增加 worker（worker扩展比较容易）才解决。hive 的并发瓶颈需要我们去思考更好的方案，未来考虑直接从 OSS 同步数据。 ","date":"2020-05-22","objectID":"/airflow-sync/:4:0","tags":["hive","AnalyticDB","数据同步"],"title":"Hive To AnalyticDB 数据同步方案设计","uri":"/airflow-sync/"},{"categories":null,"content":"最近在做同步数仓数据的工作，从设计到实现整个过程其实是挺有意思的。这里记录和分享一下我们的实施方案，也能给有类似需求的同学提供一些参考价值。 ","date":"2020-05-22","objectID":"/airflow-sync/:0:0","tags":["hive","AnalyticDB","数据同步"],"title":"Hive To AnalyticDB 数据同步方案设计","uri":"/airflow-sync/"},{"categories":null,"content":"需求 在调研了阿里云的 AnalyticDB MySQL 后，我们决定把业务系统使用 Presto 查询 Hive 数据的过程替换成业直接查询 ADB(文中 ADB 都指的是 AnalyticDB MySQL)。具体原因这里就不做展开，简单说三点就是第一解耦我们目前业务直接依赖 EMR， 第二是性能上有很好的提升， 第三点用钱买服务省心。 所以简单来说我们需要把数仓 DWS 层的数据同步到 ADB。 ","date":"2020-05-22","objectID":"/airflow-sync/:1:0","tags":["hive","AnalyticDB","数据同步"],"title":"Hive To AnalyticDB 数据同步方案设计","uri":"/airflow-sync/"},{"categories":null,"content":"目标 在明确了需求后，数仓数据同步的任务需要达到以下目标 扩展性高 新建的表或者删除表都不需要改动同步逻辑，或者其他代码 延时低 数仓的数据生产完毕或者更新完毕，相关的表就能开始同步 容错率高 不会出现漏同步，或者重复同步 ","date":"2020-05-22","objectID":"/airflow-sync/:2:0","tags":["hive","AnalyticDB","数据同步"],"title":"Hive To AnalyticDB 数据同步方案设计","uri":"/airflow-sync/"},{"categories":null,"content":"方案设计 为了达到上述的目标，数据同步的主要流程如下 首先我们的同步任务最终执行的形式是 Airflow 中的一个 task，各个不同表的同步任务组成了 Airflow 中的一个 dag。 扩展性 为了达到扩展性的要求，我们把需要同步的 table 和他的依赖 task 以及一些别的信息存入 mysql 表中(表名 sync_task)。上面提到的 dag 就是根据 sync_task 的有效记录动态生成的。所以无论新建同步任务或者删除同步任务，只需要更新 sync_task， 那么 dag 也会动态调整。 dag 示例代码如下 # 读取同步记录 sync_tasks = get_mysql_dataset( mysql_conn_id=\"mysql_metadata\", schema=\"metadata\", sql=f\"select * from sync_task where status = 1\", ) # 生成 dag 里的同步 task for sync_task in sync_tasks: adb_table = sync_task[\"adb_table\"] sync_task_id = sync_task[\"id\"] sync_conditions = sync_task[\"conditions\"] if not sync_conditions or sync_conditions == \"None\": sync_conditions = \"\" HiveRecordToMySqlOperator( task_id=f\"sync_to_adb_{adb_table}_{sync_task_id}\", dag=dag, hive_table=sync_task[\"hive_table\"], mysql_table=sync_task[\"adb_table\"], mysql_conn_id=sync_task[\"mysql_conn_id\"], dml_operator=sync_task[\"dml_operator\"], conditions=sync_conditions, sync_task_record_id=sync_task_id, ) 延时低 为了达到高效的同步，我们做了以下几点来保证 多线程读取和写入 我们知道在 Python 中多线程真正能分配到 CPU 时间片的只有一线程，所以很多情况下多线程没有太多用处。但是我们这个同步数据过程是典型的 IO 密集型任务，如果单线程的话 CPU 大部分时间处于空闲状态，这个时候多线程就有意义了，能显著的提高同步效率。 示例代码如下 with ThreadPoolExecutor(5 * multiprocessing.cpu_count()) as executor: while rows: serialize_rows = [] for row in rows: lst = [] for cell in row: lst.append(self._serialize_cell(cell)) serialize_rows.append(tuple(lst)) placeholders = \",\".join([\"%s\"] * len(serialize_rows[0])) sql = f\"{self.dml_operator} {tmp_mysql_table} {target_fields} VALUES ({placeholders})\" task = executor.submit( self._mysql_run, sql, serialize_rows, mysql, tmp_mysql_table ) all_tasks.append(task) rows = hive_cursor.fetchmany(self.query_every) 数据分片 上面我们提到的多线程只利用了 airflow worker 的一个进程，所以如果我们把数据分片，每一片数据是一个独立的同步任务，那么自然就能生成一个 dag 中的 task, 多个数据分片就同时生成多个同步任务，这些任务能被 airflow scheduler 调度给不同的 worker 或者同一个 worker 的不同进程。 示例代码如下 # 获取同步数据量 # 这里为了不消耗 EMR 的资源，我们去 hive metastore 表中获取待同步表的数据量，而不是使用 select count(*) hive_conn = HiveCliHook(hive_cli_conn_id=\"hive_cli_default\") hive_conn.run_cli(\"ANALYZE TABLE dm.delphi_membership_properties COMPUTE STATISTICS;\") table_rows = get_mysql_dataset( mysql_conn_id=\"hivemeta_db\", schema=\"hivemeta\", sql=\"select PARAM_KEY, PARAM_VALUE from TBLS as tb inner join TABLE_PARAMS as tp on(tb.TBL_ID=tp.TBL_ID) \" \"where tb.TBL_NAME='delphi_membership_properties' and tp.PARAM_KEY='numRows';\" ) if table_rows: table_count = int(table_rows[0][\"PARAM_VALUE\"]) # 这里分片数量可以根据实际的 worker 数和 worker 进程数，动态调整 # 下面只是一个简单的示例 workers = multiprocessing.cpu_count() // 2 step = math.ceil(table_count / workers) conf_ids = get_mysql_dataset( mysql_conn_id=\"mysql_metadata\", schema=\"metadata\", sql=\"select id from adb_sync_task where hive_table='dm.delphi_membership_properties' and status=1\", ) mysql = MySqlHook(mysql_conn_id=\"mysql_metadata\") if conf_ids: conf_ids = [item[\"id\"] for item in conf_ids] # 根据分片数，生成同步记录 for i in range(workers): conditions = f\"limit {i * step},{step}\" if i \u003c len(conf_ids) and conf_ids[i]: sql = f\"update adb_sync_task set conditions='{conditions}' where id={conf_ids[i]}\" else: sql = ( f\"insert into adb_sync_task (dag_id, task_id, hive_table, adb_table, dml_operator, \" f\"conditions, extra_sql) values ('ods_to_delphi_membership_dm','update_membership_properties_trigger_conf',\" f\"'dm.delphi_membership_properties','delphi_membership_properties','REPLACE INTO','{conditions}',\" f\"'primary key (membership_id));')\" ) mysql.run(sql) 调整 dag 执行频率 我们可以根据实际情况调整 dag 的执行频率， 比如可以调整到分钟级别，这样能尽快的跟上同步任务的依赖，能及时触发同步操作。 容错率高 为了保证同步任务的正确执行，我们充分利用了 airflow 中的 task_instace, 每一次 task 的执行都会生成记录被 airflow 写入 mysql。 不漏跑 每次执行同步任务需要先确保依赖已经在当前时间之前成功执行 示例代码如下 # 筛选出同步任务依赖的 dag 最近的执行时间 dep_sql = f\"select end_date from task_instance where dag_id='{dag_id}' and task_id='{task_id}' and state='success' order by end_date desc limit 1\" dep_res = get_mysql_dataset( mysql_conn_id=\"airflow_db\", schema=\"airflow\", sql=dep_sql ) print('dep：', dep_sql, dep_res) # 筛选出同步任务最近成功的执行时间 sync_task_id = f\"sync_to_adb_{self.mysql_table}_{self.sync_task_record_id}\" sql = f\"select end_date from task_instanc","date":"2020-05-22","objectID":"/airflow-sync/:3:0","tags":["hive","AnalyticDB","数据同步"],"title":"Hive To AnalyticDB 数据同步方案设计","uri":"/airflow-sync/"},{"categories":null,"content":"存在的问题 当同步任务特别多且在同一时间段触发了较多的同步操作，那么我们的数据源 hive 和 airflow worker 都可能成为瓶颈，影响到数据同步。airflow worker 我们可以通过限制 task concurrency 和增加 worker（worker扩展比较容易）才解决。hive 的并发瓶颈需要我们去思考更好的方案，未来考虑直接从 OSS 同步数据。 ","date":"2020-05-22","objectID":"/airflow-sync/:4:0","tags":["hive","AnalyticDB","数据同步"],"title":"Hive To AnalyticDB 数据同步方案设计","uri":"/airflow-sync/"},{"categories":null,"content":"我们都知道在 Java 中类的实例都是在 heap 中分配内存，也就是说实例对象都是存储在 heap 中。那么类对象是否也存在 heap 中呢? 为了找到这个问题的答案我们使用 HSDB(HotSpot Debugger) 来看看类对象的内存布局。 ","date":"2020-04-22","objectID":"/hsdb-class/:0:0","tags":["java","jvm","HSDB"],"title":"HSDB 告诉我们 Java 对象的内存地址","uri":"/hsdb-class/"},{"categories":null,"content":"背景知识 在此之前我们应该了解有关 Java 对象的两个重要概念 Oop 和 Klass Oop 在 Java 程序运行的过程中，每创建一个新的对象，在 JVM 内部就会相应地创建一个对应类型的 oop（普通对象指针） 对象。各种 oop 类的共同基类为 oopDesc 类。 在 JVM 内部，一个 Java 对象在内存中的布局可以连续分成两部分：对象头（instanceOopDesc） 和实例数据（成员变量）。 instanceOopDesc 对象头包含两部分信息：Mark Word 和 元数据指针(Klass*)： volatile markOop _mark; union _metadata { Klass* _klass; narrowKlass _compressed_klass; } _metadata; Klass 每个Java对象的对象头里，_klass 字段会指向一个VM内部用来记录类的元数据用的 InstanceKlass 对象；InsanceKlass 里有个 _java_mirror 字段，指向该类所对应的Java镜像——java.lang.Class实例。HotSpot VM 会给 Class 对象注入一个隐藏字段 “klass”，用于指回到其对应的 InstanceKlass 对象。这样，klass 与 mirror 之间就有双向引用，可以来回导航。 这个模型里，java.lang.Class 实例并不负责记录真正的类元数据，而只是对VM内部的 InstanceKlass 对象的一个包装供 Java 的反射访问用。 Java object ---\u003e InstanceKlass \u003c---\u003e java.lang.Class instance(java mirror) [_mark] [...] [klass](隐藏字段) [_klass] [_java_mirror] [fileds] [...] ","date":"2020-04-22","objectID":"/hsdb-class/:0:1","tags":["java","jvm","HSDB"],"title":"HSDB 告诉我们 Java 对象的内存地址","uri":"/hsdb-class/"},{"categories":null,"content":"Java 对象探究 示例代码 public class BaseApplication { final static CountDownLatch cd = new CountDownLatch(10); private static int a = 0; private static class Task { private String b = \"task\"; private void method(){ a++; System.out.println(b + a); } } public static void main(String[] args) { Task task = new Task(); task.method(); } } 使用 HSDB 步骤 在 method 内部打上断点，以 debug 模式运行 启动 HSDB (根据自己 jdk 安装路径) sudo java -cp /Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home/lib/sa-jdi.jar sun.jvm.hotspot.HSDB attch to HotSpot process 这一步先使用 jps 获取到 java 进程 id 分析 Java 对象 进入 HSDB 后选择对应的线程，如 main 线程，然后在 Tools tab 里选择 Object Histogram 找到要分析的类 然后双击该类会看到有多少实例被创建出来，选中某一个实例 inspect 我们就能看到这个对象的真正在 JVM 里的构成即我们上面所说的 Oop 可以使用另外一种方式找到 Klass, 在 Tools tab 选择 Class Browser 搜索关键字 现在来看 Task 的实例内存地址 0x00000007959380b8，这个地址位于 JVM 中的内存模型中哪一个区域呢？ 在 Window 命令行中执行 hsdb\u003e universe Heap Parameters: ParallelScavengeHeap [ PSYoungGen [ eden = [0x0000000795580000,0x0000000795a14678,0x0000000797600000] , from = [0x0000000797b00000,0x0000000797b00000,0x0000000798000000] , to = [0x0000000797600000,0x0000000797600000,0x0000000797b00000] ] PSOldGen [ [0x0000000740000000,0x0000000740000000,0x0000000745580000] ] ] 能清楚的看到实例的内存地址 0x00000007959380b8 在 eden 的地址范围之内，所以是实例都是在 JVM 的堆里分配内存。 分析 Class 对象 以示例代码中的 BaseApplication 类为例，我们先找到 InstanceKlass，然后通过 _java_mirror 找到 Class 对象 通过上图和 Class 对象的内存地址 0x0000000795924de0 清楚的看到 Class 对象的地址同样在 eden 的范围之内，即在堆上，并且类的静态成员变量就在 Class 对象中。 指针压缩 在上面的探究过程中，同样发现了指针压缩的证据。在分析实例对象的时候，我们知道对象头有指向 InstanceKlass 的指针，我们先具体看看这个指针的数据是什么样子的？ 通过上图我们看到指针的值是 0x00000000f800c392, 而真实的 InstanceKlass的内存地址是 0x00000007c0061c90 这就是指针压缩的的结果，当开启指针压缩的时候，JVM 按照8字节寻址。 CompressedOops 转换成地址：ObjectAddress64 = BaseAddress64 + 8*CompressedOops JVM 进程可以请求操作系统把堆的基址分配在虚地址为 0 的位置，那么 CompressedOops 转换成地址，就成了：ObjectAddress64 = 8*CompressedOops 也就是说 InstanceKlass 内存地址是对象中的指针左移 3 位可得(32位压缩指针能寻址2^35个字节（即32GB）的地址空间，超过 32GB 则会关闭压缩指针)。转换成二进制可以清楚的看出 # 0x00000000f800c392 (左移3位得到内存地址) 11111000000000001100001110010010 # 0x00000007c0061c90 11111000000000001100001110010010000 所以，如果我们关闭指针压缩，JVM 按照1字节来寻址，那是不是 ObjectAddress64 = CompressedOops 首先加上启动参数，关闭指针压缩 -XX:-UseCompressedOops 通过下图我们看到确实两者一致 ","date":"2020-04-22","objectID":"/hsdb-class/:0:2","tags":["java","jvm","HSDB"],"title":"HSDB 告诉我们 Java 对象的内存地址","uri":"/hsdb-class/"},{"categories":null,"content":"我们都知道在 Java 中类的实例都是在 heap 中分配内存，也就是说实例对象都是存储在 heap 中。那么类对象是否也存在 heap 中呢? 为了找到这个问题的答案我们使用 HSDB(HotSpot Debugger) 来看看类对象的内存布局。 ","date":"2020-04-22","objectID":"/hsdb-class/:0:0","tags":["java","jvm","HSDB"],"title":"HSDB 告诉我们 Java 对象的内存地址","uri":"/hsdb-class/"},{"categories":null,"content":"背景知识 在此之前我们应该了解有关 Java 对象的两个重要概念 Oop 和 Klass Oop 在 Java 程序运行的过程中，每创建一个新的对象，在 JVM 内部就会相应地创建一个对应类型的 oop（普通对象指针） 对象。各种 oop 类的共同基类为 oopDesc 类。 在 JVM 内部，一个 Java 对象在内存中的布局可以连续分成两部分：对象头（instanceOopDesc） 和实例数据（成员变量）。 instanceOopDesc 对象头包含两部分信息：Mark Word 和 元数据指针(Klass*)： volatile markOop _mark; union _metadata { Klass* _klass; narrowKlass _compressed_klass; } _metadata; Klass 每个Java对象的对象头里，_klass 字段会指向一个VM内部用来记录类的元数据用的 InstanceKlass 对象；InsanceKlass 里有个 _java_mirror 字段，指向该类所对应的Java镜像——java.lang.Class实例。HotSpot VM 会给 Class 对象注入一个隐藏字段 “klass”，用于指回到其对应的 InstanceKlass 对象。这样，klass 与 mirror 之间就有双向引用，可以来回导航。 这个模型里，java.lang.Class 实例并不负责记录真正的类元数据，而只是对VM内部的 InstanceKlass 对象的一个包装供 Java 的反射访问用。 Java object ---\u003e InstanceKlass \u003c---\u003e java.lang.Class instance(java mirror) [_mark] [...] [klass](隐藏字段) [_klass] [_java_mirror] [fileds] [...] ","date":"2020-04-22","objectID":"/hsdb-class/:0:1","tags":["java","jvm","HSDB"],"title":"HSDB 告诉我们 Java 对象的内存地址","uri":"/hsdb-class/"},{"categories":null,"content":"Java 对象探究 示例代码 public class BaseApplication { final static CountDownLatch cd = new CountDownLatch(10); private static int a = 0; private static class Task { private String b = \"task\"; private void method(){ a++; System.out.println(b + a); } } public static void main(String[] args) { Task task = new Task(); task.method(); } } 使用 HSDB 步骤 在 method 内部打上断点，以 debug 模式运行 启动 HSDB (根据自己 jdk 安装路径) sudo java -cp /Library/Java/JavaVirtualMachines/jdk1.8.0_171.jdk/Contents/Home/lib/sa-jdi.jar sun.jvm.hotspot.HSDB attch to HotSpot process 这一步先使用 jps 获取到 java 进程 id 分析 Java 对象 进入 HSDB 后选择对应的线程，如 main 线程，然后在 Tools tab 里选择 Object Histogram 找到要分析的类 然后双击该类会看到有多少实例被创建出来，选中某一个实例 inspect 我们就能看到这个对象的真正在 JVM 里的构成即我们上面所说的 Oop 可以使用另外一种方式找到 Klass, 在 Tools tab 选择 Class Browser 搜索关键字 现在来看 Task 的实例内存地址 0x00000007959380b8，这个地址位于 JVM 中的内存模型中哪一个区域呢？ 在 Window 命令行中执行 hsdb\u003e universe Heap Parameters: ParallelScavengeHeap [ PSYoungGen [ eden = [0x0000000795580000,0x0000000795a14678,0x0000000797600000] , from = [0x0000000797b00000,0x0000000797b00000,0x0000000798000000] , to = [0x0000000797600000,0x0000000797600000,0x0000000797b00000] ] PSOldGen [ [0x0000000740000000,0x0000000740000000,0x0000000745580000] ] ] 能清楚的看到实例的内存地址 0x00000007959380b8 在 eden 的地址范围之内，所以是实例都是在 JVM 的堆里分配内存。 分析 Class 对象 以示例代码中的 BaseApplication 类为例，我们先找到 InstanceKlass，然后通过 _java_mirror 找到 Class 对象 通过上图和 Class 对象的内存地址 0x0000000795924de0 清楚的看到 Class 对象的地址同样在 eden 的范围之内，即在堆上，并且类的静态成员变量就在 Class 对象中。 指针压缩 在上面的探究过程中，同样发现了指针压缩的证据。在分析实例对象的时候，我们知道对象头有指向 InstanceKlass 的指针，我们先具体看看这个指针的数据是什么样子的？ 通过上图我们看到指针的值是 0x00000000f800c392, 而真实的 InstanceKlass的内存地址是 0x00000007c0061c90 这就是指针压缩的的结果，当开启指针压缩的时候，JVM 按照8字节寻址。 CompressedOops 转换成地址：ObjectAddress64 = BaseAddress64 + 8*CompressedOops JVM 进程可以请求操作系统把堆的基址分配在虚地址为 0 的位置，那么 CompressedOops 转换成地址，就成了：ObjectAddress64 = 8*CompressedOops 也就是说 InstanceKlass 内存地址是对象中的指针左移 3 位可得(32位压缩指针能寻址2^35个字节（即32GB）的地址空间，超过 32GB 则会关闭压缩指针)。转换成二进制可以清楚的看出 # 0x00000000f800c392 (左移3位得到内存地址) 11111000000000001100001110010010 # 0x00000007c0061c90 11111000000000001100001110010010000 所以，如果我们关闭指针压缩，JVM 按照1字节来寻址，那是不是 ObjectAddress64 = CompressedOops 首先加上启动参数，关闭指针压缩 -XX:-UseCompressedOops 通过下图我们看到确实两者一致 ","date":"2020-04-22","objectID":"/hsdb-class/:0:2","tags":["java","jvm","HSDB"],"title":"HSDB 告诉我们 Java 对象的内存地址","uri":"/hsdb-class/"},{"categories":null,"content":"我们用 Spark 处理数据的时候，Spark 应用和它的 job 运行状态的监控十分重要。关于 Spark 的监控从官方文档上我们看到有这三种方式 Web UI, Metrics, 其他辅助工具。 这里简单提一下 Web UI 提供的监控信息。 Spark Web 每个SparkContext启动一个Web UI， 缺省接口为4040,用来显示应用的有用信息， 包括: scheduler stages 和 tasks列表 RDD 大小和内存使用率的汇总 环境变量(配置) 运行中的 executor 的信息 Spark history server 如果开启了 spark event log ，我们可以通过访问 spark histroy server 看到应用的信息，包括正在运行和已完成的应用。 Web UI 上只提供应用的运行的状态和信息，但是当应用出现异常和失败，我们并不能及时知道。也就是说缺失失败报警的功能。 Spark有一个基于 Dropwizard Metrics Library 的可配置的 metrics 系统，允许用户通过各种方式呈现 metrics， 如 HTTP, JMX 和 CSV 文件方式。我们可以通过 $SPARK_HOME/conf/metrics.properties 配置或者通过 spark.metrics.conf 指定一个自定义的配置文件. Spark metrics 根据不同的组件被解耦成不同的实例。每个实例中都可以可以配置多个 sinks 来接收 metrics。 包括以下实例: master: The Spark standalone master process. applications: A component within the master which reports on various applications. worker: A Spark standalone worker process. executor: A Spark executor. driver: The Spark driver process (the process in which your SparkContext is created). shuffleService: The Spark shuffle service. applicationMaster: The Spark ApplicationMaster when running on YARN. 目前 Spark 支持的 sinks 全部在 org.apache.spark.metrics.sink package 里面，包括： ConsoleSink: 在控制台中显示 metrics. CSVSink: 以 CSV 文件的方式定期提供报告. JmxSink: 以 JMX 方式提供. MetricsServlet: 以 servlet 方式在 Spark UI 中提供 JSON 数据. GraphiteSink: 发送给 Graphite 节点. Slf4jSink: 记录到日志中. StatsdSink: 发送给 Statsd 节点. 我们现在的监控报警系统核心是 Promethues，所以 Spark 的监控报警我们想的方案也是要基于 Prometheus。我们看到 Spark 支持的 sink 里有 jmx, 顺着这个思路我们可以采取 JMXSink + JmxExporter(https://github.com/prometheus/jmx_exporter) 的方式。另外一种思路更直接，我们可以自己开发一个 sink 把 metrics 推到 Prometheus。 在调研的过程中发现别人也有使用 Prometheus 监控 Spark 的需求，于是发现了这样一个包 https://github.com/banzaicloud/spark-metrics，自定义的 Prometheus Sink。 下面介绍一下怎么使用这样的 Prometheus Sink 监控 Spark。 ","date":"2020-03-27","objectID":"/spark-prome/:0:0","tags":["promethues","spark","monitoring"],"title":"怎么使用 Promethues 监控 Spark 应用","uri":"/spark-prome/"},{"categories":null,"content":"Prometheus Sink 配置 因为这里是利用 Prometheus 的 pushgateway，先把 metrics 推给 pushgateway, 所以配置文件里一定要配置 pushgateway 的地址。配置文件就是我们上面所说的 $SPARK_HOME/conf/metrics.properties 或者 spark.metrics.conf 配置的自定义配置文件 metrics.conf # Enable Prometheus for all instances by class name *.sink.prometheus.class=org.apache.spark.banzaicloud.metrics.sink.PrometheusSink # Prometheus pushgateway address *.sink.prometheus.pushgateway-address-protocol=http *.sink.prometheus.pushgateway-address=11.22.33.44:9091 *.sink.prometheus.period=15 #*.sink.prometheus.unit=\u003c unit\u003e - defaults to seconds (TimeUnit.SECONDS) #*.sink.prometheus.pushgateway-enable-timestamp=\u003cenable/disable metrics timestamp\u003e - defaults to false # Metrics name processing (version 2.3-1.1.0 +) #*.sink.prometheus.metrics-name-capture-regex=\u003cregular expression to capture sections metric name sections to be replaces\u003e #*.sink.prometheus.metrics-name-replacement=\u003creplacement captured sections to be replaced with\u003e #*.sink.prometheus.labels=\u003clabels in label=value format separated by comma\u003e # Support for JMX Collector (version 2.3-2.0.0 +) *.sink.prometheus.enable-dropwizard-collector=false *.sink.prometheus.enable-jmx-collector=true *.sink.prometheus.jmx-collector-config=/home/hadoop/jmxCollector.yaml # Enable HostName in Instance instead of Appid (Default value is false i.e. instance=${appid}) *.sink.prometheus.enable-hostname-in-instance=true # Enable JVM metrics source for all instances by class name *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink *.source.jvm.class=org.apache.spark.metrics.source.JvmSource 这里需要特别注意的一点，默认情况下 dirver 和 executor 的 metrics 使用的命名空间是 application id, 例如 metrics_application_1584587365518_0205_1_executor_bytesRead_Count, 但因为这个 app id 是随着每个 Spark app 变的，我们往往关心的是所有 app 的 metrics， 所以这里我们通过配置 sprak.metrics.namespace 来达到目的，可以修改成 app 的名字 ${spark.app.name} 或是一个固定值。 jmxCollector 配置 可以配置收集或不收集哪些对象的指标，这个配置文件可以灵活选择目录。 lowercaseOutputName: false lowercaseOutputLabelNames: false whitelistObjectNames: [\"*:*\"] blacklistObjectNames: [\"java.lang:*\", \"kafka.consumer:*\"] ","date":"2020-03-27","objectID":"/spark-prome/:0:1","tags":["promethues","spark","monitoring"],"title":"怎么使用 Promethues 监控 Spark 应用","uri":"/spark-prome/"},{"categories":null,"content":"依赖 我们知道添加 Spark 的第三方依赖包，我们可以在 spark-submit 通过 –package 指定 例如 spark-submit --master yarn --package com.banzaicloud:spark-metrics_2.11:2.4-1.0.5 --class Test 为了全局生效和省去命令行配置我们把相关的依赖都添加到 Spark 的 CLass Path 下, 这里列出所有需要的依赖包的中央仓库地址。版本可以换根据自己使用的 Spark 版本调整。 https://repo1.maven.org/maven2/com/banzaicloud/spark-metrics_2.11/2.4-1.0.5/spark-metrics_2.11-2.4-1.0.5.jar https://repo1.maven.org/maven2/io/prometheus/simpleclient/0.8.1/simpleclient-0.8.1.jar https://repo1.maven.org/maven2/io/prometheus/simpleclient_dropwizard/0.8.1/simpleclient_dropwizard-0.8.1.jar https://repo1.maven.org/maven2/io/prometheus/simpleclient_pushgateway/0.8.1/simpleclient_pushgateway-0.8.1.jar https://repo1.maven.org/maven2/io/prometheus/simpleclient_common/0.8.1/simpleclient_common-0.8.1.jar https://repo1.maven.org/maven2/io/prometheus/jmx/collector/0.12.0/collector-0.12.0.jar https://repo1.maven.org/maven2/org/yaml/snakeyaml/1.26/snakeyaml-1.26.jar ","date":"2020-03-27","objectID":"/spark-prome/:0:2","tags":["promethues","spark","monitoring"],"title":"怎么使用 Promethues 监控 Spark 应用","uri":"/spark-prome/"},{"categories":null,"content":"Prometheus 报警配置 当我们完成上述步骤后，我们应该就能在 Promethues pushgateway 中看到 spark 应用的 metrics, 那么 prometheus 中自然也会收集到 metrics。 通过其中的一些指标我们可以监控和告警包括 stage, executor, app 等的执行，如果有失败我们就你能及时收到告警，从而能快速处理。 下面是 Prometheus 部分告警规则的示例配置 - name: spark rules: - alert: spark executor fail expr: \"{__name__=~'metrics_application.+applicationMaster_numExecutorsFailed_Value'} \u003e 0\" for: 1m labels: severity: critical annotations: summary: \"spark {{ $labels.__name__ }} 大于 0, 达到 {{ $value }}\" description: \"spark executor 有失败应用\" - alert: spark stage fail expr: metrics_zaihui_driver_DAGScheduler_stage_failedStages_Value \u003e 0 for: 1m labels: severity: critical annotations: summary: \"spark 应用 {{ $labels.app_name }} failed stage 达到 {{ $value }}\" description: \"spark 应用有 stage 失败\" - alert: spark app run fail expr: delta(metrics_zaihui_driver_LiveListenerBus_listenerProcessingTime_org_apache_spark_HeartbeatReceiver_Count[5m]) == 0 for: 5m labels: severity: critical group: alert-admin annotations: summary: \"spark 应用 {{ $labels.app_name }} 5min 之内没有心跳，应用可能已经挂掉\" description: \"spark 应用挂掉，请注意\" ","date":"2020-03-27","objectID":"/spark-prome/:0:3","tags":["promethues","spark","monitoring"],"title":"怎么使用 Promethues 监控 Spark 应用","uri":"/spark-prome/"},{"categories":null,"content":"我们用 Spark 处理数据的时候，Spark 应用和它的 job 运行状态的监控十分重要。关于 Spark 的监控从官方文档上我们看到有这三种方式 Web UI, Metrics, 其他辅助工具。 这里简单提一下 Web UI 提供的监控信息。 Spark Web 每个SparkContext启动一个Web UI， 缺省接口为4040,用来显示应用的有用信息， 包括: scheduler stages 和 tasks列表 RDD 大小和内存使用率的汇总 环境变量(配置) 运行中的 executor 的信息 Spark history server 如果开启了 spark event log ，我们可以通过访问 spark histroy server 看到应用的信息，包括正在运行和已完成的应用。 Web UI 上只提供应用的运行的状态和信息，但是当应用出现异常和失败，我们并不能及时知道。也就是说缺失失败报警的功能。 Spark有一个基于 Dropwizard Metrics Library 的可配置的 metrics 系统，允许用户通过各种方式呈现 metrics， 如 HTTP, JMX 和 CSV 文件方式。我们可以通过 $SPARK_HOME/conf/metrics.properties 配置或者通过 spark.metrics.conf 指定一个自定义的配置文件. Spark metrics 根据不同的组件被解耦成不同的实例。每个实例中都可以可以配置多个 sinks 来接收 metrics。 包括以下实例: master: The Spark standalone master process. applications: A component within the master which reports on various applications. worker: A Spark standalone worker process. executor: A Spark executor. driver: The Spark driver process (the process in which your SparkContext is created). shuffleService: The Spark shuffle service. applicationMaster: The Spark ApplicationMaster when running on YARN. 目前 Spark 支持的 sinks 全部在 org.apache.spark.metrics.sink package 里面，包括： ConsoleSink: 在控制台中显示 metrics. CSVSink: 以 CSV 文件的方式定期提供报告. JmxSink: 以 JMX 方式提供. MetricsServlet: 以 servlet 方式在 Spark UI 中提供 JSON 数据. GraphiteSink: 发送给 Graphite 节点. Slf4jSink: 记录到日志中. StatsdSink: 发送给 Statsd 节点. 我们现在的监控报警系统核心是 Promethues，所以 Spark 的监控报警我们想的方案也是要基于 Prometheus。我们看到 Spark 支持的 sink 里有 jmx, 顺着这个思路我们可以采取 JMXSink + JmxExporter(https://github.com/prometheus/jmx_exporter) 的方式。另外一种思路更直接，我们可以自己开发一个 sink 把 metrics 推到 Prometheus。 在调研的过程中发现别人也有使用 Prometheus 监控 Spark 的需求，于是发现了这样一个包 https://github.com/banzaicloud/spark-metrics，自定义的 Prometheus Sink。 下面介绍一下怎么使用这样的 Prometheus Sink 监控 Spark。 ","date":"2020-03-27","objectID":"/spark-prome/:0:0","tags":["promethues","spark","monitoring"],"title":"怎么使用 Promethues 监控 Spark 应用","uri":"/spark-prome/"},{"categories":null,"content":"Prometheus Sink 配置 因为这里是利用 Prometheus 的 pushgateway，先把 metrics 推给 pushgateway, 所以配置文件里一定要配置 pushgateway 的地址。配置文件就是我们上面所说的 $SPARK_HOME/conf/metrics.properties 或者 spark.metrics.conf 配置的自定义配置文件 metrics.conf # Enable Prometheus for all instances by class name *.sink.prometheus.class=org.apache.spark.banzaicloud.metrics.sink.PrometheusSink # Prometheus pushgateway address *.sink.prometheus.pushgateway-address-protocol=http *.sink.prometheus.pushgateway-address=11.22.33.44:9091 *.sink.prometheus.period=15 #*.sink.prometheus.unit=\u003c unit\u003e - defaults to seconds (TimeUnit.SECONDS) #*.sink.prometheus.pushgateway-enable-timestamp=\u003cenable/disable metrics timestamp\u003e - defaults to false # Metrics name processing (version 2.3-1.1.0 +) #*.sink.prometheus.metrics-name-capture-regex=\u003cregular expression to capture sections metric name sections to be replaces\u003e #*.sink.prometheus.metrics-name-replacement=\u003creplacement captured sections to be replaced with\u003e #*.sink.prometheus.labels=\u003clabels in label=value format separated by comma\u003e # Support for JMX Collector (version 2.3-2.0.0 +) *.sink.prometheus.enable-dropwizard-collector=false *.sink.prometheus.enable-jmx-collector=true *.sink.prometheus.jmx-collector-config=/home/hadoop/jmxCollector.yaml # Enable HostName in Instance instead of Appid (Default value is false i.e. instance=${appid}) *.sink.prometheus.enable-hostname-in-instance=true # Enable JVM metrics source for all instances by class name *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink *.source.jvm.class=org.apache.spark.metrics.source.JvmSource 这里需要特别注意的一点，默认情况下 dirver 和 executor 的 metrics 使用的命名空间是 application id, 例如 metrics_application_1584587365518_0205_1_executor_bytesRead_Count, 但因为这个 app id 是随着每个 Spark app 变的，我们往往关心的是所有 app 的 metrics， 所以这里我们通过配置 sprak.metrics.namespace 来达到目的，可以修改成 app 的名字 ${spark.app.name} 或是一个固定值。 jmxCollector 配置 可以配置收集或不收集哪些对象的指标，这个配置文件可以灵活选择目录。 lowercaseOutputName: false lowercaseOutputLabelNames: false whitelistObjectNames: [\"*:*\"] blacklistObjectNames: [\"java.lang:*\", \"kafka.consumer:*\"] ","date":"2020-03-27","objectID":"/spark-prome/:0:1","tags":["promethues","spark","monitoring"],"title":"怎么使用 Promethues 监控 Spark 应用","uri":"/spark-prome/"},{"categories":null,"content":"依赖 我们知道添加 Spark 的第三方依赖包，我们可以在 spark-submit 通过 –package 指定 例如 spark-submit --master yarn --package com.banzaicloud:spark-metrics_2.11:2.4-1.0.5 --class Test 为了全局生效和省去命令行配置我们把相关的依赖都添加到 Spark 的 CLass Path 下, 这里列出所有需要的依赖包的中央仓库地址。版本可以换根据自己使用的 Spark 版本调整。 https://repo1.maven.org/maven2/com/banzaicloud/spark-metrics_2.11/2.4-1.0.5/spark-metrics_2.11-2.4-1.0.5.jar https://repo1.maven.org/maven2/io/prometheus/simpleclient/0.8.1/simpleclient-0.8.1.jar https://repo1.maven.org/maven2/io/prometheus/simpleclient_dropwizard/0.8.1/simpleclient_dropwizard-0.8.1.jar https://repo1.maven.org/maven2/io/prometheus/simpleclient_pushgateway/0.8.1/simpleclient_pushgateway-0.8.1.jar https://repo1.maven.org/maven2/io/prometheus/simpleclient_common/0.8.1/simpleclient_common-0.8.1.jar https://repo1.maven.org/maven2/io/prometheus/jmx/collector/0.12.0/collector-0.12.0.jar https://repo1.maven.org/maven2/org/yaml/snakeyaml/1.26/snakeyaml-1.26.jar ","date":"2020-03-27","objectID":"/spark-prome/:0:2","tags":["promethues","spark","monitoring"],"title":"怎么使用 Promethues 监控 Spark 应用","uri":"/spark-prome/"},{"categories":null,"content":"Prometheus 报警配置 当我们完成上述步骤后，我们应该就能在 Promethues pushgateway 中看到 spark 应用的 metrics, 那么 prometheus 中自然也会收集到 metrics。 通过其中的一些指标我们可以监控和告警包括 stage, executor, app 等的执行，如果有失败我们就你能及时收到告警，从而能快速处理。 下面是 Prometheus 部分告警规则的示例配置 - name: spark rules: - alert: spark executor fail expr: \"{__name__=~'metrics_application.+applicationMaster_numExecutorsFailed_Value'} \u003e 0\" for: 1m labels: severity: critical annotations: summary: \"spark {{ $labels.__name__ }} 大于 0, 达到 {{ $value }}\" description: \"spark executor 有失败应用\" - alert: spark stage fail expr: metrics_zaihui_driver_DAGScheduler_stage_failedStages_Value \u003e 0 for: 1m labels: severity: critical annotations: summary: \"spark 应用 {{ $labels.app_name }} failed stage 达到 {{ $value }}\" description: \"spark 应用有 stage 失败\" - alert: spark app run fail expr: delta(metrics_zaihui_driver_LiveListenerBus_listenerProcessingTime_org_apache_spark_HeartbeatReceiver_Count[5m]) == 0 for: 5m labels: severity: critical group: alert-admin annotations: summary: \"spark 应用 {{ $labels.app_name }} 5min 之内没有心跳，应用可能已经挂掉\" description: \"spark 应用挂掉，请注意\" ","date":"2020-03-27","objectID":"/spark-prome/:0:3","tags":["promethues","spark","monitoring"],"title":"怎么使用 Promethues 监控 Spark 应用","uri":"/spark-prome/"},{"categories":null,"content":"Apache Zeppelin 是一个可以通过浏览器实现交互式数据查询、数据分析、数据可视化并且能多人协作开发的 NoteBook(如果用过 jupyter 对此应该会很熟悉)。其前端提供丰富的可视化图形库，不限于 SparkSQL，后端支持 HBase、Flink 等大数据系统以插件扩展的方式，并支持 Spark、Python、JDBC、Markdown、Shell 等各种常用 Interpreter，这使得开发者可以方便地使用 SQL，SCALA, Python 等其他语言 在 Zeppelin 中做数据开发。 我们在用 Scala 基于 Spark 做数据开发的时候，代码调试是必不可少的，这个过程可能是反复多次的，最基本的要求就是我们得有完整的 Spark 环境。随之而来的是需要在本地搭建和维护这样的环境，当我们有实时数据的需求的时候可能会需要用到其他的组件比如 kafka。本地环境和不方便就会成为开发的一个大问题。此外很多场景我们可能只是需要调试一段代码或一个小的模块，如果我们想达到这个“块级别”的调试不借助别的工具就很难实现。 在这里我们介绍一下如何使用 Zeppelin 来解决上述的痛点，帮助我们有效提高开发效率。 ","date":"2020-02-25","objectID":"/zeppelin-intellij/:0:0","tags":["数据开发","zeppelin","spark"],"title":"在 IntelliJ 中使用 Zeppelin 提速数据开发","uri":"/zeppelin-intellij/"},{"categories":null,"content":"Zeppelin 原理 在此之前，我们有必要了解 Zeppelin 的工作原理。我们最关心的是两个模块 Notebook 和 Interpreter. Notebook Notebook 可以理解成多个代码片段的交互式容器 一个 Notebook 可以包含不同语言写成的代码片段 Notebook 可以导入导出，对于一个 Notebook下的所有代码片段可以一起执行、停止和删除的操作 Interpreter 解释器是 Zeppelin 的语言后端。例如，在 Zeppelin 中使用 scala 代码，您需要一个 scala 解释器、使用 Python 需要 Python 的解释器，每个解释器都属于一个解释器组。相同解释器组中的解释器可以互相引用。例如 Spark、SparkSQL、Pyspark、SparkR 都属于同一个解释器组，同一个实例中他们之间可以相互引用，RDD 也可以共享，在用户发起执行请求时 Zeppelin Server 会通过代码中的 repl 找到相应的解释器，调用解释器的 interpreter 方法执行，Zeppelin Server 会通过 WebSocket 实时的把进度反馈到web前端。 架构图如下 解释器有两种启动形式，本地或者远程，本地解释器通过 classLoader 动态加载到 JVM 直接调用，远程解释器会在本地重启一个进程，启动一个Thrift Server 通过 Thrift 远程调用,以 Spark 插件为例, Spark 插件本身就是一个常驻的 Spark 任务，在 Spark 的 Driver 端启动一个 Thirft Server，然后通过 Thirft 和 Zeppelin Server 进行通信,接受执行 SQL、Scala、Python等任务、反馈执行进度等。 ","date":"2020-02-25","objectID":"/zeppelin-intellij/:0:1","tags":["数据开发","zeppelin","spark"],"title":"在 IntelliJ 中使用 Zeppelin 提速数据开发","uri":"/zeppelin-intellij/"},{"categories":null,"content":"Zeppelin 依赖配置 我们想要让代码跑起来，代码所用到依赖必须在 Zeppelin 中先配置好。这里有两种方式可以来加载依赖包。(以 Spark interpreter 示例), 包括如下功能 从 maven 仓库加载 从本地文件系统加载 添加指定的仓库地址 通过 %sprak.dep 动态加载 当我们的代码需要用到第三方库时,不需要手动下载包和重启 Zepplin, 可以通过 spark.dep实现动态加载 使用示例如下 ``` %spark.dep z.reset() // clean up previously added artifact and repository // add maven repository z.addRepo(\"RepoName\").url(\"RepoURL\") // add maven snapshot repository z.addRepo(\"RepoName\").url(\"RepoURL\").snapshot() // add credentials for private maven repository z.addRepo(\"RepoName\").url(\"RepoURL\").username(\"username\").password(\"password\") // add artifact from filesystem z.load(\"/path/to.jar\") // add artifact from maven repository, with no dependency z.load(\"groupId:artifactId:version\").excludeAll() // add artifact recursively z.load(\"groupId:artifactId:version\") // add artifact recursively except comma separated GroupID:ArtifactId list z.load(\"groupId:artifactId:version\").exclude(\"groupId:artifactId,groupId:artifactId, ...\") // exclude with pattern z.load(\"groupId:artifactId:version\").exclude(*) z.load(\"groupId:artifactId:version\").exclude(\"groupId:artifactId:*\") z.load(\"groupId:artifactId:version\").exclude(\"groupId:*\") // local() skips adding artifact to spark clusters (skipping sc.addJar()) z.load(\"groupId:artifactId:version\").local() ``` Web UI 通过 Interpreter 设置 配置依赖的源 加载依赖到相应的 interpreter 配置包的 artifact、group和版本信息或者本地路径 ","date":"2020-02-25","objectID":"/zeppelin-intellij/:0:2","tags":["数据开发","zeppelin","spark"],"title":"在 IntelliJ 中使用 Zeppelin 提速数据开发","uri":"/zeppelin-intellij/"},{"categories":null,"content":"Intellij Big Data Tools 在依赖配置好后，我们就可以创建 Notebook 开始开发了，但是有一点遗憾在 Zeppelin 的 Notebook 中编写代码并没有 IDE 中的代码提示等帮助我们提高开发效率的功能。所幸的是 IntelliJ IDEA 2019.2 及以后的版本提供了插件 Big Data tools。这个插件集成了 Zeppelin, Spark等，我们使用 Zeppelin 搭配 Big data tools 插件即可以实现高效的本地数据开发和调试。 环境配置 Intellij 安装插件 Big Data Tools Scala 创建 project 配置 Zeppelin 连接 ","date":"2020-02-25","objectID":"/zeppelin-intellij/:0:3","tags":["数据开发","zeppelin","spark"],"title":"在 IntelliJ 中使用 Zeppelin 提速数据开发","uri":"/zeppelin-intellij/"},{"categories":null,"content":"开发示例 创建 Notebook 和调试 输出结果 ","date":"2020-02-25","objectID":"/zeppelin-intellij/:0:4","tags":["数据开发","zeppelin","spark"],"title":"在 IntelliJ 中使用 Zeppelin 提速数据开发","uri":"/zeppelin-intellij/"},{"categories":null,"content":"Apache Zeppelin 是一个可以通过浏览器实现交互式数据查询、数据分析、数据可视化并且能多人协作开发的 NoteBook(如果用过 jupyter 对此应该会很熟悉)。其前端提供丰富的可视化图形库，不限于 SparkSQL，后端支持 HBase、Flink 等大数据系统以插件扩展的方式，并支持 Spark、Python、JDBC、Markdown、Shell 等各种常用 Interpreter，这使得开发者可以方便地使用 SQL，SCALA, Python 等其他语言 在 Zeppelin 中做数据开发。 我们在用 Scala 基于 Spark 做数据开发的时候，代码调试是必不可少的，这个过程可能是反复多次的，最基本的要求就是我们得有完整的 Spark 环境。随之而来的是需要在本地搭建和维护这样的环境，当我们有实时数据的需求的时候可能会需要用到其他的组件比如 kafka。本地环境和不方便就会成为开发的一个大问题。此外很多场景我们可能只是需要调试一段代码或一个小的模块，如果我们想达到这个“块级别”的调试不借助别的工具就很难实现。 在这里我们介绍一下如何使用 Zeppelin 来解决上述的痛点，帮助我们有效提高开发效率。 ","date":"2020-02-25","objectID":"/zeppelin-intellij/:0:0","tags":["数据开发","zeppelin","spark"],"title":"在 IntelliJ 中使用 Zeppelin 提速数据开发","uri":"/zeppelin-intellij/"},{"categories":null,"content":"Zeppelin 原理 在此之前，我们有必要了解 Zeppelin 的工作原理。我们最关心的是两个模块 Notebook 和 Interpreter. Notebook Notebook 可以理解成多个代码片段的交互式容器 一个 Notebook 可以包含不同语言写成的代码片段 Notebook 可以导入导出，对于一个 Notebook下的所有代码片段可以一起执行、停止和删除的操作 Interpreter 解释器是 Zeppelin 的语言后端。例如，在 Zeppelin 中使用 scala 代码，您需要一个 scala 解释器、使用 Python 需要 Python 的解释器，每个解释器都属于一个解释器组。相同解释器组中的解释器可以互相引用。例如 Spark、SparkSQL、Pyspark、SparkR 都属于同一个解释器组，同一个实例中他们之间可以相互引用，RDD 也可以共享，在用户发起执行请求时 Zeppelin Server 会通过代码中的 repl 找到相应的解释器，调用解释器的 interpreter 方法执行，Zeppelin Server 会通过 WebSocket 实时的把进度反馈到web前端。 架构图如下 解释器有两种启动形式，本地或者远程，本地解释器通过 classLoader 动态加载到 JVM 直接调用，远程解释器会在本地重启一个进程，启动一个Thrift Server 通过 Thrift 远程调用,以 Spark 插件为例, Spark 插件本身就是一个常驻的 Spark 任务，在 Spark 的 Driver 端启动一个 Thirft Server，然后通过 Thirft 和 Zeppelin Server 进行通信,接受执行 SQL、Scala、Python等任务、反馈执行进度等。 ","date":"2020-02-25","objectID":"/zeppelin-intellij/:0:1","tags":["数据开发","zeppelin","spark"],"title":"在 IntelliJ 中使用 Zeppelin 提速数据开发","uri":"/zeppelin-intellij/"},{"categories":null,"content":"Zeppelin 依赖配置 我们想要让代码跑起来，代码所用到依赖必须在 Zeppelin 中先配置好。这里有两种方式可以来加载依赖包。(以 Spark interpreter 示例), 包括如下功能 从 maven 仓库加载 从本地文件系统加载 添加指定的仓库地址 通过 %sprak.dep 动态加载 当我们的代码需要用到第三方库时,不需要手动下载包和重启 Zepplin, 可以通过 spark.dep实现动态加载 使用示例如下 ``` %spark.dep z.reset() // clean up previously added artifact and repository // add maven repository z.addRepo(\"RepoName\").url(\"RepoURL\") // add maven snapshot repository z.addRepo(\"RepoName\").url(\"RepoURL\").snapshot() // add credentials for private maven repository z.addRepo(\"RepoName\").url(\"RepoURL\").username(\"username\").password(\"password\") // add artifact from filesystem z.load(\"/path/to.jar\") // add artifact from maven repository, with no dependency z.load(\"groupId:artifactId:version\").excludeAll() // add artifact recursively z.load(\"groupId:artifactId:version\") // add artifact recursively except comma separated GroupID:ArtifactId list z.load(\"groupId:artifactId:version\").exclude(\"groupId:artifactId,groupId:artifactId, ...\") // exclude with pattern z.load(\"groupId:artifactId:version\").exclude(*) z.load(\"groupId:artifactId:version\").exclude(\"groupId:artifactId:*\") z.load(\"groupId:artifactId:version\").exclude(\"groupId:*\") // local() skips adding artifact to spark clusters (skipping sc.addJar()) z.load(\"groupId:artifactId:version\").local() ``` Web UI 通过 Interpreter 设置 配置依赖的源 加载依赖到相应的 interpreter 配置包的 artifact、group和版本信息或者本地路径 ","date":"2020-02-25","objectID":"/zeppelin-intellij/:0:2","tags":["数据开发","zeppelin","spark"],"title":"在 IntelliJ 中使用 Zeppelin 提速数据开发","uri":"/zeppelin-intellij/"},{"categories":null,"content":"Intellij Big Data Tools 在依赖配置好后，我们就可以创建 Notebook 开始开发了，但是有一点遗憾在 Zeppelin 的 Notebook 中编写代码并没有 IDE 中的代码提示等帮助我们提高开发效率的功能。所幸的是 IntelliJ IDEA 2019.2 及以后的版本提供了插件 Big Data tools。这个插件集成了 Zeppelin, Spark等，我们使用 Zeppelin 搭配 Big data tools 插件即可以实现高效的本地数据开发和调试。 环境配置 Intellij 安装插件 Big Data Tools Scala 创建 project 配置 Zeppelin 连接 ","date":"2020-02-25","objectID":"/zeppelin-intellij/:0:3","tags":["数据开发","zeppelin","spark"],"title":"在 IntelliJ 中使用 Zeppelin 提速数据开发","uri":"/zeppelin-intellij/"},{"categories":null,"content":"开发示例 创建 Notebook 和调试 输出结果 ","date":"2020-02-25","objectID":"/zeppelin-intellij/:0:4","tags":["数据开发","zeppelin","spark"],"title":"在 IntelliJ 中使用 Zeppelin 提速数据开发","uri":"/zeppelin-intellij/"},{"categories":null,"content":"在使用 MySQL 和存储引擎是 InnoDB 的情况下，当我们想从一个 SQL 语句分析出这个语句对应的加锁情况需要掌握哪些知识呢？ 在这篇文章我就想总结一下 InnoDB 的锁类型和这些不同的锁用在什么场景。 影响加锁的因素有这几个： 事务隔离级别 事务的隔离级别以及他们能解决的事务问题这里就不做介绍了，这里我们只针对常用的 Read Committed, Repeatable Read 分析。 SQL 语句本身(一致性非锁定读还是一致性锁定读；DML(update,insert,delete)) 普通的 SELECT 对应的就是一致性非锁定读，即不需要加锁，InnoDB 使用 MVCC(multiversion concurrency control)来增加读操作的并发性, MVCC是指，InnoDB 使用基于时间点的快照来获取查询结果，读取时在访问的表上不加锁，因此，在事务T1读取的同一时刻，事务 T2 可以自由的修改事务 T1 所读取的数据。 SELECT for share 和 SELECT for update 对应的则是锁定读，即在搜索到的每条索引记录(index record) 加共享锁或排他锁。 使用的索引类型(主键索引，唯一索引，非唯一索引，没有用到索引) ","date":"2020-01-20","objectID":"/mysql-lock/:0:0","tags":["mysql","innodb","事务"],"title":"MySQL InnoDB 加锁实践分析","uri":"/mysql-lock/"},{"categories":null,"content":"InnoDB 锁类型 InnoDB 一共有8种锁类型，其中，意向锁(Intention Locks)和自增锁(AUTO-INC Locks)是表级锁，剩余全部都是行级锁。此外，共享锁或排它锁(Shared and Exclusive Locks)尽管也作为8种锁类型之一，它却并不是具体的锁，它是锁的模式，用来“修饰”其他各种类型的锁。 共享锁或排它锁(Shared and Exclusive Locks) 它并不是一种锁的类型，而是其他各种锁的模式，每种锁都有shard或exclusive两种模式。 S 锁和 X 锁兼容情况如下 假设 T1 持有数据行 r 上的 S 锁，则当 T2 请求 r 上的锁时： T2 请求 r 上的 S 锁，则，T2 立即获得 S 锁。T1 和T2 同时都持有 r 上的 S 锁。 T2 请求 r 上的 X 锁，则，T2 无法获得 X 锁。T2 必须要等待直到 T1 释放 r 上的 S 锁。 假设 T1 持有 r 上的X锁，则当 T2 请求 r 上的锁时： T2 请求 r 上的任何类型的锁时，T2 都无法获得锁，此时，T2 必须要等待直到 T1 释放 r 上的 X 锁 意向锁(Intention Locks) 表锁。含义是已经持有了表锁，稍候将获取该表上某个/些行的行锁。有 shard 或 exclusive 两种模式。 LOCK_MODE分别是：IS 或 IX 意向锁的目的是告知其他事务，某事务已经锁定了或即将锁定某个/些数据行。事务在获取行锁之前，首先要获取到意向锁，即： 事务在获取行上的 S 锁之前，事务必须首先获取 表上的 IS 锁或表上的更强的锁。 事务在获取行上的 X 锁之前，事务必须首先获取 表上的 IX 锁。 意向锁和任何行锁都是兼容的，它只会阻塞全表请求(如 lock tables…write) 索引记录锁(Record Locks) 即行锁，在某个索引的特定索引记录（或称索引条目、索引项、索引入口）上设置锁。有 shard 或 exclusive 两种模式。 LOCK_MODE分别是：S,REC_NOT_GAP 或 X,REC_NOT_GAP 间隙锁(Gap Locks) 索引记录之间的间隙上的锁，锁定尚未存在的记录，即索引记录之间的间隙。有 shard 或 exclusive 两种模式，二者等价。 LOCK_MODE分别是：S,GAP 或 X,GAP gap lock 锁住的间隙可以是第一个索引记录前面的间隙，或相邻两条索引记录之间的间隙，或最后一个索引记录后面的间隙。 gap lock 存在的唯一目的就是阻止其他事务向gap中插入数据行，它用于在隔离级别为RR时，阻止幻影行(phantom row)的产生；隔离级别为RC时，搜索和索引扫描时，gap lock 是被禁用的，只在外键约束检查和重复 key 检查时 gap lock 才有效，正是因为此，RC 时会有幻影行问题。 Next-Key Locks next-key lock 是 (索引记录上的索引记录锁) + (该索引记录前面的间隙上的锁) 二者的合体，它锁定索引记录以及该索引记录前面的间隙。有 shard 或e xclusive两种模式。 LOCK_MODE分别是：S 或 X 当 InnoDB 搜索或扫描索引时，InnoDB 在它遇到的索引记录上所设置的锁就是 next-key lock，它会锁定索引记录本身以及该索引记录前面的 gap(“gap” immediately before that index record)。即：如果事务T1 在索引记录r 上有一个next-key lock，则 T2 无法在 紧靠着 r 前面的那个间隙中插入新的索引记录(gap immediately before r in the index order)。 next-key lock 还会加在 supremum pseudo-record(索引中的伪记录(pseudo-record)，代表此索引中可能存在的最大值) 上，该 next-key lock实际上锁定了“此索引中可能存在的最大值”前面的间隙，也就是此索引中当前实际存在的最大值后面的间隙。 插入意向锁(Insert Intention Locks) INSERT 操作插入成功后，会在新插入的行上设置 index record lock，但在插入行之前，INSERT 操作会首先在索引记录之间的间隙上设置 insert intention lock，该锁的范围是(插入值, 向下的一个索引值)。有 shard 或 exclusive 两种模式，二者等价。 LOCK_MODE分别是：S,GAP,INSERT_INTENTION 或 X,GAP,INSERT_INTENTION insert intention lock 相互不会阻塞，即多个事务向同一个 index gap 并发进行插入时，多个事务无需相互等待，极大提高了插入的并发性。那么为什么需要 insert intenton lock 呢？答案就是上面我们提到的 gap lock 或 next-key lock 会阻塞 insert intention lock, 这个特性解决了 RR 隔离级别中的 phantom row。 自增锁(AUTO-INC Locks) 表锁。向带有AUTO_INCREMENT列 的表时插入数据行时，事务需要首先获取到该表的AUTO-INC表级锁，以便可以生成连续的自增值。插入语句开始时请求该锁，插入语句结束后释放该锁(不是事务结束) 我们日常开发中大部分场景都是 simple-inserts, 即待插入的条数提前可以确定，那么自增值的个数也可以别提前确定就不需要 auto-inc 锁。 空间索引(Predicate Locks for Spatial Indexes) 平时很少用到，这里不涉及 ","date":"2020-01-20","objectID":"/mysql-lock/:1:0","tags":["mysql","innodb","事务"],"title":"MySQL InnoDB 加锁实践分析","uri":"/mysql-lock/"},{"categories":null,"content":"加锁分析 这里使用的环境是 MySQL 8.0.19 测试表如下 CREATE TABLE `test` ( `id` int NOT NULL AUTO_INCREMENT, `a` varchar(10) DEFAULT NULL, `b` varchar(10) DEFAULT NULL, `n` int NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `index_n` (`n`), KEY `index_b` (`b`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci 没有使用索引，全表扫描 这种情况下，InnoDB 会在所有记录上加行锁 mysql\u003e begin; Query OK, 0 rows affected (0.00 sec) mysql\u003e select * from test where a='a' for update; Empty set (0.00 sec) # 查看加锁情况，结果如下 # 可以看到全部记录加了 next-key lock(X) +----------+------------+-----------+-----------+-------------+------------------------+ | trans_id | index_name | lock_type | lock_mode | lock_status | lock_data | +----------+------------+-----------+-----------+-------------+------------------------+ | 2091 | NULL | TABLE | IX | GRANTED | NULL | | 2091 | PRIMARY | RECORD | X | GRANTED | supremum pseudo-record | | 2091 | PRIMARY | RECORD | X | GRANTED | 1 | | 2091 | PRIMARY | RECORD | X | GRANTED | 2 | +----------+------------+-----------+-----------+-------------+------------------------+ 上面看到了，查询条件无法使用索引则不得不全部行加锁，可见索引的重要性。 使用了唯一索引 首先如果是等值查询，只需要索引记录锁(index record lock) mysql\u003e begin; Query OK, 0 rows affected (0.00 sec) mysql\u003e select * from test where n=5 for update; +----+------+------+---+ | id | a | b | n | +----+------+------+---+ | 1 | s | ss | 5 | +----+------+------+---+ 1 row in set (0.00 sec) # 查看加锁情况，结果如下 # 在 index_n 上只加了 index record lock(X,REC_NOT_GAP) +----------+------------+-----------+---------------+-------------+-----------+ | trans_id | index_name | lock_type | lock_mode | lock_status | lock_data | +----------+------------+-----------+---------------+-------------+-----------+ | 2116 | NULL | TABLE | IX | GRANTED | NULL | | 2116 | index_n | RECORD | X,REC_NOT_GAP | GRANTED | 5, 1 | | 2116 | PRIMARY | RECORD | X,REC_NOT_GAP | GRANTED | 1 | +----------+------------+-----------+---------------+-------------+-----------+ 范围查询，对符合条件的每条记录加 next-key lock 和在第一个不满足条件的索引记录上设置next-key lock # 目前表中的所有记录 mysql\u003e select * from test; +----+------+------+----+ | id | a | b | n | +----+------+------+----+ | 1 | s | ss | 5 | | 2 | d | dd | 10 | | 4 | g | gg | 20 | | 6 | l | ll | 50 | +----+------+------+----+ mysql\u003e select * from test where n \u003e 1 and n \u003c=15 for update; +----+------+------+----+ | id | a | b | n | +----+------+------+----+ | 1 | s | ss | 5 | | 2 | d | dd | 10 | +----+------+------+----+ # 加锁情况如下 # 可以看到符合条件(5，10)的记录都被加上了 next-key lock(X)， 不符合条件的第一条(20)也加上了 next-key lock +----------+------------+-----------+---------------+-------------+-----------+ | trans_id | index_name | lock_type | lock_mode | lock_status | lock_data | +----------+------------+-----------+---------------+-------------+-----------+ | 2119 | NULL | TABLE | IX | GRANTED | NULL | | 2119 | index_n | RECORD | X | GRANTED | 5, 1 | | 2119 | index_n | RECORD | X | GRANTED | 10, 2 | | 2119 | index_n | RECORD | X | GRANTED | 20, 4 | | 2119 | PRIMARY | RECORD | X,REC_NOT_GAP | GRANTED | 1 | | 2119 | PRIMARY | RECORD | X,REC_NOT_GAP | GRANTED | 2 | +----------+------------+-----------+---------------+-------------+-----------+ 使用非唯一索引 InnoDB 会锁住索引本身，还会锁住索引记录前面的间隙会对所有满足条件的记录 即加 next-key lock。 mysql\u003e select * from test where b \u003e 'gg' for update; +----+------+------+----+ | id | a | b | n | +----+------+------+----+ | 6 | l | ll | 50 | | 1 | s | ss | 5 | +----+------+------+----+ # 加锁情况如下，可以看到在非唯一索引 index_b 加的是 next-key lock +----------+------------+-----------+---------------+-------------+------------------------+ | trans_id | index_name | lock_type | lock_mode | lock_status | lock_data | +----------+------------+-----------+---------------+-------------+------------------------+ | 2159 | NULL | TABLE | IX | GRANTED | NULL | | 2159 | index_b | RECORD | X | GRANTED | supremum pseudo-record | | 2159 | index_b | RECORD | X | GRANTED | 'ss', 1 | | 2159 | index_b | RECORD | X | GRANTED | 'll', 6 | | 2159 | PRIMARY | RECORD | X,REC_NOT_GAP | GRANTED | 1 | | 2159 | PRIMARY | RECORD | X,REC_NOT_GAP | ","date":"2020-01-20","objectID":"/mysql-lock/:2:0","tags":["mysql","innodb","事务"],"title":"MySQL InnoDB 加锁实践分析","uri":"/mysql-lock/"},{"categories":null,"content":"在使用 MySQL 和存储引擎是 InnoDB 的情况下，当我们想从一个 SQL 语句分析出这个语句对应的加锁情况需要掌握哪些知识呢？ 在这篇文章我就想总结一下 InnoDB 的锁类型和这些不同的锁用在什么场景。 影响加锁的因素有这几个： 事务隔离级别 事务的隔离级别以及他们能解决的事务问题这里就不做介绍了，这里我们只针对常用的 Read Committed, Repeatable Read 分析。 SQL 语句本身(一致性非锁定读还是一致性锁定读；DML(update,insert,delete)) 普通的 SELECT 对应的就是一致性非锁定读，即不需要加锁，InnoDB 使用 MVCC(multiversion concurrency control)来增加读操作的并发性, MVCC是指，InnoDB 使用基于时间点的快照来获取查询结果，读取时在访问的表上不加锁，因此，在事务T1读取的同一时刻，事务 T2 可以自由的修改事务 T1 所读取的数据。 SELECT for share 和 SELECT for update 对应的则是锁定读，即在搜索到的每条索引记录(index record) 加共享锁或排他锁。 使用的索引类型(主键索引，唯一索引，非唯一索引，没有用到索引) ","date":"2020-01-20","objectID":"/mysql-lock/:0:0","tags":["mysql","innodb","事务"],"title":"MySQL InnoDB 加锁实践分析","uri":"/mysql-lock/"},{"categories":null,"content":"InnoDB 锁类型 InnoDB 一共有8种锁类型，其中，意向锁(Intention Locks)和自增锁(AUTO-INC Locks)是表级锁，剩余全部都是行级锁。此外，共享锁或排它锁(Shared and Exclusive Locks)尽管也作为8种锁类型之一，它却并不是具体的锁，它是锁的模式，用来“修饰”其他各种类型的锁。 共享锁或排它锁(Shared and Exclusive Locks) 它并不是一种锁的类型，而是其他各种锁的模式，每种锁都有shard或exclusive两种模式。 S 锁和 X 锁兼容情况如下 假设 T1 持有数据行 r 上的 S 锁，则当 T2 请求 r 上的锁时： T2 请求 r 上的 S 锁，则，T2 立即获得 S 锁。T1 和T2 同时都持有 r 上的 S 锁。 T2 请求 r 上的 X 锁，则，T2 无法获得 X 锁。T2 必须要等待直到 T1 释放 r 上的 S 锁。 假设 T1 持有 r 上的X锁，则当 T2 请求 r 上的锁时： T2 请求 r 上的任何类型的锁时，T2 都无法获得锁，此时，T2 必须要等待直到 T1 释放 r 上的 X 锁 意向锁(Intention Locks) 表锁。含义是已经持有了表锁，稍候将获取该表上某个/些行的行锁。有 shard 或 exclusive 两种模式。 LOCK_MODE分别是：IS 或 IX 意向锁的目的是告知其他事务，某事务已经锁定了或即将锁定某个/些数据行。事务在获取行锁之前，首先要获取到意向锁，即： 事务在获取行上的 S 锁之前，事务必须首先获取 表上的 IS 锁或表上的更强的锁。 事务在获取行上的 X 锁之前，事务必须首先获取 表上的 IX 锁。 意向锁和任何行锁都是兼容的，它只会阻塞全表请求(如 lock tables…write) 索引记录锁(Record Locks) 即行锁，在某个索引的特定索引记录（或称索引条目、索引项、索引入口）上设置锁。有 shard 或 exclusive 两种模式。 LOCK_MODE分别是：S,REC_NOT_GAP 或 X,REC_NOT_GAP 间隙锁(Gap Locks) 索引记录之间的间隙上的锁，锁定尚未存在的记录，即索引记录之间的间隙。有 shard 或 exclusive 两种模式，二者等价。 LOCK_MODE分别是：S,GAP 或 X,GAP gap lock 锁住的间隙可以是第一个索引记录前面的间隙，或相邻两条索引记录之间的间隙，或最后一个索引记录后面的间隙。 gap lock 存在的唯一目的就是阻止其他事务向gap中插入数据行，它用于在隔离级别为RR时，阻止幻影行(phantom row)的产生；隔离级别为RC时，搜索和索引扫描时，gap lock 是被禁用的，只在外键约束检查和重复 key 检查时 gap lock 才有效，正是因为此，RC 时会有幻影行问题。 Next-Key Locks next-key lock 是 (索引记录上的索引记录锁) + (该索引记录前面的间隙上的锁) 二者的合体，它锁定索引记录以及该索引记录前面的间隙。有 shard 或e xclusive两种模式。 LOCK_MODE分别是：S 或 X 当 InnoDB 搜索或扫描索引时，InnoDB 在它遇到的索引记录上所设置的锁就是 next-key lock，它会锁定索引记录本身以及该索引记录前面的 gap(“gap” immediately before that index record)。即：如果事务T1 在索引记录r 上有一个next-key lock，则 T2 无法在 紧靠着 r 前面的那个间隙中插入新的索引记录(gap immediately before r in the index order)。 next-key lock 还会加在 supremum pseudo-record(索引中的伪记录(pseudo-record)，代表此索引中可能存在的最大值) 上，该 next-key lock实际上锁定了“此索引中可能存在的最大值”前面的间隙，也就是此索引中当前实际存在的最大值后面的间隙。 插入意向锁(Insert Intention Locks) INSERT 操作插入成功后，会在新插入的行上设置 index record lock，但在插入行之前，INSERT 操作会首先在索引记录之间的间隙上设置 insert intention lock，该锁的范围是(插入值, 向下的一个索引值)。有 shard 或 exclusive 两种模式，二者等价。 LOCK_MODE分别是：S,GAP,INSERT_INTENTION 或 X,GAP,INSERT_INTENTION insert intention lock 相互不会阻塞，即多个事务向同一个 index gap 并发进行插入时，多个事务无需相互等待，极大提高了插入的并发性。那么为什么需要 insert intenton lock 呢？答案就是上面我们提到的 gap lock 或 next-key lock 会阻塞 insert intention lock, 这个特性解决了 RR 隔离级别中的 phantom row。 自增锁(AUTO-INC Locks) 表锁。向带有AUTO_INCREMENT列 的表时插入数据行时，事务需要首先获取到该表的AUTO-INC表级锁，以便可以生成连续的自增值。插入语句开始时请求该锁，插入语句结束后释放该锁(不是事务结束) 我们日常开发中大部分场景都是 simple-inserts, 即待插入的条数提前可以确定，那么自增值的个数也可以别提前确定就不需要 auto-inc 锁。 空间索引(Predicate Locks for Spatial Indexes) 平时很少用到，这里不涉及 ","date":"2020-01-20","objectID":"/mysql-lock/:1:0","tags":["mysql","innodb","事务"],"title":"MySQL InnoDB 加锁实践分析","uri":"/mysql-lock/"},{"categories":null,"content":"加锁分析 这里使用的环境是 MySQL 8.0.19 测试表如下 CREATE TABLE `test` ( `id` int NOT NULL AUTO_INCREMENT, `a` varchar(10) DEFAULT NULL, `b` varchar(10) DEFAULT NULL, `n` int NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `index_n` (`n`), KEY `index_b` (`b`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci 没有使用索引，全表扫描 这种情况下，InnoDB 会在所有记录上加行锁 mysql\u003e begin; Query OK, 0 rows affected (0.00 sec) mysql\u003e select * from test where a='a' for update; Empty set (0.00 sec) # 查看加锁情况，结果如下 # 可以看到全部记录加了 next-key lock(X) +----------+------------+-----------+-----------+-------------+------------------------+ | trans_id | index_name | lock_type | lock_mode | lock_status | lock_data | +----------+------------+-----------+-----------+-------------+------------------------+ | 2091 | NULL | TABLE | IX | GRANTED | NULL | | 2091 | PRIMARY | RECORD | X | GRANTED | supremum pseudo-record | | 2091 | PRIMARY | RECORD | X | GRANTED | 1 | | 2091 | PRIMARY | RECORD | X | GRANTED | 2 | +----------+------------+-----------+-----------+-------------+------------------------+ 上面看到了，查询条件无法使用索引则不得不全部行加锁，可见索引的重要性。 使用了唯一索引 首先如果是等值查询，只需要索引记录锁(index record lock) mysql\u003e begin; Query OK, 0 rows affected (0.00 sec) mysql\u003e select * from test where n=5 for update; +----+------+------+---+ | id | a | b | n | +----+------+------+---+ | 1 | s | ss | 5 | +----+------+------+---+ 1 row in set (0.00 sec) # 查看加锁情况，结果如下 # 在 index_n 上只加了 index record lock(X,REC_NOT_GAP) +----------+------------+-----------+---------------+-------------+-----------+ | trans_id | index_name | lock_type | lock_mode | lock_status | lock_data | +----------+------------+-----------+---------------+-------------+-----------+ | 2116 | NULL | TABLE | IX | GRANTED | NULL | | 2116 | index_n | RECORD | X,REC_NOT_GAP | GRANTED | 5, 1 | | 2116 | PRIMARY | RECORD | X,REC_NOT_GAP | GRANTED | 1 | +----------+------------+-----------+---------------+-------------+-----------+ 范围查询，对符合条件的每条记录加 next-key lock 和在第一个不满足条件的索引记录上设置next-key lock # 目前表中的所有记录 mysql\u003e select * from test; +----+------+------+----+ | id | a | b | n | +----+------+------+----+ | 1 | s | ss | 5 | | 2 | d | dd | 10 | | 4 | g | gg | 20 | | 6 | l | ll | 50 | +----+------+------+----+ mysql\u003e select * from test where n \u003e 1 and n \u003c=15 for update; +----+------+------+----+ | id | a | b | n | +----+------+------+----+ | 1 | s | ss | 5 | | 2 | d | dd | 10 | +----+------+------+----+ # 加锁情况如下 # 可以看到符合条件(5，10)的记录都被加上了 next-key lock(X)， 不符合条件的第一条(20)也加上了 next-key lock +----------+------------+-----------+---------------+-------------+-----------+ | trans_id | index_name | lock_type | lock_mode | lock_status | lock_data | +----------+------------+-----------+---------------+-------------+-----------+ | 2119 | NULL | TABLE | IX | GRANTED | NULL | | 2119 | index_n | RECORD | X | GRANTED | 5, 1 | | 2119 | index_n | RECORD | X | GRANTED | 10, 2 | | 2119 | index_n | RECORD | X | GRANTED | 20, 4 | | 2119 | PRIMARY | RECORD | X,REC_NOT_GAP | GRANTED | 1 | | 2119 | PRIMARY | RECORD | X,REC_NOT_GAP | GRANTED | 2 | +----------+------------+-----------+---------------+-------------+-----------+ 使用非唯一索引 InnoDB 会锁住索引本身，还会锁住索引记录前面的间隙会对所有满足条件的记录 即加 next-key lock。 mysql\u003e select * from test where b \u003e 'gg' for update; +----+------+------+----+ | id | a | b | n | +----+------+------+----+ | 6 | l | ll | 50 | | 1 | s | ss | 5 | +----+------+------+----+ # 加锁情况如下，可以看到在非唯一索引 index_b 加的是 next-key lock +----------+------------+-----------+---------------+-------------+------------------------+ | trans_id | index_name | lock_type | lock_mode | lock_status | lock_data | +----------+------------+-----------+---------------+-------------+------------------------+ | 2159 | NULL | TABLE | IX | GRANTED | NULL | | 2159 | index_b | RECORD | X | GRANTED | supremum pseudo-record | | 2159 | index_b | RECORD | X | GRANTED | 'ss', 1 | | 2159 | index_b | RECORD | X | GRANTED | 'll', 6 | | 2159 | PRIMARY | RECORD | X,REC_NOT_GAP | GRANTED | 1 | | 2159 | PRIMARY | RECORD | X,REC_NOT_GAP | ","date":"2020-01-20","objectID":"/mysql-lock/:2:0","tags":["mysql","innodb","事务"],"title":"MySQL InnoDB 加锁实践分析","uri":"/mysql-lock/"},{"categories":null,"content":"Nginx Ingress Controller 作为 Kubernetes 官方推荐的 ingress controller 实现，可以说是非常好用，尤其是在你之前如果使用 Nginx 作为反向代理或者负载均衡，那么对于 nginx ingress controller 的应用也会得心应手。Nginx ingress controller 对外暴露 kubernetes 中的服务，作为外部流量的入口，是集群中非常关键的组件。我们这篇文章关注 nginx ingress 通过 annotation 提供的流量管理功能，利用 annotation 的配置可以轻松帮助我们实现金丝雀发布和A/B test。 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:0","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"Nginx ingress annotations 假设我们现在部署了两个版本的服务，老版本和 canary 版本 nginx.ingress.kubernetes.io/canary-by-header 请求头中如果包含了这个 annotation 设置的值，则流量被转发到 canary 版本的服务 nginx.ingress.kubernetes.io/canary-by-header-value 这个 annotation 是配合 nginx.ingress.kubernetes.io/canary-by-header 来使用的，当请求头对应上面配置的 header 的值与这个 annotation 匹配， 则流量转发到 canary 版本 nginx.ingress.kubernetes.io/canary-weight 根据权重来转发流量，配置的权重值为 0-100。如配置 30 ，则把 30% 的流量转发到 canary 版本 nginx.ingress.kubernetes.io/canary-by-cookie 根据请求头中的 cookie 转发流量，如果 cookie 与配置的 annotation 的值匹配，且 cookie 的 value 是 always, 则该请求会被转发到 canary 版本 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:1","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"部署服务 这里我们服务的 deployment 就不展示了，service 配置如下 # 测试版本 apiVersion: v1 kind: Service metadata: name: hello-service labels: app: hello-service spec: ports: - port: 80 protocol: TCP selector: app: hello-service # canary 版本 apiVersion: v1 kind: Service metadata: name: canary-hello-service labels: app: canary-hello-service spec: ports: - port: 80 protocol: TCP selector: app: canary-hello-service ","date":"2019-12-27","objectID":"/nginx-ingress/:0:2","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"根据权重转发 ingress 配置如下 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: canary annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-weight: \"30\" spec: rules: - host: canary-service.abc.com http: paths: - backend: serviceName: canary-hello-service servicePort: 80 测试结果如下 $ for i in $(seq 1 10); do curl http://canary-service.abc.com; echo '\\n'; done hello world-version1 hello world-version1 hello world-version2 hello world-version2 hello world-version1 hello world-version1 hello world-version1 hello world-version1 hello world-version1 hello world-version1 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:3","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"根据请求头转发 annotation 配置如下（ingress 其余部分省略） annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-by-header: \"test\" 测试结果如下 $ for i in $(seq 1 5); do curl http://canary-service.abc.com; echo '\\n'; done hello world-version1 hello world-version1 hello world-version1 hello world-version1 hello world-version1 $ for i in $(seq 1 5); do curl -H 'test:always' http://canary-service.abc.com; echo '\\n'; done hello world-version2 hello world-version2 hello world-version2 hello world-version2 hello world-version2 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:4","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"根据特定的请求头和值转发 annotation 配置如下 kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-by-header: \"test\" nginx.ingress.kubernetes.io/canary-by-header-value: \"abc\" 测试结果如下： $ for i in $(seq 1 5); do curl -H 'test:always' http://canary-service.abc.com; echo '\\n'; done hello world-version1 hello world-version1 hello world-version1 hello world-version1 hello world-version1 $ for i in $(seq 1 5); do curl -H 'test:abc' http://canary-service.abc.com; echo '\\n'; done hello world-version2 hello world-version2 hello world-version2 hello world-version2 hello world-version2 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:5","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"根据 cookie 转发 使用 cookie 来进行流量管理的场景比较适合用于 A/B test，比如用户的请求 cookie 中含有特殊的标签，那么我们可以把这部分用户的请求转发到特定的服务进行处理。 annotation 配置如下 kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-by-cookie: \"like_music\" 测试结果如下 $ for i in $(seq 1 5); do curl -b 'like_music=1' http://canary-service.abc.com; echo '\\n'; done hello world-version1 hello world-version1 hello world-version1 hello world-version1 hello world-version1 $ for i in $(seq 1 5); do curl -b 'like_music=always' http://canary-service.abc.com; echo '\\n'; done hello world-version2 hello world-version2 hello world-version2 hello world-version2 hello world-version2 三种 annotation 按如下顺序匹配 canary-by-header \u003e canary-by-cookie \u003e canary-weight ","date":"2019-12-27","objectID":"/nginx-ingress/:0:6","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"总结 我们从以上的测试结果来看，可以把流量的转发分成以下两类 根据权重 nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-weight: \"30\" 70% |------\u003e 生产版本 users --- 100% ---\u003e Nginx Ingress ----| 30% |------\u003e canary 版本 根据用户 nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-by-header: \"test\" nginx.ingress.kubernetes.io/canary-by-header-value: \"abc\" others |-----------\u003e 生产版本 users ------\u003e Nginx Ingress ------| \"test:abc\" |-----------\u003e canary 版本 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:7","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"Nginx Ingress Controller 作为 Kubernetes 官方推荐的 ingress controller 实现，可以说是非常好用，尤其是在你之前如果使用 Nginx 作为反向代理或者负载均衡，那么对于 nginx ingress controller 的应用也会得心应手。Nginx ingress controller 对外暴露 kubernetes 中的服务，作为外部流量的入口，是集群中非常关键的组件。我们这篇文章关注 nginx ingress 通过 annotation 提供的流量管理功能，利用 annotation 的配置可以轻松帮助我们实现金丝雀发布和A/B test。 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:0","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"Nginx ingress annotations 假设我们现在部署了两个版本的服务，老版本和 canary 版本 nginx.ingress.kubernetes.io/canary-by-header 请求头中如果包含了这个 annotation 设置的值，则流量被转发到 canary 版本的服务 nginx.ingress.kubernetes.io/canary-by-header-value 这个 annotation 是配合 nginx.ingress.kubernetes.io/canary-by-header 来使用的，当请求头对应上面配置的 header 的值与这个 annotation 匹配， 则流量转发到 canary 版本 nginx.ingress.kubernetes.io/canary-weight 根据权重来转发流量，配置的权重值为 0-100。如配置 30 ，则把 30% 的流量转发到 canary 版本 nginx.ingress.kubernetes.io/canary-by-cookie 根据请求头中的 cookie 转发流量，如果 cookie 与配置的 annotation 的值匹配，且 cookie 的 value 是 always, 则该请求会被转发到 canary 版本 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:1","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"部署服务 这里我们服务的 deployment 就不展示了，service 配置如下 # 测试版本 apiVersion: v1 kind: Service metadata: name: hello-service labels: app: hello-service spec: ports: - port: 80 protocol: TCP selector: app: hello-service # canary 版本 apiVersion: v1 kind: Service metadata: name: canary-hello-service labels: app: canary-hello-service spec: ports: - port: 80 protocol: TCP selector: app: canary-hello-service ","date":"2019-12-27","objectID":"/nginx-ingress/:0:2","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"根据权重转发 ingress 配置如下 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: canary annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-weight: \"30\" spec: rules: - host: canary-service.abc.com http: paths: - backend: serviceName: canary-hello-service servicePort: 80 测试结果如下 $ for i in $(seq 1 10); do curl http://canary-service.abc.com; echo '\\n'; done hello world-version1 hello world-version1 hello world-version2 hello world-version2 hello world-version1 hello world-version1 hello world-version1 hello world-version1 hello world-version1 hello world-version1 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:3","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"根据请求头转发 annotation 配置如下（ingress 其余部分省略） annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-by-header: \"test\" 测试结果如下 $ for i in $(seq 1 5); do curl http://canary-service.abc.com; echo '\\n'; done hello world-version1 hello world-version1 hello world-version1 hello world-version1 hello world-version1 $ for i in $(seq 1 5); do curl -H 'test:always' http://canary-service.abc.com; echo '\\n'; done hello world-version2 hello world-version2 hello world-version2 hello world-version2 hello world-version2 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:4","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"根据特定的请求头和值转发 annotation 配置如下 kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-by-header: \"test\" nginx.ingress.kubernetes.io/canary-by-header-value: \"abc\" 测试结果如下： $ for i in $(seq 1 5); do curl -H 'test:always' http://canary-service.abc.com; echo '\\n'; done hello world-version1 hello world-version1 hello world-version1 hello world-version1 hello world-version1 $ for i in $(seq 1 5); do curl -H 'test:abc' http://canary-service.abc.com; echo '\\n'; done hello world-version2 hello world-version2 hello world-version2 hello world-version2 hello world-version2 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:5","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"根据 cookie 转发 使用 cookie 来进行流量管理的场景比较适合用于 A/B test，比如用户的请求 cookie 中含有特殊的标签，那么我们可以把这部分用户的请求转发到特定的服务进行处理。 annotation 配置如下 kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-by-cookie: \"like_music\" 测试结果如下 $ for i in $(seq 1 5); do curl -b 'like_music=1' http://canary-service.abc.com; echo '\\n'; done hello world-version1 hello world-version1 hello world-version1 hello world-version1 hello world-version1 $ for i in $(seq 1 5); do curl -b 'like_music=always' http://canary-service.abc.com; echo '\\n'; done hello world-version2 hello world-version2 hello world-version2 hello world-version2 hello world-version2 三种 annotation 按如下顺序匹配 canary-by-header \u003e canary-by-cookie \u003e canary-weight ","date":"2019-12-27","objectID":"/nginx-ingress/:0:6","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"总结 我们从以上的测试结果来看，可以把流量的转发分成以下两类 根据权重 nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-weight: \"30\" 70% |------\u003e 生产版本 users --- 100% ---\u003e Nginx Ingress ----| 30% |------\u003e canary 版本 根据用户 nginx.ingress.kubernetes.io/canary: \"true\" nginx.ingress.kubernetes.io/canary-by-header: \"test\" nginx.ingress.kubernetes.io/canary-by-header-value: \"abc\" others |-----------\u003e 生产版本 users ------\u003e Nginx Ingress ------| \"test:abc\" |-----------\u003e canary 版本 ","date":"2019-12-27","objectID":"/nginx-ingress/:0:7","tags":["kubernetes","nginx ingress","流量管理"],"title":"Nginx Ingress 进阶——灰度发布","uri":"/nginx-ingress/"},{"categories":null,"content":"随着我们服务的全面容器化，代码和配置都以 docker image 的形式存储在 docker registry 中。搭建高可用，安全的企业r私有 docker registy 就变得十分必要。 Harbor 作为 CNCF 托管的开药 docker registry 项目是我们的首先。它除了提供存储、签名、扫描镜像的功能，还有常用的功能如鉴权、授权、权限管理等来增强 docker registry 的安全性。 除此之外，Harbor 还有从其他 registry 复制镜像的功能，新版本集成了 chartmuseum 从而可以作为 Helm Charts 的 repository。Harbor 一个重要的特点是它是通过可插拔的方式来 集合第三方组件来提供不同的功能，比如用于镜像签名的 notary；镜像扫描的 clair; 镜像存储，推送和拉取的 docker registry；helm charts 存储，管理的 chartmuseum。 在之前我们其实已经通过 docker-compose 搭建了 Harbor。这里主要介绍的是 Harbor 的工作原理和在 k8s 中部署 Harbor。 首先，我们结合 Harbor 的架构来介绍一下各个组件的功能。 ","date":"2019-11-15","objectID":"/k8s-harbor/:0:0","tags":["harbor","registry","k8s"],"title":"k8s 中部署 Harbor","uri":"/k8s-harbor/"},{"categories":null,"content":"Harbor 架构 下面这张图是来自 Harbor github 的官方架构图，结合这张图我们介绍一下关键的组件 数据层 redis 用于数据缓存和定时，异步任务的临时存储 镜像存储 支持的方式非常多，比如本地磁盘，网络文件系统(NAS), 对象存储(S3, OSS)等等 数据库 新版本使用 PostgreSQL, 用来存储 Harbor 里创建的 projects, users, roles , replication policies, tag retention policies, clair 从各大开源社区获取的CVE漏洞缺陷数据以及镜像层的扫描结构数据 ","date":"2019-11-15","objectID":"/k8s-harbor/:0:1","tags":["harbor","registry","k8s"],"title":"k8s 中部署 Harbor","uri":"/k8s-harbor/"},{"categories":null,"content":"核心组件 proxy 用 nginx 作为反向代理，转发来自浏览器和 docker 客户端的请求到不同的后端服务，比如 core, registry, token service, 网页客户端入口服务等 core 核心模块，包括鉴权和授权；配置管理；项目管理；项目容量管理；代理 chartmuseum; 镜像复制管理; 镜像扫描管理；webhook 提供的回调等等 job service 处理定时任务和异步任务，主要包括镜像同步，镜像扫描任务 chart museum 第三方开源 chart repository, 提供 helm chart 的管理和 api 访问 docker registry 第三方 registry 服务, 镜像处理的核心组件，处理镜像的上传、下拉等，直接操作镜像存储服务，如 S3、nas 等 notary 第三方内容信任服务，用来保证镜像在 pull，push 和传输过程中的一致性和完整性 ","date":"2019-11-15","objectID":"/k8s-harbor/:0:2","tags":["harbor","registry","k8s"],"title":"k8s 中部署 Harbor","uri":"/k8s-harbor/"},{"categories":null,"content":"鉴权过程 这里我们重点描述一下 docker 客户端登录 Harbor 的过程 在客户端登录我们自然首先会输入以下命令 docker login harbor.abc.com 然后输入我们的用户名和密码 1.客户端首先发送一个 GET 请求 harbor.abc.com/v2 到 registry 服务 2.registry 服务配置的是通过 JWT token 鉴权，所以返回的是一个 401 错误码，并且同时会在 response 的 header 里通知客户端去获取 token 的真正地址。Harbor 里这个地址指向的是 core service 3.客户度收到 401 和 response, 接着发送获取 token 的请求到指定的 url, 这个 url 会是 harbor.abc.com/service.token，在 request 的 header 中包含用户民和密码 4.token service（被包含在 core 的容器中）接收到请求后，解析请求拿到用户民和密码，然后去数据库中验证用户的有效性。token service 可以 配置对接外部的 LDAP/AD 来进行认证。认证成功后，返回 200 并且 response body 中包含由私钥产生的 JWT token 5.docker 客户端收到 200 请求，则登录成功。打印出 Login Succeeded |---------------| |----------| |---------------| | docker client | | registry | | token service | |---------------| |----------| |---------------| | 1. harbor.abc.com/v2 | | |-------------------------\u003e| | | 2. return 401 | | |\u003c-------------------------| | | 3. harbor.abc.com/service/token | |---------------------------------------------------\u003e| | 4. return 200 | |\u003c---------------------------------------------------| | | | | 5. login success | | push 镜像的过程和登录类似，主要区别在 鉴权那一步，会具体检查用户是否有往对应项目推送 image 的权限。 ","date":"2019-11-15","objectID":"/k8s-harbor/:0:3","tags":["harbor","registry","k8s"],"title":"k8s 中部署 Harbor","uri":"/k8s-harbor/"},{"categories":null,"content":"k8s 部署 推荐使用 Helm 安装 下载 harbor helm chart wget https://github.com/goharbor/harbor-helm/archive/v1.2.2.tar.gz 解压，复制一份 values.yaml 进行定制 推荐使用 ingress 对外暴露服务 主要需要修改的配置是数据持久化和镜像存储，这里我们使用 NAS 通过区分 subpath, 把同一个 PVC 挂载到不同的容器 # The persistence is enabled by default and a default StorageClass # is needed in the k8s cluster to provision volumes dynamicly. # Specify another StorageClass in the \"storageClass\" or set \"existingClaim\" # if you have already existing persistent volumes to use # # For storing images and charts, you can also use \"azure\", \"gcs\", \"s3\", # \"swift\" or \"oss\". Set it in the \"imageChartStorage\" section persistence: enabled: true # Setting it to \"keep\" to avoid removing PVCs during a helm delete # operation. Leaving it empty will delete PVCs after the chart deleted resourcePolicy: \"keep\" persistentVolumeClaim: registry: # Use the existing PVC which must be created manually before bound, # and specify the \"subPath\" if the PVC is shared with other components existingClaim: \"harbor-nas\" # Specify the \"storageClass\" used to provision the volume. Or the default # StorageClass will be used(the default). # Set it to \"-\" to disable dynamic provisioning storageClass: \"\" subPath: \"registry\" accessMode: ReadWriteOnce size: 5Gi chartmuseum: existingClaim: \"harbor-nas\" storageClass: \"\" subPath: \"chartmuseum\" accessMode: ReadWriteOnce size: 5Gi jobservice: existingClaim: \"harbor-nas\" storageClass: \"\" subPath: \"jobservice\" accessMode: ReadWriteOnce size: 1Gi # If external database is used, the following settings for database will # be ignored database: existingClaim: \"harbor-nas\" storageClass: \"\" subPath: \"database\" accessMode: ReadWriteOnce size: 10Gi # If external Redis is used, the following settings for Redis will # be ignored redis: existingClaim: \"harbor-nas\" storageClass: \"\" subPath: \"redis\" accessMode: ReadWriteOnce size: 1Gi NAS PVC, 前提是在阿里云中开通了 NAS 并且集群中要创建 alicloud-nas-controller(具体可参照阿里云 NAS 文档) apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: alicloud-nas mountOptions: - nolock,tcp,noresvport - vers=3 parameters: volumeAs: subpath server: \"xxxxx\" provisioner: alicloud/nas reclaimPolicy: Delete --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: harbor-nas spec: accessModes: - ReadWriteMany resources: requests: storage: 50Gi storageClassName: alicloud-nas 部署 在解压后的目录里执行 helm install --name harbor -f edited_values.yaml . --namespace harbor 检查部署 部署完成后我们可以看到部署了以下服务 NAME READY UP-TO-DATE AVAILABLE AGE harbor-chartmuseum 1/1 1 1 2d1h harbor-clair 1/1 1 1 2d1h harbor-core 1/1 1 1 2d1h harbor-jobservice 1/1 1 1 2d1h harbor-notary-server 1/1 1 1 2d1h harbor-notary-signer 1/1 1 1 2d1h harbor-portal 1/1 1 1 2d1h harbor-registry 1/1 1 1 2d1h ","date":"2019-11-15","objectID":"/k8s-harbor/:0:4","tags":["harbor","registry","k8s"],"title":"k8s 中部署 Harbor","uri":"/k8s-harbor/"},{"categories":null,"content":"随着我们服务的全面容器化，代码和配置都以 docker image 的形式存储在 docker registry 中。搭建高可用，安全的企业r私有 docker registy 就变得十分必要。 Harbor 作为 CNCF 托管的开药 docker registry 项目是我们的首先。它除了提供存储、签名、扫描镜像的功能，还有常用的功能如鉴权、授权、权限管理等来增强 docker registry 的安全性。 除此之外，Harbor 还有从其他 registry 复制镜像的功能，新版本集成了 chartmuseum 从而可以作为 Helm Charts 的 repository。Harbor 一个重要的特点是它是通过可插拔的方式来 集合第三方组件来提供不同的功能，比如用于镜像签名的 notary；镜像扫描的 clair; 镜像存储，推送和拉取的 docker registry；helm charts 存储，管理的 chartmuseum。 在之前我们其实已经通过 docker-compose 搭建了 Harbor。这里主要介绍的是 Harbor 的工作原理和在 k8s 中部署 Harbor。 首先，我们结合 Harbor 的架构来介绍一下各个组件的功能。 ","date":"2019-11-15","objectID":"/k8s-harbor/:0:0","tags":["harbor","registry","k8s"],"title":"k8s 中部署 Harbor","uri":"/k8s-harbor/"},{"categories":null,"content":"Harbor 架构 下面这张图是来自 Harbor github 的官方架构图，结合这张图我们介绍一下关键的组件 数据层 redis 用于数据缓存和定时，异步任务的临时存储 镜像存储 支持的方式非常多，比如本地磁盘，网络文件系统(NAS), 对象存储(S3, OSS)等等 数据库 新版本使用 PostgreSQL, 用来存储 Harbor 里创建的 projects, users, roles , replication policies, tag retention policies, clair 从各大开源社区获取的CVE漏洞缺陷数据以及镜像层的扫描结构数据 ","date":"2019-11-15","objectID":"/k8s-harbor/:0:1","tags":["harbor","registry","k8s"],"title":"k8s 中部署 Harbor","uri":"/k8s-harbor/"},{"categories":null,"content":"核心组件 proxy 用 nginx 作为反向代理，转发来自浏览器和 docker 客户端的请求到不同的后端服务，比如 core, registry, token service, 网页客户端入口服务等 core 核心模块，包括鉴权和授权；配置管理；项目管理；项目容量管理；代理 chartmuseum; 镜像复制管理; 镜像扫描管理；webhook 提供的回调等等 job service 处理定时任务和异步任务，主要包括镜像同步，镜像扫描任务 chart museum 第三方开源 chart repository, 提供 helm chart 的管理和 api 访问 docker registry 第三方 registry 服务, 镜像处理的核心组件，处理镜像的上传、下拉等，直接操作镜像存储服务，如 S3、nas 等 notary 第三方内容信任服务，用来保证镜像在 pull，push 和传输过程中的一致性和完整性 ","date":"2019-11-15","objectID":"/k8s-harbor/:0:2","tags":["harbor","registry","k8s"],"title":"k8s 中部署 Harbor","uri":"/k8s-harbor/"},{"categories":null,"content":"鉴权过程 这里我们重点描述一下 docker 客户端登录 Harbor 的过程 在客户端登录我们自然首先会输入以下命令 docker login harbor.abc.com 然后输入我们的用户名和密码 1.客户端首先发送一个 GET 请求 harbor.abc.com/v2 到 registry 服务 2.registry 服务配置的是通过 JWT token 鉴权，所以返回的是一个 401 错误码，并且同时会在 response 的 header 里通知客户端去获取 token 的真正地址。Harbor 里这个地址指向的是 core service 3.客户度收到 401 和 response, 接着发送获取 token 的请求到指定的 url, 这个 url 会是 harbor.abc.com/service.token，在 request 的 header 中包含用户民和密码 4.token service（被包含在 core 的容器中）接收到请求后，解析请求拿到用户民和密码，然后去数据库中验证用户的有效性。token service 可以 配置对接外部的 LDAP/AD 来进行认证。认证成功后，返回 200 并且 response body 中包含由私钥产生的 JWT token 5.docker 客户端收到 200 请求，则登录成功。打印出 Login Succeeded |---------------| |----------| |---------------| | docker client | | registry | | token service | |---------------| |----------| |---------------| | 1. harbor.abc.com/v2 | | |-------------------------\u003e| | | 2. return 401 | | |\u003c-------------------------| | | 3. harbor.abc.com/service/token | |---------------------------------------------------\u003e| | 4. return 200 | |\u003c---------------------------------------------------| | | | | 5. login success | | push 镜像的过程和登录类似，主要区别在 鉴权那一步，会具体检查用户是否有往对应项目推送 image 的权限。 ","date":"2019-11-15","objectID":"/k8s-harbor/:0:3","tags":["harbor","registry","k8s"],"title":"k8s 中部署 Harbor","uri":"/k8s-harbor/"},{"categories":null,"content":"k8s 部署 推荐使用 Helm 安装 下载 harbor helm chart wget https://github.com/goharbor/harbor-helm/archive/v1.2.2.tar.gz 解压，复制一份 values.yaml 进行定制 推荐使用 ingress 对外暴露服务 主要需要修改的配置是数据持久化和镜像存储，这里我们使用 NAS 通过区分 subpath, 把同一个 PVC 挂载到不同的容器 # The persistence is enabled by default and a default StorageClass # is needed in the k8s cluster to provision volumes dynamicly. # Specify another StorageClass in the \"storageClass\" or set \"existingClaim\" # if you have already existing persistent volumes to use # # For storing images and charts, you can also use \"azure\", \"gcs\", \"s3\", # \"swift\" or \"oss\". Set it in the \"imageChartStorage\" section persistence: enabled: true # Setting it to \"keep\" to avoid removing PVCs during a helm delete # operation. Leaving it empty will delete PVCs after the chart deleted resourcePolicy: \"keep\" persistentVolumeClaim: registry: # Use the existing PVC which must be created manually before bound, # and specify the \"subPath\" if the PVC is shared with other components existingClaim: \"harbor-nas\" # Specify the \"storageClass\" used to provision the volume. Or the default # StorageClass will be used(the default). # Set it to \"-\" to disable dynamic provisioning storageClass: \"\" subPath: \"registry\" accessMode: ReadWriteOnce size: 5Gi chartmuseum: existingClaim: \"harbor-nas\" storageClass: \"\" subPath: \"chartmuseum\" accessMode: ReadWriteOnce size: 5Gi jobservice: existingClaim: \"harbor-nas\" storageClass: \"\" subPath: \"jobservice\" accessMode: ReadWriteOnce size: 1Gi # If external database is used, the following settings for database will # be ignored database: existingClaim: \"harbor-nas\" storageClass: \"\" subPath: \"database\" accessMode: ReadWriteOnce size: 10Gi # If external Redis is used, the following settings for Redis will # be ignored redis: existingClaim: \"harbor-nas\" storageClass: \"\" subPath: \"redis\" accessMode: ReadWriteOnce size: 1Gi NAS PVC, 前提是在阿里云中开通了 NAS 并且集群中要创建 alicloud-nas-controller(具体可参照阿里云 NAS 文档) apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: alicloud-nas mountOptions: - nolock,tcp,noresvport - vers=3 parameters: volumeAs: subpath server: \"xxxxx\" provisioner: alicloud/nas reclaimPolicy: Delete --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: harbor-nas spec: accessModes: - ReadWriteMany resources: requests: storage: 50Gi storageClassName: alicloud-nas 部署 在解压后的目录里执行 helm install --name harbor -f edited_values.yaml . --namespace harbor 检查部署 部署完成后我们可以看到部署了以下服务 NAME READY UP-TO-DATE AVAILABLE AGE harbor-chartmuseum 1/1 1 1 2d1h harbor-clair 1/1 1 1 2d1h harbor-core 1/1 1 1 2d1h harbor-jobservice 1/1 1 1 2d1h harbor-notary-server 1/1 1 1 2d1h harbor-notary-signer 1/1 1 1 2d1h harbor-portal 1/1 1 1 2d1h harbor-registry 1/1 1 1 2d1h ","date":"2019-11-15","objectID":"/k8s-harbor/:0:4","tags":["harbor","registry","k8s"],"title":"k8s 中部署 Harbor","uri":"/k8s-harbor/"},{"categories":null,"content":"如果直接在云厂商提供的实例上搭建 kafka 集群可以说很简单，一般不会有什么困难。当我们选择把 kafka 部署到 k8s 里，希望利用 k8s 提供的编排能力来帮助我们更方便的管理 kafka 集群。在这种情况下部署会变得复杂起来，主要两个问题 有状态的服务部署 从 k8s 集群外访问 ","date":"2019-10-25","objectID":"/k8s-kafka/:0:0","tags":["kafka","k8s"],"title":"外部访问 k8s 中的 kafka 集群","uri":"/k8s-kafka/"},{"categories":null,"content":"zookeeper 部署 我们都知道 kafka 依赖 zk, 所以首先需要在 k8s 集群部署 zookeeper。 zookeeper 是有状态的服务，所以选择的方式是 StatefulSet + PVC。 这里我们使用的 zk 镜像是 k8s 官方的 k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10, 从这里 我们能看到，zk 在启动时候会自动创建配置文件并且根据 pod 的编号动态的把 myid 写入到 zk 的配置文件。 StatefulSet 部署 这里部署3副本的 zk 集群 StatefulSet 部署 yaml 示例如下: apiVersion: apps/v1 kind: StatefulSet metadata: name: zookeeper spec: selector: matchLabels: app: zookeeper serviceName: zookeeper-hs replicas: 3 updateStrategy: type: RollingUpdate podManagementPolicy: OrderedReady template: metadata: labels: app: zookeeper spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \"app\" operator: In values: - zookeeper topologyKey: \"kubernetes.io/hostname\" containers: - name: zookeeper imagePullPolicy: Always image: \"k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10\" command: - sh - -c - \"start-zookeeper \\ --servers=3 \\ --data_dir=/var/lib/zookeeper/data \\ --data_log_dir=/var/lib/zookeeper/data/log \\ --conf_dir=/opt/zookeeper/conf \\ --client_port=2181 \\ --election_port=3888 \\ --server_port=2888 \\ --tick_time=2000 \\ --init_limit=10 \\ --sync_limit=5 \\ --heap=512M \\ --max_client_cnxns=60 \\ --snap_retain_count=3 \\ --purge_interval=12 \\ --max_session_timeout=40000 \\ --min_session_timeout=4000 \\ --log_level=INFO\" ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election volumeMounts: - name: zookeeper-data mountPath: /var/lib/zookeeper securityContext: runAsUser: 1000 fsGroup: 1000 volumeClaimTemplates: - metadata: name: zookeeper-data spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: alicloud-disk-efficiency resources: requests: storage: 20Gi 创建 headless service 和 service zk 集群节点之间通过 headless service 互通，客户端访问 zk 通过 service headless service 和 service yaml 示例如下： apiVersion: v1 kind: Service metadata: name: zookeeper-hs labels: app: zookeeper spec: ports: - port: 2888 name: server - port: 3888 name: leader-election clusterIP: None selector: app: zookeeper --- apiVersion: v1 kind: Service metadata: name: zookeeper labels: app: zookeeper spec: ports: - port: 2181 name: zookeeper-client selector: app: zookeeper ","date":"2019-10-25","objectID":"/k8s-kafka/:0:1","tags":["kafka","k8s"],"title":"外部访问 k8s 中的 kafka 集群","uri":"/k8s-kafka/"},{"categories":null,"content":"kafka 部署 把 kafka 部署到 k8s 后，我们肯定是通过 service 从 k8s 外部访问 kafaka。这里的 service 要么是 NodePort， 要么是 LoadBalancer 类型。我们使用的方式是 LoadBalancer。 我们先看下面这张图，这是 kafka 在集群中的网络拓扑。当我们通过地址 12.345.67:31090 访问到 kafka 后，kafka 返回的访问地址是类似这样的 endpoint kafka-0.kafka-hs.kafka.default.svc.cluster.local:9092。这是 k8s 集群内部能访问的 headless service endpoint 地址，从集群外部自然使用这个地址是访问不通的。 所以，我们需要解决两个问题： kafka 不同的 pod 需要不通的对外能访问的地址 不能使用 kafka 默认的 advertised.listeners 解决方案 问题1，我们为每个 pod 创建类型是 LoadBalancer 的 service 并且监听不同的端口。这样通过 LB IP + port 就能找到特定的 kafka broker。 service 示例如下： apiVersion: v1 kind: Service metadata: name: kafka-{index} spec: externalTrafficPolicy: Local type: LoadBalancer selector: statefulset.kubernetes.io/pod-name: kafka-{index} ports: - protocol: TCP port: {9092+index} targetPort: 9092 问题2，我们在容器启动的时候，执行脚本动态获取 pod 编号，生成容器需要的环境变量 KAFKA_CFG_ADVERTISED_LISTENERS（对应 kafka broker 的配置 advertised.listener） HOSTNAME=`hostname -s` if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then ORD=${BASH_REMATCH[2]} PORT=$((ORD + 9092)) #12.345.67.8 是 LB 的 ip export KAFKA_CFG_ADVERTISED_LISTENERS=\"PLAINTEXT://12.345.67.8:$PORT\" else echo \"Failed to get index from hostname $HOST\" exit 1 fi 完整的 kafka StatefulSet 示例如下： apiVersion: apps/v1 kind: StatefulSet metadata: name: kafka spec: selector: matchLabels: app: kafka serviceName: kafka replicas: 3 updateStrategy: type: RollingUpdate podManagementPolicy: OrderedReady template: metadata: labels: app: kafka spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \"app\" operator: In values: - kafka topologyKey: \"kubernetes.io/hostname\" containers: - name: kafka command: - bash - -ec - | HOSTNAME=`hostname -s` if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then ORD=${BASH_REMATCH[2]} PORT=$((ORD + 9092)) export KAFKA_CFG_ADVERTISED_LISTENERS=\"PLAINTEXT://12.345.67.8:$PORT\" else echo \"Failed to get index from hostname $HOST\" exit 1 fi exec /entrypoint.sh /run.sh imagePullPolicy: Always image: \"bitnami/kafka:2\" env: - name: ALLOW_PLAINTEXT_LISTENER value: \"yes\" - name: KAFKA_CFG_ZOOKEEPER_CONNECT value: \"zookeeper-0.zookeeper-hs:2181,zookeeper-1.zookeeper-hs:2181,zookeeper-2.zookeeper-hs:2181\" - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR value: \"3\" - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR value: \"3\" - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR value: \"3\" ports: - containerPort: 9092 volumeMounts: - name: kafka-data mountPath: /bitnami securityContext: runAsUser: 1000 fsGroup: 1000 volumeClaimTemplates: - metadata: name: kafka-data spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: alicloud-disk-efficiency resources: requests: storage: 20Gi 最终，我们从集群外面就能通过 12.345.67.8:9092,12.345.67.8:9093,12.345.67.8:9094这样的地址访问到 k8s 中的 kafka 集群。 ","date":"2019-10-25","objectID":"/k8s-kafka/:0:2","tags":["kafka","k8s"],"title":"外部访问 k8s 中的 kafka 集群","uri":"/k8s-kafka/"},{"categories":null,"content":"如果直接在云厂商提供的实例上搭建 kafka 集群可以说很简单，一般不会有什么困难。当我们选择把 kafka 部署到 k8s 里，希望利用 k8s 提供的编排能力来帮助我们更方便的管理 kafka 集群。在这种情况下部署会变得复杂起来，主要两个问题 有状态的服务部署 从 k8s 集群外访问 ","date":"2019-10-25","objectID":"/k8s-kafka/:0:0","tags":["kafka","k8s"],"title":"外部访问 k8s 中的 kafka 集群","uri":"/k8s-kafka/"},{"categories":null,"content":"zookeeper 部署 我们都知道 kafka 依赖 zk, 所以首先需要在 k8s 集群部署 zookeeper。 zookeeper 是有状态的服务，所以选择的方式是 StatefulSet + PVC。 这里我们使用的 zk 镜像是 k8s 官方的 k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10, 从这里 我们能看到，zk 在启动时候会自动创建配置文件并且根据 pod 的编号动态的把 myid 写入到 zk 的配置文件。 StatefulSet 部署 这里部署3副本的 zk 集群 StatefulSet 部署 yaml 示例如下: apiVersion: apps/v1 kind: StatefulSet metadata: name: zookeeper spec: selector: matchLabels: app: zookeeper serviceName: zookeeper-hs replicas: 3 updateStrategy: type: RollingUpdate podManagementPolicy: OrderedReady template: metadata: labels: app: zookeeper spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \"app\" operator: In values: - zookeeper topologyKey: \"kubernetes.io/hostname\" containers: - name: zookeeper imagePullPolicy: Always image: \"k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10\" command: - sh - -c - \"start-zookeeper \\ --servers=3 \\ --data_dir=/var/lib/zookeeper/data \\ --data_log_dir=/var/lib/zookeeper/data/log \\ --conf_dir=/opt/zookeeper/conf \\ --client_port=2181 \\ --election_port=3888 \\ --server_port=2888 \\ --tick_time=2000 \\ --init_limit=10 \\ --sync_limit=5 \\ --heap=512M \\ --max_client_cnxns=60 \\ --snap_retain_count=3 \\ --purge_interval=12 \\ --max_session_timeout=40000 \\ --min_session_timeout=4000 \\ --log_level=INFO\" ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election volumeMounts: - name: zookeeper-data mountPath: /var/lib/zookeeper securityContext: runAsUser: 1000 fsGroup: 1000 volumeClaimTemplates: - metadata: name: zookeeper-data spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: alicloud-disk-efficiency resources: requests: storage: 20Gi 创建 headless service 和 service zk 集群节点之间通过 headless service 互通，客户端访问 zk 通过 service headless service 和 service yaml 示例如下： apiVersion: v1 kind: Service metadata: name: zookeeper-hs labels: app: zookeeper spec: ports: - port: 2888 name: server - port: 3888 name: leader-election clusterIP: None selector: app: zookeeper --- apiVersion: v1 kind: Service metadata: name: zookeeper labels: app: zookeeper spec: ports: - port: 2181 name: zookeeper-client selector: app: zookeeper ","date":"2019-10-25","objectID":"/k8s-kafka/:0:1","tags":["kafka","k8s"],"title":"外部访问 k8s 中的 kafka 集群","uri":"/k8s-kafka/"},{"categories":null,"content":"kafka 部署 把 kafka 部署到 k8s 后，我们肯定是通过 service 从 k8s 外部访问 kafaka。这里的 service 要么是 NodePort， 要么是 LoadBalancer 类型。我们使用的方式是 LoadBalancer。 我们先看下面这张图，这是 kafka 在集群中的网络拓扑。当我们通过地址 12.345.67:31090 访问到 kafka 后，kafka 返回的访问地址是类似这样的 endpoint kafka-0.kafka-hs.kafka.default.svc.cluster.local:9092。这是 k8s 集群内部能访问的 headless service endpoint 地址，从集群外部自然使用这个地址是访问不通的。 所以，我们需要解决两个问题： kafka 不同的 pod 需要不通的对外能访问的地址 不能使用 kafka 默认的 advertised.listeners 解决方案 问题1，我们为每个 pod 创建类型是 LoadBalancer 的 service 并且监听不同的端口。这样通过 LB IP + port 就能找到特定的 kafka broker。 service 示例如下： apiVersion: v1 kind: Service metadata: name: kafka-{index} spec: externalTrafficPolicy: Local type: LoadBalancer selector: statefulset.kubernetes.io/pod-name: kafka-{index} ports: - protocol: TCP port: {9092+index} targetPort: 9092 问题2，我们在容器启动的时候，执行脚本动态获取 pod 编号，生成容器需要的环境变量 KAFKA_CFG_ADVERTISED_LISTENERS（对应 kafka broker 的配置 advertised.listener） HOSTNAME=`hostname -s` if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then ORD=${BASH_REMATCH[2]} PORT=$((ORD + 9092)) #12.345.67.8 是 LB 的 ip export KAFKA_CFG_ADVERTISED_LISTENERS=\"PLAINTEXT://12.345.67.8:$PORT\" else echo \"Failed to get index from hostname $HOST\" exit 1 fi 完整的 kafka StatefulSet 示例如下： apiVersion: apps/v1 kind: StatefulSet metadata: name: kafka spec: selector: matchLabels: app: kafka serviceName: kafka replicas: 3 updateStrategy: type: RollingUpdate podManagementPolicy: OrderedReady template: metadata: labels: app: kafka spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \"app\" operator: In values: - kafka topologyKey: \"kubernetes.io/hostname\" containers: - name: kafka command: - bash - -ec - | HOSTNAME=`hostname -s` if [[ $HOSTNAME =~ (.*)-([0-9]+)$ ]]; then ORD=${BASH_REMATCH[2]} PORT=$((ORD + 9092)) export KAFKA_CFG_ADVERTISED_LISTENERS=\"PLAINTEXT://12.345.67.8:$PORT\" else echo \"Failed to get index from hostname $HOST\" exit 1 fi exec /entrypoint.sh /run.sh imagePullPolicy: Always image: \"bitnami/kafka:2\" env: - name: ALLOW_PLAINTEXT_LISTENER value: \"yes\" - name: KAFKA_CFG_ZOOKEEPER_CONNECT value: \"zookeeper-0.zookeeper-hs:2181,zookeeper-1.zookeeper-hs:2181,zookeeper-2.zookeeper-hs:2181\" - name: KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR value: \"3\" - name: KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR value: \"3\" - name: KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR value: \"3\" ports: - containerPort: 9092 volumeMounts: - name: kafka-data mountPath: /bitnami securityContext: runAsUser: 1000 fsGroup: 1000 volumeClaimTemplates: - metadata: name: kafka-data spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: alicloud-disk-efficiency resources: requests: storage: 20Gi 最终，我们从集群外面就能通过 12.345.67.8:9092,12.345.67.8:9093,12.345.67.8:9094这样的地址访问到 k8s 中的 kafka 集群。 ","date":"2019-10-25","objectID":"/k8s-kafka/:0:2","tags":["kafka","k8s"],"title":"外部访问 k8s 中的 kafka 集群","uri":"/k8s-kafka/"},{"categories":null,"content":"我们整个日志系统是使用 EFK 这套开源方案来搭建的，在以前的文章中我们有提到过。在这个体系中，最关键的组件当属 Elasticsearch, 它负责数据的存储和搜索。在这里我们总结一下 es 的读写原理。 首先我们需要了解一下文档和分片的关系 ","date":"2019-09-29","objectID":"/es-wr/:0:0","tags":["elasticsearch"],"title":"Elasticsearch 读写总结","uri":"/es-wr/"},{"categories":null,"content":"文档元数据 在 es 中我们把存储的一条日志(记录)称为文档，一个文档不仅仅包含它的数据 ，也包含元数据 —— 有关文档的信息。 三个必须的元数据元素如下： _index 文档在哪存放，_index 应该是因共同的特性被分组到一起的文档集合。比如相同格式的日志我们会放到同一个 index 中 _type 文档表示的对象类别，type 可以理解成是 index 的子类，不同的 type 包含的字段可能不同 _id 文档唯一标识, id 是一个字符串， 当它和 _index 以及 _type 组合就可以唯一确定 Elasticsearch 中的一个文档。 当创建一个新的文档，要么提供自己的 _id ，要么让 Elasticsearch 帮我们生成。 { \"_index\": \"ingress-access-2019.09.39\", \"_type\": \"_doc\", \"_id\": \"IY6KfG0BRpn5jI2pyyVf\", \"_version\": 1, \"_score\": null, \"_source\": { \"input\": { \"type\": \"log\" }, ... } } 倒排索引 传统的数据库每个字段存储单个值，但这对全文检索并不够。文本字段中的每个单词需要被搜索，对数据库意味着需要单个字段有索引多值(这里指单词)的能力。倒排索引就是这样一种数据结构，倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 Term | Doc 1 | Doc 2 | Doc 3 | ... ------------------------------------ brown | X | | X | ... fox | X | X | X | ... quick | X | X | | ... the | X | | X | ... 倒排索引相比特定词项出现过的文档列表，会包含更多其它信息。它会保存每一个词项出现过的文档总数， 在对应的文档中一个具体词项出现的总次数，词项在文档中的顺序，每个文档的长度，所有文档的平均长度，等等。这些统计信息允许 Elasticsearch 决定哪些词比其它词更重要，哪些文档比其它文档更重要。 分片 一个索引可以存储超出单个节点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点可能没有这样大的磁盘空间来存储或者单个节点处理搜索请求，响应会太慢。为了解决这个问题，Elasticsearch 提供了将索引划分成多片的能力，这些片叫做分片。当创建一个索引的时候，可以指定你分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引” 可以被放置到集群中的任何节点上。 分片之所以重要，主要有两方面的原因： 允许水平分割/扩展容量 允许并行操作，提高了性能/吞吐量 文档路由到分片 当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？ 首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的： shard = hash(routing) % number_of_primary_shards routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到 余数 。这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。 这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量, 因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。 ","date":"2019-09-29","objectID":"/es-wr/:0:1","tags":["elasticsearch"],"title":"Elasticsearch 读写总结","uri":"/es-wr/"},{"categories":null,"content":"搜索过程 查询阶段 查询阶段包含以下三个步骤: 客户端发送一个 search 请求到 Node 3 ， Node 3 会创建一个大小为 from + size 的空优先队列。 Node 3 将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中。 每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，也就是 Node 3 ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。 当一个搜索请求被发送到某个节点时，这个节点就变成了协调节点。 这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。 第一步是广播请求到索引中每一个节点的分片拷贝，查询请求可以被某个主分片或某个副本分片处理， 这就是为什么更多的副本（当结合更多的硬件）能够增加搜索吞吐率。 协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载。 每个分片在本地执行查询请求并且创建一个长度为 from + size 的优先队列—也就是说，每个分片创建的结果集足够大，均可以满足全局的搜索请求。 分片返回一个轻量级的结果列表到协调节点，它仅包含文档 ID 集合以及任何排序需要用到的值，例如 _score 。 协调节点将这些分片级的结果合并到自己的有序优先队列里，它代表了全局排序结果集合。至此查询过程结束。 取回阶段 取回阶段由以下步骤构成： 协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。 每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节 点。 一旦所有的文档都被取回了，协调节点返回结果给客户端。 协调节点首先决定哪些文档 确实 需要被取回。例如，如果我们的查询指定了 { “from”: 90, “size”: 10 } ，最初的90个结果会被丢弃，只有从第91个开始的10个结果需要被取回。这些文档可能来自和最初搜索请求有关的一个、多个甚至全部分片。 协调节点会给持有相关文档的每个分片发送请求。分片加载文档体– _source 字段，如果有需要，用元数据丰富结果文档。 一旦协调节点接收到所有的结果文档，它就组装这些结果为单个响应返回给客户端。 ","date":"2019-09-29","objectID":"/es-wr/:0:2","tags":["elasticsearch"],"title":"Elasticsearch 读写总结","uri":"/es-wr/"},{"categories":null,"content":"写入过程 主流程 客户端向 Node 1 发送新建、索引或者删除请求。 节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3(因为分片 0 的主分片目前被分配在 Node 3 上)。 Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。 在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。 详细过程 P0 收到 document, 同时将数据写入到 内存 buffer 和 translog 中 每隔 1s 或 buffer 满时, buffer 中的数据会 refresh 到 segment 中, 而后进入os cache, 一旦 segment进入到 cache 中,其中的数据, 则可以被搜索到 refresh 时间可以手动设置, 也可以手动触发 refresh 清空buffer, translog不处理 随着 translog 不断增大, translog 每隔30分钟,或大到一定量时, 会触发commit操作 将 buffer 中内容刷新到 segment中, 并清空buffer 将一个commit point 写入到磁盘文件中, 标识此次commit 对应的 segment 执行 fsync 将 os cache 中的数据强制刷新到磁盘文件中 删除 translog 文件 |------------------||-------------------| | buffer in memory || translog in cache | |------------------||-------------------| || || refresh \\/ |------------------||------------------| | segment in cache || translog in file | |------------------||------------------| || || flush \\/ |------------------||-------------------| | segment in disk || del old translog | |------------------||-------------------| 服务宕机重启, translog 日志作用 translog 是先写入到 os cache 中, 然后每隔5s写入到磁盘文件中, 假如服务宕掉, 可能会失去5s数据, 也可以修改写入磁盘的时机, 但是可能会影响性能 translog 中记录的是数据操作信息, 在服务宕机重启时, 会读取 translog 磁盘文件, 然后将 translog 中的数据重新恢复到 segment中, 然后进行后续操作 segment merge 过程 segment 持续生成, 会导致 segment 不断变多, 占用文件句柄, cpu 资源等等 es 后台有一个专门的程序负责合并 segment, 将小的 segment 合成大的 segment, 同时写一个 commit point, 标识 新的 segment file. 打开新的 segment 供查询使用, 删除旧的 segment, segment 合并过程中, 被标记位 deleted 的 document 不会被合并. 即: 在合并 segment时, 才将 document 真正物理删除 合并的 segment 可以使磁盘上已经 commit 的索引 也可以是内存中还未 commit 的索引。 ","date":"2019-09-29","objectID":"/es-wr/:0:3","tags":["elasticsearch"],"title":"Elasticsearch 读写总结","uri":"/es-wr/"},{"categories":null,"content":"我们整个日志系统是使用 EFK 这套开源方案来搭建的，在以前的文章中我们有提到过。在这个体系中，最关键的组件当属 Elasticsearch, 它负责数据的存储和搜索。在这里我们总结一下 es 的读写原理。 首先我们需要了解一下文档和分片的关系 ","date":"2019-09-29","objectID":"/es-wr/:0:0","tags":["elasticsearch"],"title":"Elasticsearch 读写总结","uri":"/es-wr/"},{"categories":null,"content":"文档元数据 在 es 中我们把存储的一条日志(记录)称为文档，一个文档不仅仅包含它的数据 ，也包含元数据 —— 有关文档的信息。 三个必须的元数据元素如下： _index 文档在哪存放，_index 应该是因共同的特性被分组到一起的文档集合。比如相同格式的日志我们会放到同一个 index 中 _type 文档表示的对象类别，type 可以理解成是 index 的子类，不同的 type 包含的字段可能不同 _id 文档唯一标识, id 是一个字符串， 当它和 _index 以及 _type 组合就可以唯一确定 Elasticsearch 中的一个文档。 当创建一个新的文档，要么提供自己的 _id ，要么让 Elasticsearch 帮我们生成。 { \"_index\": \"ingress-access-2019.09.39\", \"_type\": \"_doc\", \"_id\": \"IY6KfG0BRpn5jI2pyyVf\", \"_version\": 1, \"_score\": null, \"_source\": { \"input\": { \"type\": \"log\" }, ... } } 倒排索引 传统的数据库每个字段存储单个值，但这对全文检索并不够。文本字段中的每个单词需要被搜索，对数据库意味着需要单个字段有索引多值(这里指单词)的能力。倒排索引就是这样一种数据结构，倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 Term | Doc 1 | Doc 2 | Doc 3 | ... ------------------------------------ brown | X | | X | ... fox | X | X | X | ... quick | X | X | | ... the | X | | X | ... 倒排索引相比特定词项出现过的文档列表，会包含更多其它信息。它会保存每一个词项出现过的文档总数， 在对应的文档中一个具体词项出现的总次数，词项在文档中的顺序，每个文档的长度，所有文档的平均长度，等等。这些统计信息允许 Elasticsearch 决定哪些词比其它词更重要，哪些文档比其它文档更重要。 分片 一个索引可以存储超出单个节点硬件限制的大量数据。比如，一个具有10亿文档的索引占据1TB的磁盘空间，而任一节点可能没有这样大的磁盘空间来存储或者单个节点处理搜索请求，响应会太慢。为了解决这个问题，Elasticsearch 提供了将索引划分成多片的能力，这些片叫做分片。当创建一个索引的时候，可以指定你分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引” 可以被放置到集群中的任何节点上。 分片之所以重要，主要有两方面的原因： 允许水平分割/扩展容量 允许并行操作，提高了性能/吞吐量 文档路由到分片 当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？ 首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的： shard = hash(routing) % number_of_primary_shards routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到 余数 。这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。 这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量, 因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。 ","date":"2019-09-29","objectID":"/es-wr/:0:1","tags":["elasticsearch"],"title":"Elasticsearch 读写总结","uri":"/es-wr/"},{"categories":null,"content":"搜索过程 查询阶段 查询阶段包含以下三个步骤: 客户端发送一个 search 请求到 Node 3 ， Node 3 会创建一个大小为 from + size 的空优先队列。 Node 3 将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 from + size 的本地有序优先队列中。 每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，也就是 Node 3 ，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。 当一个搜索请求被发送到某个节点时，这个节点就变成了协调节点。 这个节点的任务是广播查询请求到所有相关分片并将它们的响应整合成全局排序后的结果集合，这个结果集合会返回给客户端。 第一步是广播请求到索引中每一个节点的分片拷贝，查询请求可以被某个主分片或某个副本分片处理， 这就是为什么更多的副本（当结合更多的硬件）能够增加搜索吞吐率。 协调节点将在之后的请求中轮询所有的分片拷贝来分摊负载。 每个分片在本地执行查询请求并且创建一个长度为 from + size 的优先队列—也就是说，每个分片创建的结果集足够大，均可以满足全局的搜索请求。 分片返回一个轻量级的结果列表到协调节点，它仅包含文档 ID 集合以及任何排序需要用到的值，例如 _score 。 协调节点将这些分片级的结果合并到自己的有序优先队列里，它代表了全局排序结果集合。至此查询过程结束。 取回阶段 取回阶段由以下步骤构成： 协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。 每个分片加载并 丰富 文档，如果有需要的话，接着返回文档给协调节 点。 一旦所有的文档都被取回了，协调节点返回结果给客户端。 协调节点首先决定哪些文档 确实 需要被取回。例如，如果我们的查询指定了 { “from”: 90, “size”: 10 } ，最初的90个结果会被丢弃，只有从第91个开始的10个结果需要被取回。这些文档可能来自和最初搜索请求有关的一个、多个甚至全部分片。 协调节点会给持有相关文档的每个分片发送请求。分片加载文档体– _source 字段，如果有需要，用元数据丰富结果文档。 一旦协调节点接收到所有的结果文档，它就组装这些结果为单个响应返回给客户端。 ","date":"2019-09-29","objectID":"/es-wr/:0:2","tags":["elasticsearch"],"title":"Elasticsearch 读写总结","uri":"/es-wr/"},{"categories":null,"content":"写入过程 主流程 客户端向 Node 1 发送新建、索引或者删除请求。 节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3(因为分片 0 的主分片目前被分配在 Node 3 上)。 Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。 在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。 详细过程 P0 收到 document, 同时将数据写入到 内存 buffer 和 translog 中 每隔 1s 或 buffer 满时, buffer 中的数据会 refresh 到 segment 中, 而后进入os cache, 一旦 segment进入到 cache 中,其中的数据, 则可以被搜索到 refresh 时间可以手动设置, 也可以手动触发 refresh 清空buffer, translog不处理 随着 translog 不断增大, translog 每隔30分钟,或大到一定量时, 会触发commit操作 将 buffer 中内容刷新到 segment中, 并清空buffer 将一个commit point 写入到磁盘文件中, 标识此次commit 对应的 segment 执行 fsync 将 os cache 中的数据强制刷新到磁盘文件中 删除 translog 文件 |------------------||-------------------| | buffer in memory || translog in cache | |------------------||-------------------| || || refresh \\/ |------------------||------------------| | segment in cache || translog in file | |------------------||------------------| || || flush \\/ |------------------||-------------------| | segment in disk || del old translog | |------------------||-------------------| 服务宕机重启, translog 日志作用 translog 是先写入到 os cache 中, 然后每隔5s写入到磁盘文件中, 假如服务宕掉, 可能会失去5s数据, 也可以修改写入磁盘的时机, 但是可能会影响性能 translog 中记录的是数据操作信息, 在服务宕机重启时, 会读取 translog 磁盘文件, 然后将 translog 中的数据重新恢复到 segment中, 然后进行后续操作 segment merge 过程 segment 持续生成, 会导致 segment 不断变多, 占用文件句柄, cpu 资源等等 es 后台有一个专门的程序负责合并 segment, 将小的 segment 合成大的 segment, 同时写一个 commit point, 标识 新的 segment file. 打开新的 segment 供查询使用, 删除旧的 segment, segment 合并过程中, 被标记位 deleted 的 document 不会被合并. 即: 在合并 segment时, 才将 document 真正物理删除 合并的 segment 可以使磁盘上已经 commit 的索引 也可以是内存中还未 commit 的索引。 ","date":"2019-09-29","objectID":"/es-wr/:0:3","tags":["elasticsearch"],"title":"Elasticsearch 读写总结","uri":"/es-wr/"},{"categories":null,"content":"在处理过的 k8s 集群的众多问题中数据链路的问题往往是最复杂，最难排查的。这需要我们对 k8s 集群的网络通信的过程有着清晰的认识。Docker 通过 veth 虚拟网络设备以及 Linux bridge 实现了同一台主机上的容器网络通信，再通过 iptables 进行 NAT 实现了容器和外部网络的通信。而 k8s 作为容器编排平台，集群中有众多节点，并且每个节点上也部署了众多 pod, 要实现 pod 跨节点通信，就没有这么简单了。 ","date":"2019-08-22","objectID":"/flannel/:0:0","tags":["k8s","flannel","cni"],"title":"Flannel 在 k8s 中的使用","uri":"/flannel/"},{"categories":null,"content":"k8s 网络模型 为了解决 k8s 当中网络通信的问题，K8s 作为一个容器编排平台提出了 k8s 网络模型，但是并没有自己去实现，具体网络通信方案通过网络插件来实现。 其实 K8s 网络模型当中总共只作了三点要求： 运行在一个节点当中的 Pod 能在不经过NAT的情况下跟集群中所有的 Pod 进行通信 节点当中的客户端（system daemon、kubelet）能跟该节点当中的所有 Pod 进行通信 以 host network 模式运行在一个节点上的Pod能跟集群中所有的 Pod 进行通信 从 k8s 的网络模型我们可以看出来，在 k8s 当中希望做到的是每一个 Pod 都有一个在集群当中独一无二的IP，并且可以通过这个 IP 直接跟集群当中的其他 Pod 以及节点自身的网络进行通信，一句话概括就是 k8s 当中希望网络是扁平化的。 所以只要再符合 k8s 网络模型的要求，就可以以插件的方式在 k8s 集群中作为跨节点网络通信实现。目前比较流行的实现有 flannel, calico, weave, canal等。 这里结合我们自己在阿里云中的 k8s 集群的 CNI flannel 来解释 pod 之间的通信过程。 ","date":"2019-08-22","objectID":"/flannel/:0:1","tags":["k8s","flannel","cni"],"title":"Flannel 在 k8s 中的使用","uri":"/flannel/"},{"categories":null,"content":"Flannel 介绍 Flannel 是由 CoreOS 团队针对 k8s 设计的一个 Overlay Network(也就是将TCP数据包装在另一种网络包里面进行路由转发和通信) 实现，目前已经支持UDP、VxLAN、AWS VPC和GCE路由等数据转发方式，实现集群间网络通讯。 在 k8s 各个节点上通过DaemonSet的方式运行了一个 flannel 的Pod kubectl get ds -n kube-system app=flannel NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-flannel-ds 4 4 4 4 4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux 63d 每一个 flannel 的 Pod 当中都运行了一个 flanneld 进程，且 flanneld 的配置文件以 ConfigMap 的形式挂载到容器内的 /etc/kube-flannel/目录供 flanneld 使用。 flannel 通过在每一个节点上启动一个叫 flanneld 的进程，负责每一个节点上的子网划分，并将相关的配置信息如各个节点的子网网段、外部IP等保存到 etcd 当中，而具体的网络包转发交给具体的 Backend 来实现。 flanneld 可以在启动的时候通过配置文件来指定不同的 Backend 来进行网络通信，目前比较成熟的 Backend 有 VXLAN、host-gw以及UDP三种方式，也已经有诸如AWS，GCE and AliVPC这些还在实验阶段的 Backend。VXLAN 是目前官方最推崇的一种 Backend 实现方式，host-gw 一般用于对网络性能要求比较高的场景，但需要基础架构本身的支持，UDP 则一般用于 Debu g和一些比较老的不支持 VXLAN 的 Linux 内核。 flannel 默认使用 UDP 作为集群间通讯实现，如上图所示，flannel 通过 etcd 管理整个集群中所有节点与子网的映射关系，如上图所示，flannel分别为节点A和B划分了两个子网：10.1.15.0/16和10.1.20.0/16。同时通过修改docker启动参数，确保\bDocker启动的容器能够特定的网段中如10.1.15.1/24。 同一 Pod 实例容器间通信：对于 Pod 而言，其可以包含1~n个容器实例，这些容器实例共享 Pod 的存储以及网络资源，Pod 直接可以直接通过127.0.0.1进行通讯。其通过 Linux 的 Network Namespace\b 为这组容器实现了一个隔离网络。 相同主机上 Pod 间通信：对于 Pod 而言，每一个 Pod 实例都有一个独立的 Pod IP，该\b IP 是挂载到虚拟网卡（VETH）上，并且\b bridge 到docker0 的网卡上。以节点A为例，其节点上运行的 Pod 均在10.1.15.1/24的网段中，其属于相同网络，因此直接\b通过 docker0 进行通信。 对于跨节点间的 Pod 通信：以节点A和节点B通讯而言，由于不同节点 docker0 网卡的网段并不相同，因此 flannel 通过主机路由表的方式，将对节点B POD \b\bIP网段地址的访问路由到 flannel0 的网卡上。 而 flannel0 网卡的背后运行的则是 flannel 在每个节点上运行的进程 flanneld。由于 flannel通过 ETCD 维护了节点间所有网络的路由关系，原本容器将的数据报文，被 flanneld 封装成 UDP 协议，发送到了目标节点的 flanneld 进程，再对 udp 报文进行解包，后将数据发送到 docker0，从而实现跨主机的 Pod 通讯。 ","date":"2019-08-22","objectID":"/flannel/:0:2","tags":["k8s","flannel","cni"],"title":"Flannel 在 k8s 中的使用","uri":"/flannel/"},{"categories":null,"content":"公有云中的 flannel 上面我们解释了 flannel backend 为 UDP的通信过程。由于 UDP 作为backend存在明显的性能问题，一般生产环境不会使用这种方式，而在像 AWS 阿里云这样的公有云厂商提供的 k8s 集群服务中的 flannel 优先选择 vpc 作为backend, 这种方式其实类似 host-gw(Host Gateway)。 就像上面我们所说，flannel 通过 etcd 会统一为每个节点分配相应的网段， 下面为一个特定节点的网段 $ cat /run/flannel/subnet.env FLANNEL_NETWORK=172.20.0.0/16 FLANNEL_SUBNET=172.20.1.1/25 FLANNEL_MTU=1500 FLANNEL_IPMASQ=true 如上所示，flannel 建立了一个 172.20.0.0/16 的大网段，而这个节点则分配了 一个172.20.1.1/25的小网段。所以该节点上所有Pod的IP地址一定是在该网段中（172.20.1.1 ~ 172.20.1.126). 节点的网卡信息如下 cni0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 172.20.1.1 netmask 255.255.255.128 broadcast 0.0.0.0 ether 0a:58:ac:14:01:01 txqueuelen 1000 (Ethernet) RX packets 78766736 bytes 6621717501 (6.1 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 106139605 bytes 12076081112 (11.2 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 docker0: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 inet 169.254.123.1 netmask 255.255.255.0 broadcast 169.254.123.255 ether 02:42:ed:90:79:ea txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 172.19.220.255 netmask 255.255.240.0 broadcast 172.19.223.255 ether 00:16:3e:1a:6b:dc txqueuelen 1000 (Ethernet) RX packets 119587253 bytes 30187869460 (28.1 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 84789814 bytes 20787336532 (19.3 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u003cUP,LOOPBACK,RUNNING\u003e mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 1191 bytes 172882 (168.8 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1191 bytes 172882 (168.8 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 veth005c911b: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether d2:de:24:f6:e2:13 txqueuelen 0 (Ethernet) RX packets 66417441 bytes 6329850773 (5.8 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 95129695 bytes 8452269811 (7.8 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 veth027b91b9: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether b6:e3:ee:c6:86:2f txqueuelen 0 (Ethernet) RX packets 1746691 bytes 150672105 (143.6 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1790627 bytes 164054145 (156.4 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 其他veth省略 其中 veth177b8616 是每个 Pod 的虚拟网卡，并且连接到网桥 cni0： $ brctl show cni0 bridge name bridge id STP enabled interfaces cni0 8000.0a58ac140101 no veth005c911b veth027b91b9 veth4963357e veth6b1d3200 veth6c793e18 veth8e551bfa 数据包出 该点上的 pod 内的数据包通过网桥方式发送到 cni0. cni0怎么转发数据包呢？这里是使用的就是在 vpc 中建立的路由规则 例如数据包的目标 pod ip 是 172.20.1.43 则会匹配到图中的路由规则，然后转发到 pod 所在的节点。 数据包入 此时，数据包已经到达节点，那么怎么发送到目标 pod 呢？ 查看接收流量主机的路由规则： $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.19.223.253 0.0.0.0 UG 0 0 0 eth0 169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0 169.254.123.0 0.0.0.0 255.255.255.0 U 0 0 0 docker0 172.19.208.0 0.0.0.0 255.255.240.0 U 0 0 0 eth0 172.20.1.0 0.0.0.0 255.255.255.128 U 0 0 0 cni0 根据主机路由表规则，发送到 pod 172.16.1.43 的请求会落到路由表 172.20.1.0 0.0.0.0 255.255.255.128 U 0 0 0 cni0 然后数据包被发给网桥 cni0 然后经 veth 虚拟网卡 发送到 pod. 上述过程我们用下面这张示意图来总结 像开始我们说的那样一个主机中的容器通过 bridge 模式就能实现容器与容器，容器与主机的通信。flannel 更像是这种模式的扩展，集群的主机如果都在一个子网内，就配置路由转发过去；若是不在一个子网内，就通过隧道转发过去。 ","date":"2019-08-22","objectID":"/flannel/:0:3","tags":["k8s","flannel","cni"],"title":"Flannel 在 k8s 中的使用","uri":"/flannel/"},{"categories":null,"content":"在处理过的 k8s 集群的众多问题中数据链路的问题往往是最复杂，最难排查的。这需要我们对 k8s 集群的网络通信的过程有着清晰的认识。Docker 通过 veth 虚拟网络设备以及 Linux bridge 实现了同一台主机上的容器网络通信，再通过 iptables 进行 NAT 实现了容器和外部网络的通信。而 k8s 作为容器编排平台，集群中有众多节点，并且每个节点上也部署了众多 pod, 要实现 pod 跨节点通信，就没有这么简单了。 ","date":"2019-08-22","objectID":"/flannel/:0:0","tags":["k8s","flannel","cni"],"title":"Flannel 在 k8s 中的使用","uri":"/flannel/"},{"categories":null,"content":"k8s 网络模型 为了解决 k8s 当中网络通信的问题，K8s 作为一个容器编排平台提出了 k8s 网络模型，但是并没有自己去实现，具体网络通信方案通过网络插件来实现。 其实 K8s 网络模型当中总共只作了三点要求： 运行在一个节点当中的 Pod 能在不经过NAT的情况下跟集群中所有的 Pod 进行通信 节点当中的客户端（system daemon、kubelet）能跟该节点当中的所有 Pod 进行通信 以 host network 模式运行在一个节点上的Pod能跟集群中所有的 Pod 进行通信 从 k8s 的网络模型我们可以看出来，在 k8s 当中希望做到的是每一个 Pod 都有一个在集群当中独一无二的IP，并且可以通过这个 IP 直接跟集群当中的其他 Pod 以及节点自身的网络进行通信，一句话概括就是 k8s 当中希望网络是扁平化的。 所以只要再符合 k8s 网络模型的要求，就可以以插件的方式在 k8s 集群中作为跨节点网络通信实现。目前比较流行的实现有 flannel, calico, weave, canal等。 这里结合我们自己在阿里云中的 k8s 集群的 CNI flannel 来解释 pod 之间的通信过程。 ","date":"2019-08-22","objectID":"/flannel/:0:1","tags":["k8s","flannel","cni"],"title":"Flannel 在 k8s 中的使用","uri":"/flannel/"},{"categories":null,"content":"Flannel 介绍 Flannel 是由 CoreOS 团队针对 k8s 设计的一个 Overlay Network(也就是将TCP数据包装在另一种网络包里面进行路由转发和通信) 实现，目前已经支持UDP、VxLAN、AWS VPC和GCE路由等数据转发方式，实现集群间网络通讯。 在 k8s 各个节点上通过DaemonSet的方式运行了一个 flannel 的Pod kubectl get ds -n kube-system app=flannel NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-flannel-ds 4 4 4 4 4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux 63d 每一个 flannel 的 Pod 当中都运行了一个 flanneld 进程，且 flanneld 的配置文件以 ConfigMap 的形式挂载到容器内的 /etc/kube-flannel/目录供 flanneld 使用。 flannel 通过在每一个节点上启动一个叫 flanneld 的进程，负责每一个节点上的子网划分，并将相关的配置信息如各个节点的子网网段、外部IP等保存到 etcd 当中，而具体的网络包转发交给具体的 Backend 来实现。 flanneld 可以在启动的时候通过配置文件来指定不同的 Backend 来进行网络通信，目前比较成熟的 Backend 有 VXLAN、host-gw以及UDP三种方式，也已经有诸如AWS，GCE and AliVPC这些还在实验阶段的 Backend。VXLAN 是目前官方最推崇的一种 Backend 实现方式，host-gw 一般用于对网络性能要求比较高的场景，但需要基础架构本身的支持，UDP 则一般用于 Debu g和一些比较老的不支持 VXLAN 的 Linux 内核。 flannel 默认使用 UDP 作为集群间通讯实现，如上图所示，flannel 通过 etcd 管理整个集群中所有节点与子网的映射关系，如上图所示，flannel分别为节点A和B划分了两个子网：10.1.15.0/16和10.1.20.0/16。同时通过修改docker启动参数，确保\bDocker启动的容器能够特定的网段中如10.1.15.1/24。 同一 Pod 实例容器间通信：对于 Pod 而言，其可以包含1~n个容器实例，这些容器实例共享 Pod 的存储以及网络资源，Pod 直接可以直接通过127.0.0.1进行通讯。其通过 Linux 的 Network Namespace\b 为这组容器实现了一个隔离网络。 相同主机上 Pod 间通信：对于 Pod 而言，每一个 Pod 实例都有一个独立的 Pod IP，该\b IP 是挂载到虚拟网卡（VETH）上，并且\b bridge 到docker0 的网卡上。以节点A为例，其节点上运行的 Pod 均在10.1.15.1/24的网段中，其属于相同网络，因此直接\b通过 docker0 进行通信。 对于跨节点间的 Pod 通信：以节点A和节点B通讯而言，由于不同节点 docker0 网卡的网段并不相同，因此 flannel 通过主机路由表的方式，将对节点B POD \b\bIP网段地址的访问路由到 flannel0 的网卡上。 而 flannel0 网卡的背后运行的则是 flannel 在每个节点上运行的进程 flanneld。由于 flannel通过 ETCD 维护了节点间所有网络的路由关系，原本容器将的数据报文，被 flanneld 封装成 UDP 协议，发送到了目标节点的 flanneld 进程，再对 udp 报文进行解包，后将数据发送到 docker0，从而实现跨主机的 Pod 通讯。 ","date":"2019-08-22","objectID":"/flannel/:0:2","tags":["k8s","flannel","cni"],"title":"Flannel 在 k8s 中的使用","uri":"/flannel/"},{"categories":null,"content":"公有云中的 flannel 上面我们解释了 flannel backend 为 UDP的通信过程。由于 UDP 作为backend存在明显的性能问题，一般生产环境不会使用这种方式，而在像 AWS 阿里云这样的公有云厂商提供的 k8s 集群服务中的 flannel 优先选择 vpc 作为backend, 这种方式其实类似 host-gw(Host Gateway)。 就像上面我们所说，flannel 通过 etcd 会统一为每个节点分配相应的网段， 下面为一个特定节点的网段 $ cat /run/flannel/subnet.env FLANNEL_NETWORK=172.20.0.0/16 FLANNEL_SUBNET=172.20.1.1/25 FLANNEL_MTU=1500 FLANNEL_IPMASQ=true 如上所示，flannel 建立了一个 172.20.0.0/16 的大网段，而这个节点则分配了 一个172.20.1.1/25的小网段。所以该节点上所有Pod的IP地址一定是在该网段中（172.20.1.1 ~ 172.20.1.126). 节点的网卡信息如下 cni0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 172.20.1.1 netmask 255.255.255.128 broadcast 0.0.0.0 ether 0a:58:ac:14:01:01 txqueuelen 1000 (Ethernet) RX packets 78766736 bytes 6621717501 (6.1 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 106139605 bytes 12076081112 (11.2 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 docker0: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 inet 169.254.123.1 netmask 255.255.255.0 broadcast 169.254.123.255 ether 02:42:ed:90:79:ea txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 172.19.220.255 netmask 255.255.240.0 broadcast 172.19.223.255 ether 00:16:3e:1a:6b:dc txqueuelen 1000 (Ethernet) RX packets 119587253 bytes 30187869460 (28.1 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 84789814 bytes 20787336532 (19.3 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u003cUP,LOOPBACK,RUNNING\u003e mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 1191 bytes 172882 (168.8 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1191 bytes 172882 (168.8 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 veth005c911b: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether d2:de:24:f6:e2:13 txqueuelen 0 (Ethernet) RX packets 66417441 bytes 6329850773 (5.8 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 95129695 bytes 8452269811 (7.8 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 veth027b91b9: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether b6:e3:ee:c6:86:2f txqueuelen 0 (Ethernet) RX packets 1746691 bytes 150672105 (143.6 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1790627 bytes 164054145 (156.4 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 其他veth省略 其中 veth177b8616 是每个 Pod 的虚拟网卡，并且连接到网桥 cni0： $ brctl show cni0 bridge name bridge id STP enabled interfaces cni0 8000.0a58ac140101 no veth005c911b veth027b91b9 veth4963357e veth6b1d3200 veth6c793e18 veth8e551bfa 数据包出 该点上的 pod 内的数据包通过网桥方式发送到 cni0. cni0怎么转发数据包呢？这里是使用的就是在 vpc 中建立的路由规则 例如数据包的目标 pod ip 是 172.20.1.43 则会匹配到图中的路由规则，然后转发到 pod 所在的节点。 数据包入 此时，数据包已经到达节点，那么怎么发送到目标 pod 呢？ 查看接收流量主机的路由规则： $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.19.223.253 0.0.0.0 UG 0 0 0 eth0 169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0 169.254.123.0 0.0.0.0 255.255.255.0 U 0 0 0 docker0 172.19.208.0 0.0.0.0 255.255.240.0 U 0 0 0 eth0 172.20.1.0 0.0.0.0 255.255.255.128 U 0 0 0 cni0 根据主机路由表规则，发送到 pod 172.16.1.43 的请求会落到路由表 172.20.1.0 0.0.0.0 255.255.255.128 U 0 0 0 cni0 然后数据包被发给网桥 cni0 然后经 veth 虚拟网卡 发送到 pod. 上述过程我们用下面这张示意图来总结 像开始我们说的那样一个主机中的容器通过 bridge 模式就能实现容器与容器，容器与主机的通信。flannel 更像是这种模式的扩展，集群的主机如果都在一个子网内，就配置路由转发过去；若是不在一个子网内，就通过隧道转发过去。 ","date":"2019-08-22","objectID":"/flannel/:0:3","tags":["k8s","flannel","cni"],"title":"Flannel 在 k8s 中的使用","uri":"/flannel/"},{"categories":null,"content":"背景 某天深夜我们 k8s 生产环境集群发出了一条报警 “某一个节点 unavailable”。问题出现后立马查看节点 CPU, Memory, Disk 状态，发现 CPU 状态正常，但是 Memory 和 Disk 的信息收集不到了(crontab 定时执行的脚本上报)，接着登录机器，没问题能登录进去(一般测试环境出现节点不可用，是由于负载长时间内太高打垮了机器，机器也会登录不了)。随便执行一个命令 tab 补全报错 “Cannot Create File: Read-Only File System”，显然文件系统变成了只读状态，写操作都会失败，意味着这个实例就是不可用的状态。 Cannot Create File: Read-Only File System 可能的原因是什么呢？ 在回答这个问题之前我们先得了解一下 Linux 文件系统是怎样的一个构成。 ","date":"2019-07-22","objectID":"/ufs-fsck/:1:0","tags":["file system","fsck","ext4"],"title":"从“Read-Only File System” 说到 Linux 文件系统结构","uri":"/ufs-fsck/"},{"categories":null,"content":"Linux 文件系统结构 目前大多Linux发行版本默认使用的文件系统一般是ext4, 从名字我们也能看出肯定还有 ext3, ext2, ext。每一代都是上一代的改进版，从 ext2 到 ext4根本设计上是没有太大变化的。 首先我们都知道文件最终是保存在硬盘上的, 磁盘是由若干扇区组成，若干个扇区组成一个块(ext2默认是由8个扇区组成一个块，即4kb), 磁盘分成若干个分区，每个分区由若干个块组成。 Boot Block EXT2 的头 1024 Bytes 称为 Boot Block，引导块，预留给 VBR，即分区的引导扇区，不受 EXT2 文件系统的管理。 Block Group 其余的空间被划分为多个块，这些块被编成 块组，即 Block Groups（BG）。任何给定文件的数据通常都尽可能地包含在一个块组中，这样，在读取连续大文件时，会减少磁盘寻道次数。 每个块组包含： 超级块：1 个块 块组描述符表：N 个块 块位图：1 个块 Inode 位图：1 个块 Inode 表：N 个块 数据块：N 个块 超级块 超级块 用于记录文件系统的 整体 信息： 块与 Inode 的总数 块与 Inode 已用、可用数量 块与 Inode 的大小 文件系统的挂载时间、最近写入数据的时间、最近校验的时间 文件系统当前挂载状态：称 Valid Bit，0 表示已挂载，1 表示未挂载 超级块 保存在整个系统第一个 BG 中，后续的每个块组都会在开头保存超级块的复本，以备不时之需。 通常在系统启动中使用的是文件系统中第一个超级块。 块组描述符表 Group Descriptor Table 这张表包含了 整个系统所有块组的信息，由一个接一个的块组描述符组成。每个块组都包含这张表的复本，紧跟超级块，作用也一样，用于修复系统。 正常工作时 EXT2 只使用第一个复本，即块组0 中的。 块组描述符包含： 该块组的块位图、Inode 位图、Inode 表的位置，可用块数量，可用 Inode 数量，已用目录数量。 块位图 Block Bitmap 该块组中块的分配位图，每一位代表一个块的当前状态，1 为已用，0 为可用。据此位图来分配或回收块。 Inode 位图 该块组中 Inode 的分配位图，工作方式同块位图。据此位图来分配或回收 Inode。 Inode 表 Inode 表是当前块组中所有 Inode 的集合，用于追踪块组中所有文件的元数据。 该表会被操作系统频繁地访问，因此应尽可能减少读取该表所用的时间。 Inode 是一个非常重要的概念 inode 就是index-node（索引节点）的简写。 inode 是位于磁盘上的一个 256 字节的块，用于存储和该 inode 对应的文件的相关数据。这些数据包含了文件的大小、文件的所有者和所属组的用户 ID、文件模式（即访问权限）以及三个时间戳用于指定：该文件最后的访问时间、该文件的最后修改时间和该 inode 中的数据的最后修改时间。 同时，这个 inode 还包含了位置数据，指向了其所对应的文件数据在硬盘中的位置。当查看某个文件时，会先从 inode table 中查出文件属性及数据存放点，再从数据块中读取数据。 ","date":"2019-07-22","objectID":"/ufs-fsck/:2:0","tags":["file system","fsck","ext4"],"title":"从“Read-Only File System” 说到 Linux 文件系统结构","uri":"/ufs-fsck/"},{"categories":null,"content":"解决问题 有了以上对文件系统的了解以后，我们再来分析出现的问题：文件系统变成只读状态 磁盘出现“read only file system”的原因有很多种，可能是文件系统数据块出现不一致导致的，也有可能是磁盘故障造成的。主流的ext3、ext4文件系统都有很强的自我修复机制，对于简单的错误，文件系统一般可自行修复，当遇到致命错误无法修复时，文件系统为了保证数据一致性和安全，会暂时屏蔽文件系统的写操作，将文件系统变为只读，进而出现了上面的“read only file system”现象。 那么关键就在文件系统的一致性，因为每时每刻系统都在创建、修改和删除文件。每次修改文件时，操作系统都会执行一系列文件系统更新。如果这些更新被可靠地写入磁盘，便会产生一致的文件系统。 用户程序执行更改文件系统的操作（例如写入操作）时，会先将要写入的数据复制到内核中的核心缓冲区。通常，以异步方式处理磁盘更新。尽管在写入系统调用返回很长时间之后才会写入数据，但是允许用户进程继续执行。这样，在任何给定时间，由于文件系统驻留在磁盘上，因此它将滞后于核心信息所表示的文件系统状态。 当缓冲区需要用于其他用途时，或者内核自动运行 fsflush 守护进程时，将更新磁盘信息以反映核心信息。 如果在未写出核心信息的情况下停止系统，则磁盘上的文件系统可能会处于不一致状态。 确认一下 EBS volume 的状态 $ mount -l | grep nvme /dev/nvme0n1p1 on / type ext4 (ro,relatime,data=ordered) [cloudimg-rootfs] 我们看到文件系统状态确实是 ro， 正常应该是 rw 修复一致性错误 FSCK( File System Consistency Check) 是一个很重要的 Linux/Unix 工具，它用于检测并修复文件系统中的错误 它可以进行三种模式的操作， 查错并在发现错误时由用户决定如何处理， 查错并自动修复， 查错但在发现错误时只显示错误而不进行修复 fsck 支持的选项有， -p 自动修复（不询问） -n 不对文件系统做出改动 -y 对所有问题都回答 “yes” -c 检查所有的坏块并将之添加到坏块列表中 -f 即使文件系统标记为 clean 也强制进行检查 -v 输出详细信息 -b superblock 使用替代的超级块 -B blocksize 指定超级块的块大小 -j external_journal 指定外部日志的位置 -l bad_blocks_file 添加到指定的坏块列表（文件） -L bad_blocks_file 指定坏块列表（文件） 我们文件系统类型是 ext4，使用 fsck.ext4 fsck.ext4 -y /dev/nvme0n1p1 修复完成，重启 sync; reboot (sync 是用来强制将内存中的文件缓冲写入磁盘，更新块信息） 如果不是硬件问题，绝大部分文件系统的一致性错误都能被 fsck 检查出来并修复。 我们使用了 fsck 自动修复后，文件系统恢复成正常状态，实例得以恢复。 ","date":"2019-07-22","objectID":"/ufs-fsck/:3:0","tags":["file system","fsck","ext4"],"title":"从“Read-Only File System” 说到 Linux 文件系统结构","uri":"/ufs-fsck/"},{"categories":null,"content":"背景 某天深夜我们 k8s 生产环境集群发出了一条报警 “某一个节点 unavailable”。问题出现后立马查看节点 CPU, Memory, Disk 状态，发现 CPU 状态正常，但是 Memory 和 Disk 的信息收集不到了(crontab 定时执行的脚本上报)，接着登录机器，没问题能登录进去(一般测试环境出现节点不可用，是由于负载长时间内太高打垮了机器，机器也会登录不了)。随便执行一个命令 tab 补全报错 “Cannot Create File: Read-Only File System”，显然文件系统变成了只读状态，写操作都会失败，意味着这个实例就是不可用的状态。 Cannot Create File: Read-Only File System 可能的原因是什么呢？ 在回答这个问题之前我们先得了解一下 Linux 文件系统是怎样的一个构成。 ","date":"2019-07-22","objectID":"/ufs-fsck/:1:0","tags":["file system","fsck","ext4"],"title":"从“Read-Only File System” 说到 Linux 文件系统结构","uri":"/ufs-fsck/"},{"categories":null,"content":"Linux 文件系统结构 目前大多Linux发行版本默认使用的文件系统一般是ext4, 从名字我们也能看出肯定还有 ext3, ext2, ext。每一代都是上一代的改进版，从 ext2 到 ext4根本设计上是没有太大变化的。 首先我们都知道文件最终是保存在硬盘上的, 磁盘是由若干扇区组成，若干个扇区组成一个块(ext2默认是由8个扇区组成一个块，即4kb), 磁盘分成若干个分区，每个分区由若干个块组成。 Boot Block EXT2 的头 1024 Bytes 称为 Boot Block，引导块，预留给 VBR，即分区的引导扇区，不受 EXT2 文件系统的管理。 Block Group 其余的空间被划分为多个块，这些块被编成 块组，即 Block Groups（BG）。任何给定文件的数据通常都尽可能地包含在一个块组中，这样，在读取连续大文件时，会减少磁盘寻道次数。 每个块组包含： 超级块：1 个块 块组描述符表：N 个块 块位图：1 个块 Inode 位图：1 个块 Inode 表：N 个块 数据块：N 个块 超级块 超级块 用于记录文件系统的 整体 信息： 块与 Inode 的总数 块与 Inode 已用、可用数量 块与 Inode 的大小 文件系统的挂载时间、最近写入数据的时间、最近校验的时间 文件系统当前挂载状态：称 Valid Bit，0 表示已挂载，1 表示未挂载 超级块 保存在整个系统第一个 BG 中，后续的每个块组都会在开头保存超级块的复本，以备不时之需。 通常在系统启动中使用的是文件系统中第一个超级块。 块组描述符表 Group Descriptor Table 这张表包含了 整个系统所有块组的信息，由一个接一个的块组描述符组成。每个块组都包含这张表的复本，紧跟超级块，作用也一样，用于修复系统。 正常工作时 EXT2 只使用第一个复本，即块组0 中的。 块组描述符包含： 该块组的块位图、Inode 位图、Inode 表的位置，可用块数量，可用 Inode 数量，已用目录数量。 块位图 Block Bitmap 该块组中块的分配位图，每一位代表一个块的当前状态，1 为已用，0 为可用。据此位图来分配或回收块。 Inode 位图 该块组中 Inode 的分配位图，工作方式同块位图。据此位图来分配或回收 Inode。 Inode 表 Inode 表是当前块组中所有 Inode 的集合，用于追踪块组中所有文件的元数据。 该表会被操作系统频繁地访问，因此应尽可能减少读取该表所用的时间。 Inode 是一个非常重要的概念 inode 就是index-node（索引节点）的简写。 inode 是位于磁盘上的一个 256 字节的块，用于存储和该 inode 对应的文件的相关数据。这些数据包含了文件的大小、文件的所有者和所属组的用户 ID、文件模式（即访问权限）以及三个时间戳用于指定：该文件最后的访问时间、该文件的最后修改时间和该 inode 中的数据的最后修改时间。 同时，这个 inode 还包含了位置数据，指向了其所对应的文件数据在硬盘中的位置。当查看某个文件时，会先从 inode table 中查出文件属性及数据存放点，再从数据块中读取数据。 ","date":"2019-07-22","objectID":"/ufs-fsck/:2:0","tags":["file system","fsck","ext4"],"title":"从“Read-Only File System” 说到 Linux 文件系统结构","uri":"/ufs-fsck/"},{"categories":null,"content":"解决问题 有了以上对文件系统的了解以后，我们再来分析出现的问题：文件系统变成只读状态 磁盘出现“read only file system”的原因有很多种，可能是文件系统数据块出现不一致导致的，也有可能是磁盘故障造成的。主流的ext3、ext4文件系统都有很强的自我修复机制，对于简单的错误，文件系统一般可自行修复，当遇到致命错误无法修复时，文件系统为了保证数据一致性和安全，会暂时屏蔽文件系统的写操作，将文件系统变为只读，进而出现了上面的“read only file system”现象。 那么关键就在文件系统的一致性，因为每时每刻系统都在创建、修改和删除文件。每次修改文件时，操作系统都会执行一系列文件系统更新。如果这些更新被可靠地写入磁盘，便会产生一致的文件系统。 用户程序执行更改文件系统的操作（例如写入操作）时，会先将要写入的数据复制到内核中的核心缓冲区。通常，以异步方式处理磁盘更新。尽管在写入系统调用返回很长时间之后才会写入数据，但是允许用户进程继续执行。这样，在任何给定时间，由于文件系统驻留在磁盘上，因此它将滞后于核心信息所表示的文件系统状态。 当缓冲区需要用于其他用途时，或者内核自动运行 fsflush 守护进程时，将更新磁盘信息以反映核心信息。 如果在未写出核心信息的情况下停止系统，则磁盘上的文件系统可能会处于不一致状态。 确认一下 EBS volume 的状态 $ mount -l | grep nvme /dev/nvme0n1p1 on / type ext4 (ro,relatime,data=ordered) [cloudimg-rootfs] 我们看到文件系统状态确实是 ro， 正常应该是 rw 修复一致性错误 FSCK( File System Consistency Check) 是一个很重要的 Linux/Unix 工具，它用于检测并修复文件系统中的错误 它可以进行三种模式的操作， 查错并在发现错误时由用户决定如何处理， 查错并自动修复， 查错但在发现错误时只显示错误而不进行修复 fsck 支持的选项有， -p 自动修复（不询问） -n 不对文件系统做出改动 -y 对所有问题都回答 “yes” -c 检查所有的坏块并将之添加到坏块列表中 -f 即使文件系统标记为 clean 也强制进行检查 -v 输出详细信息 -b superblock 使用替代的超级块 -B blocksize 指定超级块的块大小 -j external_journal 指定外部日志的位置 -l bad_blocks_file 添加到指定的坏块列表（文件） -L bad_blocks_file 指定坏块列表（文件） 我们文件系统类型是 ext4，使用 fsck.ext4 fsck.ext4 -y /dev/nvme0n1p1 修复完成，重启 sync; reboot (sync 是用来强制将内存中的文件缓冲写入磁盘，更新块信息） 如果不是硬件问题，绝大部分文件系统的一致性错误都能被 fsck 检查出来并修复。 我们使用了 fsck 自动修复后，文件系统恢复成正常状态，实例得以恢复。 ","date":"2019-07-22","objectID":"/ufs-fsck/:3:0","tags":["file system","fsck","ext4"],"title":"从“Read-Only File System” 说到 Linux 文件系统结构","uri":"/ufs-fsck/"},{"categories":null,"content":"在 Java 开发中，多线程的应用十分频繁，所以在多线程协作完成同一个任务的情况下，线程间同步的需求会非常常见。比如这样一个场景，主线程需要从10个站点下载数据，此时新建一个大小为 5 的线程池来分别从站点下载数据，主线程则必须等到子线程全部下载完成后，拿到完整数据才能进行下一步的处理。Java 给我们提供了一个类 CountDownLatch 可以方便的帮助我们来实现这样的场景。 ","date":"2019-06-24","objectID":"/java-aqs/:0:0","tags":["java","线程同步"],"title":"Java 线程同步实现探究","uri":"/java-aqs/"},{"categories":null,"content":"代码示例 public class CountdownLatchDe { private static CountDownLatch guests = new CountDownLatch(5); public static void main(String[] args) throws InterruptedException { ExecutorService executorService = Executors.newFixedThreadPool(5); for (int i = 0; i \u003c 5; i++) { executorService.execute(() -\u003e { System.out.println(Thread.currentThread().getName() + \": 游戏开始\"); System.out.println(Thread.currentThread().getName() + \": 完成\"); guests.countDown(); }); } guests.await(); System.out.println(Thread.currentThread().getName() + \"： 全部结束, 开始一下项\"); executorService.shutdown(); } } 输出如下: 上面这个 demo 展示了这样一个场景，在一场有主持人的活动中，主持人必须等参与者都完成了一个游戏，才能宣布进入下一个环节。主线程模拟的是主持人，主线程调用方法await 会被阻塞住，子线程模拟的参与者每次执行方法countDown, 则CountDownLatch 维护的计数器会减一，直到全部子线程执行完，计数器也变为0，主线程才被唤醒可以继续往下执行。 ","date":"2019-06-24","objectID":"/java-aqs/:0:1","tags":["java","线程同步"],"title":"Java 线程同步实现探究","uri":"/java-aqs/"},{"categories":null,"content":"源码查看 属性和构造函数 public class CountDownLatch { // 这是 CountDownLatch 定义的一个继承自 AQS 的内部类，数据结构就是使用 AQS 提供的两个队列 sync queue 和 condition queue private final Sync sync; // 构造给定计数的 CountDownLatch, 完成 sync 初识化 public CountDownLatch(int count) { if (count \u003c 0) throw new IllegalArgumentException(\"count \u003c 0\"); // 初始化状态数 this.sync = new Sync(count); } } 核心函数 // 会阻塞调用线程一直到计数器为0或者被中断 public void await() throws InterruptedException { // 真正调用 sync 对象的方法 sync.acquireSharedInterruptibly(1); } await 主要调用链如下所示 总结一下核心流程: 判读当前计数器是否为 0； 计数器不是0， 调用 doAcquireSharedInterruptibly 加入到同步阻塞队列； 尝试获取锁，获取失败则调用 shouldParkAfterFailedAcquire 判断是否需要阻塞等待，如果需要阻塞等待则调用 parkAndCheckInterrupt 阻塞当前线程并让出cup资源直到被前一个节点唤醒 // 计数器减一，如果减为0，则释放所有等待线程 public void countDown() { sync.releaseShared(1); } countDown 主要调用链如下所示 总结一下核心流程(doReleaseShared)： 获取头结点，头结点不为空且有下一个结点； 头结点状态为 Signal, 调用 unparkSuccessor 唤醒； 获取当前节点状态，当前节点正常情况则设置成0，获取下一个状态为非 CANCELLED的节点，调用 LockSupport.unpark 唤醒此结点 考虑另外一个场景，子线程需要同时执行一个操作，怎么让这些线程同时开始呢？ 我们可以借助 Java 提供的 CyclicBarrier 来实现此目的。 ","date":"2019-06-24","objectID":"/java-aqs/:0:2","tags":["java","线程同步"],"title":"Java 线程同步实现探究","uri":"/java-aqs/"},{"categories":null,"content":"代码实例 public class CyclicBarrierDe { private static CyclicBarrier barrier = new CyclicBarrier(5, () -\u003e { System.out.println(\"下一阶段 go -\u003e\");}); public static void main(String[] args) { ExecutorService executorService = Executors.newFixedThreadPool(5); for (int i = 0; i \u003c 5; i++) { executorService.execute(() -\u003e { try { System.out.println(Thread.currentThread().getName() + \": 热身活动\"); barrier.await(); System.out.println(Thread.currentThread().getName() + \": 比赛开始\"); System.out.println(Thread.currentThread().getName() + \": 比赛结束\"); barrier.await(); System.out.println(Thread.currentThread().getName() + \": 参加记者招待会\"); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } }); } } } 输出如下： 上面的 demo 展示了每个子线程都调用了 await 则都到达了 barrier, 才能执行后续的代码，否则被阻塞住。 ","date":"2019-06-24","objectID":"/java-aqs/:0:3","tags":["java","线程同步"],"title":"Java 线程同步实现探究","uri":"/java-aqs/"},{"categories":null,"content":"源码查看 属性和构造函数 public class CyclicBarrier { /** The lock for guarding barrier entry */ // 可重入锁 private final ReentrantLock lock = new ReentrantLock(); /** Condition to wait on until tripped */ // 条件队列 private final Condition trip = lock.newCondition(); /** The number of parties */ // 参与的线程数量 private final int parties; /* The command to run when tripped */ // 由最后一个进入 barrier 的线程执行的操作 private final Runnable barrierCommand; /** The current generation */ // 当前代 private Generation generation = new Generation(); // 正在等待进入屏障的线程数量 private int count; public CyclicBarrier(int parties, Runnable barrierAction) { // 参与的线程数量小于等于0，抛出异常 if (parties \u003c= 0) throw new IllegalArgumentException(); // 设置parties this.parties = parties; // 设置count this.count = parties; // 设置barrierCommand this.barrierCommand = barrierAction; } public CyclicBarrier(int parties) { // 调用含有两个参数的构造函数 this(parties, null); } } 核心函数 private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { // 保存当前锁 final ReentrantLock lock = this.lock; // 锁定 lock.lock(); try { // 保存当前代 final Generation g = generation; if (g.broken) // 屏障被破坏，抛出异常 throw new BrokenBarrierException(); if (Thread.interrupted()) { // 线程被中断 // 损坏当前屏障，并且唤醒所有的线程，只有拥有锁的时候才会调用 breakBarrier(); // 抛出异常 throw new InterruptedException(); } // 减少正在等待进入屏障的线程数量 int index = --count; if (index == 0) { // 正在等待进入屏障的线程数量为0，所有线程都已经进入 // 运行的动作标识 boolean ranAction = false; try { // 保存运行动作 final Runnable command = barrierCommand; if (command != null) // 动作不为空 // 运行 command.run(); // 设置ranAction状态 ranAction = true; // 进入下一代 nextGeneration(); return 0; } finally { if (!ranAction) // 没有运行的动作 // 损坏当前屏障 breakBarrier(); } } // loop until tripped, broken, interrupted, or timed out // 无限循环 for (;;) { try { if (!timed) // 没有设置等待时间 // 等待 trip.await(); else if (nanos \u003e 0L) // 设置了等待时间，并且等待时间大于0 // 等待指定时长 nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation \u0026\u0026 ! g.broken) { // 等于当前代并且屏障没有被损坏 // 损坏当前屏障 breakBarrier(); // 抛出异常 throw ie; } else { // 不等于当前带后者是屏障被损坏 // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // \"belong\" to subsequent execution. // 中断当前线程 Thread.currentThread().interrupt(); } } if (g.broken) // 屏障被损坏，抛出异常 throw new BrokenBarrierException(); if (g != generation) // 不等于当前代 // 返回索引 return index; if (timed \u0026\u0026 nanos \u003c= 0L) { // 设置了等待时间，并且等待时间小于0 // 损坏屏障 breakBarrier(); // 抛出异常 throw new TimeoutException(); } } } finally { // 释放锁 lock.unlock(); } } CyclicBarrier 与 CountDownLatch 对比: 二者都可以用来做线程同步； CyclicBarrier 到达 barrier 后唤醒全部线程，CountDownLatch 计数为0，则是一个个传播唤醒； CyclicBarrier 支持配置 Runnable 任务 CountDownLatch 不支持；CyclicBarrier 可重用 CountDownLatch 不可重用. ","date":"2019-06-24","objectID":"/java-aqs/:0:4","tags":["java","线程同步"],"title":"Java 线程同步实现探究","uri":"/java-aqs/"},{"categories":null,"content":"在 Java 开发中，多线程的应用十分频繁，所以在多线程协作完成同一个任务的情况下，线程间同步的需求会非常常见。比如这样一个场景，主线程需要从10个站点下载数据，此时新建一个大小为 5 的线程池来分别从站点下载数据，主线程则必须等到子线程全部下载完成后，拿到完整数据才能进行下一步的处理。Java 给我们提供了一个类 CountDownLatch 可以方便的帮助我们来实现这样的场景。 ","date":"2019-06-24","objectID":"/java-aqs/:0:0","tags":["java","线程同步"],"title":"Java 线程同步实现探究","uri":"/java-aqs/"},{"categories":null,"content":"代码示例 public class CountdownLatchDe { private static CountDownLatch guests = new CountDownLatch(5); public static void main(String[] args) throws InterruptedException { ExecutorService executorService = Executors.newFixedThreadPool(5); for (int i = 0; i \u003c 5; i++) { executorService.execute(() -\u003e { System.out.println(Thread.currentThread().getName() + \": 游戏开始\"); System.out.println(Thread.currentThread().getName() + \": 完成\"); guests.countDown(); }); } guests.await(); System.out.println(Thread.currentThread().getName() + \"： 全部结束, 开始一下项\"); executorService.shutdown(); } } 输出如下: 上面这个 demo 展示了这样一个场景，在一场有主持人的活动中，主持人必须等参与者都完成了一个游戏，才能宣布进入下一个环节。主线程模拟的是主持人，主线程调用方法await 会被阻塞住，子线程模拟的参与者每次执行方法countDown, 则CountDownLatch 维护的计数器会减一，直到全部子线程执行完，计数器也变为0，主线程才被唤醒可以继续往下执行。 ","date":"2019-06-24","objectID":"/java-aqs/:0:1","tags":["java","线程同步"],"title":"Java 线程同步实现探究","uri":"/java-aqs/"},{"categories":null,"content":"源码查看 属性和构造函数 public class CountDownLatch { // 这是 CountDownLatch 定义的一个继承自 AQS 的内部类，数据结构就是使用 AQS 提供的两个队列 sync queue 和 condition queue private final Sync sync; // 构造给定计数的 CountDownLatch, 完成 sync 初识化 public CountDownLatch(int count) { if (count \u003c 0) throw new IllegalArgumentException(\"count \u003c 0\"); // 初始化状态数 this.sync = new Sync(count); } } 核心函数 // 会阻塞调用线程一直到计数器为0或者被中断 public void await() throws InterruptedException { // 真正调用 sync 对象的方法 sync.acquireSharedInterruptibly(1); } await 主要调用链如下所示 总结一下核心流程: 判读当前计数器是否为 0； 计数器不是0， 调用 doAcquireSharedInterruptibly 加入到同步阻塞队列； 尝试获取锁，获取失败则调用 shouldParkAfterFailedAcquire 判断是否需要阻塞等待，如果需要阻塞等待则调用 parkAndCheckInterrupt 阻塞当前线程并让出cup资源直到被前一个节点唤醒 // 计数器减一，如果减为0，则释放所有等待线程 public void countDown() { sync.releaseShared(1); } countDown 主要调用链如下所示 总结一下核心流程(doReleaseShared)： 获取头结点，头结点不为空且有下一个结点； 头结点状态为 Signal, 调用 unparkSuccessor 唤醒； 获取当前节点状态，当前节点正常情况则设置成0，获取下一个状态为非 CANCELLED的节点，调用 LockSupport.unpark 唤醒此结点 考虑另外一个场景，子线程需要同时执行一个操作，怎么让这些线程同时开始呢？ 我们可以借助 Java 提供的 CyclicBarrier 来实现此目的。 ","date":"2019-06-24","objectID":"/java-aqs/:0:2","tags":["java","线程同步"],"title":"Java 线程同步实现探究","uri":"/java-aqs/"},{"categories":null,"content":"代码实例 public class CyclicBarrierDe { private static CyclicBarrier barrier = new CyclicBarrier(5, () -\u003e { System.out.println(\"下一阶段 go -\u003e\");}); public static void main(String[] args) { ExecutorService executorService = Executors.newFixedThreadPool(5); for (int i = 0; i \u003c 5; i++) { executorService.execute(() -\u003e { try { System.out.println(Thread.currentThread().getName() + \": 热身活动\"); barrier.await(); System.out.println(Thread.currentThread().getName() + \": 比赛开始\"); System.out.println(Thread.currentThread().getName() + \": 比赛结束\"); barrier.await(); System.out.println(Thread.currentThread().getName() + \": 参加记者招待会\"); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } }); } } } 输出如下： 上面的 demo 展示了每个子线程都调用了 await 则都到达了 barrier, 才能执行后续的代码，否则被阻塞住。 ","date":"2019-06-24","objectID":"/java-aqs/:0:3","tags":["java","线程同步"],"title":"Java 线程同步实现探究","uri":"/java-aqs/"},{"categories":null,"content":"源码查看 属性和构造函数 public class CyclicBarrier { /** The lock for guarding barrier entry */ // 可重入锁 private final ReentrantLock lock = new ReentrantLock(); /** Condition to wait on until tripped */ // 条件队列 private final Condition trip = lock.newCondition(); /** The number of parties */ // 参与的线程数量 private final int parties; /* The command to run when tripped */ // 由最后一个进入 barrier 的线程执行的操作 private final Runnable barrierCommand; /** The current generation */ // 当前代 private Generation generation = new Generation(); // 正在等待进入屏障的线程数量 private int count; public CyclicBarrier(int parties, Runnable barrierAction) { // 参与的线程数量小于等于0，抛出异常 if (parties \u003c= 0) throw new IllegalArgumentException(); // 设置parties this.parties = parties; // 设置count this.count = parties; // 设置barrierCommand this.barrierCommand = barrierAction; } public CyclicBarrier(int parties) { // 调用含有两个参数的构造函数 this(parties, null); } } 核心函数 private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { // 保存当前锁 final ReentrantLock lock = this.lock; // 锁定 lock.lock(); try { // 保存当前代 final Generation g = generation; if (g.broken) // 屏障被破坏，抛出异常 throw new BrokenBarrierException(); if (Thread.interrupted()) { // 线程被中断 // 损坏当前屏障，并且唤醒所有的线程，只有拥有锁的时候才会调用 breakBarrier(); // 抛出异常 throw new InterruptedException(); } // 减少正在等待进入屏障的线程数量 int index = --count; if (index == 0) { // 正在等待进入屏障的线程数量为0，所有线程都已经进入 // 运行的动作标识 boolean ranAction = false; try { // 保存运行动作 final Runnable command = barrierCommand; if (command != null) // 动作不为空 // 运行 command.run(); // 设置ranAction状态 ranAction = true; // 进入下一代 nextGeneration(); return 0; } finally { if (!ranAction) // 没有运行的动作 // 损坏当前屏障 breakBarrier(); } } // loop until tripped, broken, interrupted, or timed out // 无限循环 for (;;) { try { if (!timed) // 没有设置等待时间 // 等待 trip.await(); else if (nanos \u003e 0L) // 设置了等待时间，并且等待时间大于0 // 等待指定时长 nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation \u0026\u0026 ! g.broken) { // 等于当前代并且屏障没有被损坏 // 损坏当前屏障 breakBarrier(); // 抛出异常 throw ie; } else { // 不等于当前带后者是屏障被损坏 // We're about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // \"belong\" to subsequent execution. // 中断当前线程 Thread.currentThread().interrupt(); } } if (g.broken) // 屏障被损坏，抛出异常 throw new BrokenBarrierException(); if (g != generation) // 不等于当前代 // 返回索引 return index; if (timed \u0026\u0026 nanos \u003c= 0L) { // 设置了等待时间，并且等待时间小于0 // 损坏屏障 breakBarrier(); // 抛出异常 throw new TimeoutException(); } } } finally { // 释放锁 lock.unlock(); } } CyclicBarrier 与 CountDownLatch 对比: 二者都可以用来做线程同步； CyclicBarrier 到达 barrier 后唤醒全部线程，CountDownLatch 计数为0，则是一个个传播唤醒； CyclicBarrier 支持配置 Runnable 任务 CountDownLatch 不支持；CyclicBarrier 可重用 CountDownLatch 不可重用. ","date":"2019-06-24","objectID":"/java-aqs/:0:4","tags":["java","线程同步"],"title":"Java 线程同步实现探究","uri":"/java-aqs/"},{"categories":null,"content":"我们知道在默认情况下，pod 的调度会由 scheduler-crontroller 根据节点资源和优先级来自动完成。但是有一些场景，我们希望把特定的 pod 调度到特定的节点，就不能让 scheduler 自动完成。这个时候我们可以利用 k8s 的 NodeSelector 或者 NodeAffinity来实现。 有这样一个场景，在我们的集群里部署了 Prometheus, 但是随着集群规模的扩大 Prometheus 需要抓取的指标数量十分巨大。这样部署 Prometheus 的实例 I/O 压力会很大，这样会导致实例本身变得不够稳定。这样一来跟 Prometheus 部署在同一个实例上的服务就容易受到影响，导致不可用。所以，我们应该把 Prometheus 这样占用资源很多的服务与其他服务隔离开来。解决办法也是很自然的想法，就是把 Promethues 这样的服务固定到某些实例上，且其他服务不会漂移到这样的实例上。 ","date":"2019-05-24","objectID":"/k8s-affinity/:0:0","tags":["k8s","NodeAffinity","service"],"title":"利用 K8s NodeAffinity 隔离服务","uri":"/k8s-affinity/"},{"categories":null,"content":"NodeSelector 使用 NodeSelector 是最简单的方式。原理就是我们给 node 打上标签，NodeSelector 是 pod spec 的一个字段，只要给它赋值 node 对应的标签。scheduler 就只会把 pod 调度到具有该标签的 node 上。 节点打标签 # 列出节点 kubectl get no # 打标签 kubectl label no node3 test_label=yes # 查看标签 kubectl get no node3 --show-labels # 删除标签 kubectl label no node3 test_label- 配置添加 NodeSelector apiVersion: v1 kind: Pod metadata: name: hello-service labels: env: test spec: containers: - name: hello-sevice image: xxxx/hello-service:latest imagePullPolicy: IfNotPresent nodeSelector: #Add this field test_label: yes ","date":"2019-05-24","objectID":"/k8s-affinity/:0:1","tags":["k8s","NodeAffinity","service"],"title":"利用 K8s NodeAffinity 隔离服务","uri":"/k8s-affinity/"},{"categories":null,"content":"NodeAffinity NodeAffinity 应用上和 NodeSelector 相似。与后者相比优点是: 语法支持的操作符丰富 （in, NotIn, Exists, DoesNotExit, Gt, Lt） 规则可以更灵活，可以不是硬性要求，简单说就是即使不满足条件也能调度成功 具体 NodeAffinity 支持的策略包括： requiredDuringSchedulingIgnoredDuringExecution：必须满足指定的规则才可以调度Pod到Node上（功能与nodeSelector很像，语法不同），相当于硬限制。 preferredDuringSchedulingIgnoredDuringExecution：强调优先级，可以设置权重，但不是强制的，相当于软限制。 示例配置 apiVersion: v1 kind: Deployment metadata: name: dep-affinity spec: replicas: 1 template: metadata: labels: app: test-app spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - amd64 preferredDuringSchedulingIgnoredDuringExecution: # 软限制 - weight: 1 preference: matchExpressions: - key: test_label operator: In values: - yes containers: - name: dep-affinity image: xxxx/hello-service:latest 需要注意的地方 如果同时定义了 nodeSelector 和 nodeAffinity，那么必须两个条件都得到满足，pod 才能最终运行在指定的 node 上。 如果 nodeAffinity 指定了多个 nodeSelectorTerms，那么只需要其中一个能够匹配成功即可。 如果 nodeSelectorTerms 中有多个 matchExpressions,则一个节点必须满足所有 matchExpressions 才能运行该 pod。 如果一个 pod 所在的节点在Pod运行期间标签发生了变化，不再符合该 pod 的亲和性需求，pod 不会进行重新调度，继续在该节点上运行。 ","date":"2019-05-24","objectID":"/k8s-affinity/:1:0","tags":["k8s","NodeAffinity","service"],"title":"利用 K8s NodeAffinity 隔离服务","uri":"/k8s-affinity/"},{"categories":null,"content":"podAffinity and podAntiAffinity 根据正在节点上运行的 pod 的标签来对 pod 进行调度而不是使用 node 的标签。简单说就是根据已存在 pod 来决定要不要和它部署在同一 node 上。 生效策略与 NodeAffinity 一样有硬限制（requiredDuringSchedulingIgnoredDuringExecution）和软限制（preferredDuringSchedulingIgnoredDuringExecution） 示例配置如下 apiVersion: v1 kind: Pod metadata: name: pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: test_pod operator: In values: - pod1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: kubernetes.io/hostname containers: - name: pod-affinity image: xxxxx/hello-service:latest 上面的 podAffinity 表示被调度 pod 只能运行在集群中的特定节点上，这些节点已经至少运行一个具有\"test_pod=pod1\"标签的 pod，并且和这些节点在同一个 zone; podAntiAffinity 表示如果 node 上已经有 “app=nignx\"标签的 pod, 则优先不调度到该节点。 需要注意的地方 为了性能考虑只允许一些有限的topologyKey，默认情况下，有以下几个： kubernetes.io/hostname failure-domain.beta.kubernetes.io/zone failure-domain.beta.kubernetes.io/region 除了设置 LabelSelector 和 topologyKey,还可以指定 namespace 列表来进行限制，同样，使用 LabelSelector 对 namespace 进行选择。namespace 的定义和LabelSelector及 topologyKey 同级。 省略namespace的设置，表示使用定义了 affinity/anti-affinity 的 pod 所在的 namespace。 如果namespace设置为空值（\"\"），则表示所有namespace。 在所有关联requiredDuringSchedulingIgnoredDuringExecution的matchExpressions 全部满足之后，系统才能将Pod调度到某个Node上。 ","date":"2019-05-24","objectID":"/k8s-affinity/:2:0","tags":["k8s","NodeAffinity","service"],"title":"利用 K8s NodeAffinity 隔离服务","uri":"/k8s-affinity/"},{"categories":null,"content":"我们知道在默认情况下，pod 的调度会由 scheduler-crontroller 根据节点资源和优先级来自动完成。但是有一些场景，我们希望把特定的 pod 调度到特定的节点，就不能让 scheduler 自动完成。这个时候我们可以利用 k8s 的 NodeSelector 或者 NodeAffinity来实现。 有这样一个场景，在我们的集群里部署了 Prometheus, 但是随着集群规模的扩大 Prometheus 需要抓取的指标数量十分巨大。这样部署 Prometheus 的实例 I/O 压力会很大，这样会导致实例本身变得不够稳定。这样一来跟 Prometheus 部署在同一个实例上的服务就容易受到影响，导致不可用。所以，我们应该把 Prometheus 这样占用资源很多的服务与其他服务隔离开来。解决办法也是很自然的想法，就是把 Promethues 这样的服务固定到某些实例上，且其他服务不会漂移到这样的实例上。 ","date":"2019-05-24","objectID":"/k8s-affinity/:0:0","tags":["k8s","NodeAffinity","service"],"title":"利用 K8s NodeAffinity 隔离服务","uri":"/k8s-affinity/"},{"categories":null,"content":"NodeSelector 使用 NodeSelector 是最简单的方式。原理就是我们给 node 打上标签，NodeSelector 是 pod spec 的一个字段，只要给它赋值 node 对应的标签。scheduler 就只会把 pod 调度到具有该标签的 node 上。 节点打标签 # 列出节点 kubectl get no # 打标签 kubectl label no node3 test_label=yes # 查看标签 kubectl get no node3 --show-labels # 删除标签 kubectl label no node3 test_label- 配置添加 NodeSelector apiVersion: v1 kind: Pod metadata: name: hello-service labels: env: test spec: containers: - name: hello-sevice image: xxxx/hello-service:latest imagePullPolicy: IfNotPresent nodeSelector: #Add this field test_label: yes ","date":"2019-05-24","objectID":"/k8s-affinity/:0:1","tags":["k8s","NodeAffinity","service"],"title":"利用 K8s NodeAffinity 隔离服务","uri":"/k8s-affinity/"},{"categories":null,"content":"NodeAffinity NodeAffinity 应用上和 NodeSelector 相似。与后者相比优点是: 语法支持的操作符丰富 （in, NotIn, Exists, DoesNotExit, Gt, Lt） 规则可以更灵活，可以不是硬性要求，简单说就是即使不满足条件也能调度成功 具体 NodeAffinity 支持的策略包括： requiredDuringSchedulingIgnoredDuringExecution：必须满足指定的规则才可以调度Pod到Node上（功能与nodeSelector很像，语法不同），相当于硬限制。 preferredDuringSchedulingIgnoredDuringExecution：强调优先级，可以设置权重，但不是强制的，相当于软限制。 示例配置 apiVersion: v1 kind: Deployment metadata: name: dep-affinity spec: replicas: 1 template: metadata: labels: app: test-app spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - amd64 preferredDuringSchedulingIgnoredDuringExecution: # 软限制 - weight: 1 preference: matchExpressions: - key: test_label operator: In values: - yes containers: - name: dep-affinity image: xxxx/hello-service:latest 需要注意的地方 如果同时定义了 nodeSelector 和 nodeAffinity，那么必须两个条件都得到满足，pod 才能最终运行在指定的 node 上。 如果 nodeAffinity 指定了多个 nodeSelectorTerms，那么只需要其中一个能够匹配成功即可。 如果 nodeSelectorTerms 中有多个 matchExpressions,则一个节点必须满足所有 matchExpressions 才能运行该 pod。 如果一个 pod 所在的节点在Pod运行期间标签发生了变化，不再符合该 pod 的亲和性需求，pod 不会进行重新调度，继续在该节点上运行。 ","date":"2019-05-24","objectID":"/k8s-affinity/:1:0","tags":["k8s","NodeAffinity","service"],"title":"利用 K8s NodeAffinity 隔离服务","uri":"/k8s-affinity/"},{"categories":null,"content":"podAffinity and podAntiAffinity 根据正在节点上运行的 pod 的标签来对 pod 进行调度而不是使用 node 的标签。简单说就是根据已存在 pod 来决定要不要和它部署在同一 node 上。 生效策略与 NodeAffinity 一样有硬限制（requiredDuringSchedulingIgnoredDuringExecution）和软限制（preferredDuringSchedulingIgnoredDuringExecution） 示例配置如下 apiVersion: v1 kind: Pod metadata: name: pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: test_pod operator: In values: - pod1 topologyKey: failure-domain.beta.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: kubernetes.io/hostname containers: - name: pod-affinity image: xxxxx/hello-service:latest 上面的 podAffinity 表示被调度 pod 只能运行在集群中的特定节点上，这些节点已经至少运行一个具有\"test_pod=pod1\"标签的 pod，并且和这些节点在同一个 zone; podAntiAffinity 表示如果 node 上已经有 “app=nignx\"标签的 pod, 则优先不调度到该节点。 需要注意的地方 为了性能考虑只允许一些有限的topologyKey，默认情况下，有以下几个： kubernetes.io/hostname failure-domain.beta.kubernetes.io/zone failure-domain.beta.kubernetes.io/region 除了设置 LabelSelector 和 topologyKey,还可以指定 namespace 列表来进行限制，同样，使用 LabelSelector 对 namespace 进行选择。namespace 的定义和LabelSelector及 topologyKey 同级。 省略namespace的设置，表示使用定义了 affinity/anti-affinity 的 pod 所在的 namespace。 如果namespace设置为空值（\"\"），则表示所有namespace。 在所有关联requiredDuringSchedulingIgnoredDuringExecution的matchExpressions 全部满足之后，系统才能将Pod调度到某个Node上。 ","date":"2019-05-24","objectID":"/k8s-affinity/:2:0","tags":["k8s","NodeAffinity","service"],"title":"利用 K8s NodeAffinity 隔离服务","uri":"/k8s-affinity/"},{"categories":null,"content":"在之前的一些文章中我提到过我们的单点登录系统，作为我们内部系统的第一道门起着鉴权和验权的作用，总之是内部系统强有力的安全保障。 这里我们就以 CAS 协议为起点介绍一下单点登录系统的开发和搭建。 在当前的各个企业或公司都会有众多的内部系统，而且这些系统都需要身份验证成功才能访问，但是如果每个系统各自独立维护一套身份验证系统成本高不说对用户来说也是一个巨大的负担。那么能不能有一个中心的身份认证系统，只需要通过这个身份验证系统登录了众多系统中的一个，其他系统也能直接登录，自然从一个系统登出了，其他系统也就访问不了。这就是 SSO(单点登录)需要解决的问题。 CAS(Central Authentication Service) 是 Yale 大学发起的一个企业级的、开源的项目，旨在为 Web 应用系统提供一种可靠的单点登录解决方法。 ","date":"2019-04-19","objectID":"/cas/:0:0","tags":["cas","sso"],"title":"基于 CAS 实现 SSO","uri":"/cas/"},{"categories":null,"content":"CAS 结构 CAS 有服务端和客户端组成 服务端：负责用户的身份验证工作，存储用户的安全凭证，作为一个身份验证的中心自然是需要独立部署的； 客户端：其实就是我们的各个应用，当用户访问到应用，需要对请求方进行身份认证时，重定向到 CAS Server 进行认证。 ","date":"2019-04-19","objectID":"/cas/:0:1","tags":["cas","sso"],"title":"基于 CAS 实现 SSO","uri":"/cas/"},{"categories":null,"content":"重要概念 Service Ticket(ST): cas server 生成的一次性票据，用于验证服务的有效性，无论验证是否成功 ST 在使用后都已失效 Ticket-granting Cookie(TGC): 存放用户身份认证凭证的 cookie ，在浏览器和 CAS Server 间通讯时使用，并且只能基于安全通道传输(https)，是 CAS Server 用来明确用户身份的凭证 ","date":"2019-04-19","objectID":"/cas/:0:2","tags":["cas","sso"],"title":"基于 CAS 实现 SSO","uri":"/cas/"},{"categories":null,"content":"访问流程 我们结合 CAS 的访问时序图来介绍一下 CAS 的原理。 访问目标应用一； 需要身份验证，目标应用把用户重定向到 cas server 登录页面并且带上目标服务的访问地址作为请求参数； cas server 检查用户没有 session, 则展示用户身份凭证输入框如用户名和密码； 用户输入身份凭证，提交 post 请求到 cas server; cas server 验证用户身份成功，cas 创建 sso session，返回 cookie 和 ST 给客户端； 用户带着 ST 访问目标应用; 目标应用携带 ST 访问 cas server 验证 ST 的合法性； cas server 验证 ST 成功后，返回给目标应用 xml document 来说明用户身份校验成功； 目标应用建立自己的 session， 并把 cookie 返回给客户端； 客户端携带目标应用返回的 cookie 直接访问； 目标应用验证 cookie 成功，返回对应的请求内容给客户端 至此，用户第弟一次访问成功。 如果用户第二次访问目标应用，流程则如下所示 访问目标应用一 客户端因为已经存储了目标应用第一次返回的 cookie, 则第二次访问目标应用验证成功后直接返回响应内容。 现在，用户访问另外一个应用，则流程如下 访问目标应用二 需要身份验证，目标应用把用户重定向到 cas server 并且带上目标服务的访问地址作为请求参数； cas server 发现重定向过来的请求已经携带有 TGC 验证成功后，返回 ST 给客户端； 下面的步骤与用户访问应用一一致不在重复解释。 至此，用户不需要二次登陆就能成功访问应用二。 ","date":"2019-04-19","objectID":"/cas/:0:3","tags":["cas","sso"],"title":"基于 CAS 实现 SSO","uri":"/cas/"},{"categories":null,"content":"集成 我们简单把应用归一下类，自己开发的系统和利用第三方软件搭建的系统，如 Jenkins, Jira 这类。 首先自己开发的系统只要按照 CAS 协议调用 cas server 的接口就能完成集成； 例如 Jenkins, Jira 这类系统本身具有 CAS 插件，也能比较方便的利用插件完成 CAS 的集成 另外一类系统不是由自己开发，也不方便内部改造的，我们可以不完全按照 CAS 协议，在系统外部加上 cas server 的身份验证 这里我们举一个例子，例如我们使用 Kibana 来查看 es 中的日志。Kibana 使用 Nginx 做反向代理，那么我们可以利用 Nginx 的 auth_request 模块集成 cas server, 来达到身份验证的目的。 示例配置如下 server { server_name kibana.example.com; location / { auth_request /auth; proxy_set_header Host $host; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://kibana_server; client_max_body_size 200m; } location = /auth { internal; proxy_pass https://cas_server; proxy_pass_request_body off; proxy_set_header Content-Length \"\"; proxy_set_header X-Original-URI $request_uri; } error_page 401 https://cas_server/login/?service=https://$host$request_uri; error_page 403 https://cas_server/no_permission; } ","date":"2019-04-19","objectID":"/cas/:0:4","tags":["cas","sso"],"title":"基于 CAS 实现 SSO","uri":"/cas/"},{"categories":null,"content":"在之前的一些文章中我提到过我们的单点登录系统，作为我们内部系统的第一道门起着鉴权和验权的作用，总之是内部系统强有力的安全保障。 这里我们就以 CAS 协议为起点介绍一下单点登录系统的开发和搭建。 在当前的各个企业或公司都会有众多的内部系统，而且这些系统都需要身份验证成功才能访问，但是如果每个系统各自独立维护一套身份验证系统成本高不说对用户来说也是一个巨大的负担。那么能不能有一个中心的身份认证系统，只需要通过这个身份验证系统登录了众多系统中的一个，其他系统也能直接登录，自然从一个系统登出了，其他系统也就访问不了。这就是 SSO(单点登录)需要解决的问题。 CAS(Central Authentication Service) 是 Yale 大学发起的一个企业级的、开源的项目，旨在为 Web 应用系统提供一种可靠的单点登录解决方法。 ","date":"2019-04-19","objectID":"/cas/:0:0","tags":["cas","sso"],"title":"基于 CAS 实现 SSO","uri":"/cas/"},{"categories":null,"content":"CAS 结构 CAS 有服务端和客户端组成 服务端：负责用户的身份验证工作，存储用户的安全凭证，作为一个身份验证的中心自然是需要独立部署的； 客户端：其实就是我们的各个应用，当用户访问到应用，需要对请求方进行身份认证时，重定向到 CAS Server 进行认证。 ","date":"2019-04-19","objectID":"/cas/:0:1","tags":["cas","sso"],"title":"基于 CAS 实现 SSO","uri":"/cas/"},{"categories":null,"content":"重要概念 Service Ticket(ST): cas server 生成的一次性票据，用于验证服务的有效性，无论验证是否成功 ST 在使用后都已失效 Ticket-granting Cookie(TGC): 存放用户身份认证凭证的 cookie ，在浏览器和 CAS Server 间通讯时使用，并且只能基于安全通道传输(https)，是 CAS Server 用来明确用户身份的凭证 ","date":"2019-04-19","objectID":"/cas/:0:2","tags":["cas","sso"],"title":"基于 CAS 实现 SSO","uri":"/cas/"},{"categories":null,"content":"访问流程 我们结合 CAS 的访问时序图来介绍一下 CAS 的原理。 访问目标应用一； 需要身份验证，目标应用把用户重定向到 cas server 登录页面并且带上目标服务的访问地址作为请求参数； cas server 检查用户没有 session, 则展示用户身份凭证输入框如用户名和密码； 用户输入身份凭证，提交 post 请求到 cas server; cas server 验证用户身份成功，cas 创建 sso session，返回 cookie 和 ST 给客户端； 用户带着 ST 访问目标应用; 目标应用携带 ST 访问 cas server 验证 ST 的合法性； cas server 验证 ST 成功后，返回给目标应用 xml document 来说明用户身份校验成功； 目标应用建立自己的 session， 并把 cookie 返回给客户端； 客户端携带目标应用返回的 cookie 直接访问； 目标应用验证 cookie 成功，返回对应的请求内容给客户端 至此，用户第弟一次访问成功。 如果用户第二次访问目标应用，流程则如下所示 访问目标应用一 客户端因为已经存储了目标应用第一次返回的 cookie, 则第二次访问目标应用验证成功后直接返回响应内容。 现在，用户访问另外一个应用，则流程如下 访问目标应用二 需要身份验证，目标应用把用户重定向到 cas server 并且带上目标服务的访问地址作为请求参数； cas server 发现重定向过来的请求已经携带有 TGC 验证成功后，返回 ST 给客户端； 下面的步骤与用户访问应用一一致不在重复解释。 至此，用户不需要二次登陆就能成功访问应用二。 ","date":"2019-04-19","objectID":"/cas/:0:3","tags":["cas","sso"],"title":"基于 CAS 实现 SSO","uri":"/cas/"},{"categories":null,"content":"集成 我们简单把应用归一下类，自己开发的系统和利用第三方软件搭建的系统，如 Jenkins, Jira 这类。 首先自己开发的系统只要按照 CAS 协议调用 cas server 的接口就能完成集成； 例如 Jenkins, Jira 这类系统本身具有 CAS 插件，也能比较方便的利用插件完成 CAS 的集成 另外一类系统不是由自己开发，也不方便内部改造的，我们可以不完全按照 CAS 协议，在系统外部加上 cas server 的身份验证 这里我们举一个例子，例如我们使用 Kibana 来查看 es 中的日志。Kibana 使用 Nginx 做反向代理，那么我们可以利用 Nginx 的 auth_request 模块集成 cas server, 来达到身份验证的目的。 示例配置如下 server { server_name kibana.example.com; location / { auth_request /auth; proxy_set_header Host $host; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://kibana_server; client_max_body_size 200m; } location = /auth { internal; proxy_pass https://cas_server; proxy_pass_request_body off; proxy_set_header Content-Length \"\"; proxy_set_header X-Original-URI $request_uri; } error_page 401 https://cas_server/login/?service=https://$host$request_uri; error_page 403 https://cas_server/no_permission; } ","date":"2019-04-19","objectID":"/cas/:0:4","tags":["cas","sso"],"title":"基于 CAS 实现 SSO","uri":"/cas/"},{"categories":null,"content":"消息中间件是后端服务系统中一个非常重要的组件，它可以在分布式环境下提供应用解耦、流量削峰、异步通信、数据同步等关键功能。在我们的应用中首先 rabbimq 和 kafka 这两个消息队列产品。 这二者虽然我们都叫消息队列，但他们的实现和应用场景都是不一样的。我们根据不同的应用场景会选择不同的产品。 一般情况下 实时性要求极高，单条消息不大的，routing 复杂可选 RabbitMQ; 吞吐量要求高，实时性要求可稍微降低，流式数据可选 Kafka 现在我们可以有更好的选择 Pulsar，一个分布式高性能消息系统。 Pulsar 的优势在哪里？我们为什么要选它? ","date":"2019-03-25","objectID":"/pulsar/:0:0","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"系统架构 Pulsar 和其他消息系统最根本的不同是采用分层架构。 Apache Pulsar 集群由两层组成：无状态服务层，由一组接收和传递消息的 Broker 组成；以及一个有状态持久层，由一组名为 bookies 的 Apache BookKeeper 存储节点组成，可持久化地存储消息。下图显示了 Pulsar 的主体架构。 Pulsar 非常核心的设计逻辑就是消息服务和消息存储的分离, 即Broker 消息服务层和 BookKeeper 消息存储层。 ","date":"2019-03-25","objectID":"/pulsar/:1:0","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"Broker Broker 不存储消息本身，每个 topic 的消息都存储到了分布式日志存储系统 BookKeeper 中。 每个 partitioned topic 会被分配给某个 broker, 而生产者和消费者都连接到这个 broker 分别发送和消息消息。 如果一个 Broker 失败，Pulsar 会自动将其拥有的主题分区移动到群集中剩余的某一个可用 Broker 中。由于 Broker 是无状态的，当发生 Topic 的迁移时，Pulsar 只是将所有权从一个 Broker 转移到另一个 Broker，在这个过程中，不会有任何数据复制发生。Broker 层的示意图如下所示 ","date":"2019-03-25","objectID":"/pulsar/:1:1","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"BookKeeper BookKeeper 作为 puslar 的持久化层，负责存储每个 topic 的 partitions，本质上是分布式日志。每个分布式日志又被分为 Segment 分段。 每个 Segment 分段作为 BookKeeper 中的一个 Ledger，均匀分布并存储在 BookKeeper 群集中的多个 Bookie（BookKeeper 的存储节点）中。 Segment 创建的条件是基于配置的 serment 大小；基于配置的滚动时间, 或者 segment 的所有者发生了变化。通过 Segment 分段的方式，topic partition 中的消息可以均匀和平衡地分布在群集中的所有 Bookie 中。 因此一个 topic partition 的容量不受一个节点容量的限制；相反，它可以扩展到整个 BookKeeper 集群的总容量。 下图展示了一个 topic partition 存储示意 正是因为分层架构和以 segment 为中心存储的设计思想让 Pular 相比其他消息队列产品有了以下优势 无限制的 topic partition 存储； topic patition 可以扩展到整个 BookKeeper 集群的总容量，只需添加 Bookie 节点即可扩展集群容量。 即时扩展，无需数据迁移； 无缝 Broker 故障恢复 无缝集群容量扩展 无缝 Bookie 故障恢复 独立的扩展性 ","date":"2019-03-25","objectID":"/pulsar/:1:2","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"消息模型 消息系统的模型可以分为两类 队列式(queuing)和流式(streaming)。 queuing 模型主要是采用无序或者共享的方式来消费消息，当一条消息从队列发送出来后，多个消费者中的只有一个（任何一个都有可能）接收和消费这条消息。queuing 模型通常与无状态应用程序一起结合使用, 无状态应用程序不关心排序，但需要能够确认（ack）或删除单条消息，以及尽可能地扩展消费并行的能力。RabbitMQ 就是典型的 queuing 模型。 streaming 模型要求消息的消费严格排序或独占消息消费，对于一个 message channel 只会有一个消费者消费消息。流模型通常与有状态应用程序相关联。有状态的应用程序更加关注消息的顺序及其状态。消息的消费顺序决定了有状态应用程序的状态。消息的顺序将影响应用程序处理逻辑的正确性。kafka 就是 streaming 模型。 Pulsar 的消息模型既支持 queuing，也支持 streaming。 在 Pulsar 的消息消费模型中，Topic 是用于发送消息的通道, 消费者被组合在一起消费消息，每个消费组是一个订阅。每组消费者可以拥有自己不同的消费方式： 独占（Exclusive），故障切换（Failover）或共享（Share）。Pulsar 通过这种模型，将队列模型和流模型这两种模型结合在了一起，提供了统一的 API 接口。 这种模型，既不会影响消息系统的性能，也不会带来额外的开销，同时还为用户提供了更多灵活性，方便用户程序以最匹配模式来使用消息系统。 Exclusive 和 Failover 订阅，仅允许一个消费者来使用和消费每个对主题的订阅。这两种模式都按主题分区顺序使用消息。它们最适用于需要严格消息顺序的流（Stream）用例。 Share 订阅允许每个主题分区有多个消费者。同一订阅中的每个消费者仅接收主题分区的一部分消息。共享订阅最适用于不需要保证消息顺序的队列（Queue）的使用模式，并且可以按照需要任意扩展消费者的数量。 下图展示了3个不同类型的订阅和消息的流向 ","date":"2019-03-25","objectID":"/pulsar/:2:0","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"对比 Kafka 消息模型 Pulsar 提供了统一的消息模型和 API。streaming 模式——独占和故障切换订阅方式; queuing 模式——共享订阅的方式。 Kafka 主要集中在 streaming 模式，对单个 partition 是独占消费，没有共享（Queue）的消费模式； 架构 Pulsar Broker是无状态的，与存储相互分离 可以轻松添加和删除节点，而无需重新平衡整个集群 Kafka的数据直接存储在Broker上(有状态的) 任何容量扩展都需要重新平衡分区，同时还需要将被平衡的分区重新拷贝到新添加的Broker上 Ack Pulsar使用专门的 Cursor 管理。累积确认(cumulative acknowledgment)和 Kafka 效果一样；提供单条确认(individual ack)。 Kafka使用偏移 Offset Retention Pulsar 消息只有被所有订阅消费后才会删除，不会丢失数据。也允许设置保留期，保留被消费的数据。支持 TTL Kafka根据设置的保留期来删除消息。有可能消息没被消费，过期后被删除。 不支持 TTL。 总结一下，文章开始问题的答案就是 Pulsar 将高性能的 streaming（Kafka）和灵活的 queuing（RabbitMQ）结合到一个统一的消息模型和 API 中。 Pulsar 使用统一的 API 为用户提供一个支持 streaming 和 queuing 的系统，且具有同样的高性能。 ","date":"2019-03-25","objectID":"/pulsar/:3:0","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"消息中间件是后端服务系统中一个非常重要的组件，它可以在分布式环境下提供应用解耦、流量削峰、异步通信、数据同步等关键功能。在我们的应用中首先 rabbimq 和 kafka 这两个消息队列产品。 这二者虽然我们都叫消息队列，但他们的实现和应用场景都是不一样的。我们根据不同的应用场景会选择不同的产品。 一般情况下 实时性要求极高，单条消息不大的，routing 复杂可选 RabbitMQ; 吞吐量要求高，实时性要求可稍微降低，流式数据可选 Kafka 现在我们可以有更好的选择 Pulsar，一个分布式高性能消息系统。 Pulsar 的优势在哪里？我们为什么要选它? ","date":"2019-03-25","objectID":"/pulsar/:0:0","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"系统架构 Pulsar 和其他消息系统最根本的不同是采用分层架构。 Apache Pulsar 集群由两层组成：无状态服务层，由一组接收和传递消息的 Broker 组成；以及一个有状态持久层，由一组名为 bookies 的 Apache BookKeeper 存储节点组成，可持久化地存储消息。下图显示了 Pulsar 的主体架构。 Pulsar 非常核心的设计逻辑就是消息服务和消息存储的分离, 即Broker 消息服务层和 BookKeeper 消息存储层。 ","date":"2019-03-25","objectID":"/pulsar/:1:0","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"Broker Broker 不存储消息本身，每个 topic 的消息都存储到了分布式日志存储系统 BookKeeper 中。 每个 partitioned topic 会被分配给某个 broker, 而生产者和消费者都连接到这个 broker 分别发送和消息消息。 如果一个 Broker 失败，Pulsar 会自动将其拥有的主题分区移动到群集中剩余的某一个可用 Broker 中。由于 Broker 是无状态的，当发生 Topic 的迁移时，Pulsar 只是将所有权从一个 Broker 转移到另一个 Broker，在这个过程中，不会有任何数据复制发生。Broker 层的示意图如下所示 ","date":"2019-03-25","objectID":"/pulsar/:1:1","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"BookKeeper BookKeeper 作为 puslar 的持久化层，负责存储每个 topic 的 partitions，本质上是分布式日志。每个分布式日志又被分为 Segment 分段。 每个 Segment 分段作为 BookKeeper 中的一个 Ledger，均匀分布并存储在 BookKeeper 群集中的多个 Bookie（BookKeeper 的存储节点）中。 Segment 创建的条件是基于配置的 serment 大小；基于配置的滚动时间, 或者 segment 的所有者发生了变化。通过 Segment 分段的方式，topic partition 中的消息可以均匀和平衡地分布在群集中的所有 Bookie 中。 因此一个 topic partition 的容量不受一个节点容量的限制；相反，它可以扩展到整个 BookKeeper 集群的总容量。 下图展示了一个 topic partition 存储示意 正是因为分层架构和以 segment 为中心存储的设计思想让 Pular 相比其他消息队列产品有了以下优势 无限制的 topic partition 存储； topic patition 可以扩展到整个 BookKeeper 集群的总容量，只需添加 Bookie 节点即可扩展集群容量。 即时扩展，无需数据迁移； 无缝 Broker 故障恢复 无缝集群容量扩展 无缝 Bookie 故障恢复 独立的扩展性 ","date":"2019-03-25","objectID":"/pulsar/:1:2","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"消息模型 消息系统的模型可以分为两类 队列式(queuing)和流式(streaming)。 queuing 模型主要是采用无序或者共享的方式来消费消息，当一条消息从队列发送出来后，多个消费者中的只有一个（任何一个都有可能）接收和消费这条消息。queuing 模型通常与无状态应用程序一起结合使用, 无状态应用程序不关心排序，但需要能够确认（ack）或删除单条消息，以及尽可能地扩展消费并行的能力。RabbitMQ 就是典型的 queuing 模型。 streaming 模型要求消息的消费严格排序或独占消息消费，对于一个 message channel 只会有一个消费者消费消息。流模型通常与有状态应用程序相关联。有状态的应用程序更加关注消息的顺序及其状态。消息的消费顺序决定了有状态应用程序的状态。消息的顺序将影响应用程序处理逻辑的正确性。kafka 就是 streaming 模型。 Pulsar 的消息模型既支持 queuing，也支持 streaming。 在 Pulsar 的消息消费模型中，Topic 是用于发送消息的通道, 消费者被组合在一起消费消息，每个消费组是一个订阅。每组消费者可以拥有自己不同的消费方式： 独占（Exclusive），故障切换（Failover）或共享（Share）。Pulsar 通过这种模型，将队列模型和流模型这两种模型结合在了一起，提供了统一的 API 接口。 这种模型，既不会影响消息系统的性能，也不会带来额外的开销，同时还为用户提供了更多灵活性，方便用户程序以最匹配模式来使用消息系统。 Exclusive 和 Failover 订阅，仅允许一个消费者来使用和消费每个对主题的订阅。这两种模式都按主题分区顺序使用消息。它们最适用于需要严格消息顺序的流（Stream）用例。 Share 订阅允许每个主题分区有多个消费者。同一订阅中的每个消费者仅接收主题分区的一部分消息。共享订阅最适用于不需要保证消息顺序的队列（Queue）的使用模式，并且可以按照需要任意扩展消费者的数量。 下图展示了3个不同类型的订阅和消息的流向 ","date":"2019-03-25","objectID":"/pulsar/:2:0","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"对比 Kafka 消息模型 Pulsar 提供了统一的消息模型和 API。streaming 模式——独占和故障切换订阅方式; queuing 模式——共享订阅的方式。 Kafka 主要集中在 streaming 模式，对单个 partition 是独占消费，没有共享（Queue）的消费模式； 架构 Pulsar Broker是无状态的，与存储相互分离 可以轻松添加和删除节点，而无需重新平衡整个集群 Kafka的数据直接存储在Broker上(有状态的) 任何容量扩展都需要重新平衡分区，同时还需要将被平衡的分区重新拷贝到新添加的Broker上 Ack Pulsar使用专门的 Cursor 管理。累积确认(cumulative acknowledgment)和 Kafka 效果一样；提供单条确认(individual ack)。 Kafka使用偏移 Offset Retention Pulsar 消息只有被所有订阅消费后才会删除，不会丢失数据。也允许设置保留期，保留被消费的数据。支持 TTL Kafka根据设置的保留期来删除消息。有可能消息没被消费，过期后被删除。 不支持 TTL。 总结一下，文章开始问题的答案就是 Pulsar 将高性能的 streaming（Kafka）和灵活的 queuing（RabbitMQ）结合到一个统一的消息模型和 API 中。 Pulsar 使用统一的 API 为用户提供一个支持 streaming 和 queuing 的系统，且具有同样的高性能。 ","date":"2019-03-25","objectID":"/pulsar/:3:0","tags":["消息队列","backend"],"title":"Pulsar(新一代高性能消息系统)核心总结","uri":"/pulsar/"},{"categories":null,"content":"Kubernetes 集群的安全性的重要程度不用强调，当我们与集群交互时，不管是通过 kubernetes-dashborad 还是通过命令行工具 kubectl 都需要进行身份验证和鉴权。那么我们是怎么利用 k8s 本身提供的能力来做身份验证和鉴权的呢？ Kubernetes 集群的访问权限控制由 kube-apiserver 负责，kube-apiserver 的访问权限控制由身份验证(authentication)、授权(authorization)和准入控制（admission control）三步骤组成，这三步骤是按序进行的。 authentication k8s 支持 tls client certificate、basic auth、token 等方式对客户端发起的请求进行身份校验。APIServer 启动时，可以指定一种 Authentication 方法，也可以指定多种方法。如果指定了多种方法，那么 APIServer 将会逐个使用这些方法对客户端请求进行验证，只要请求数据通过其中一种方法的验证，APIServer 就会认为 Authentication 成功。 authorization APIServer 支持多种 authorization mode，包括 Node、RBAC、Webhook 等。APIServer 启动时，可以指定一种 authorization mode，也可以指定多种 authorization mode，如果是后者，只要 Request 通过了其中一种 mode 的授权，那么该环节的最终结果就是授权成功。authorization-mode 的默认配置是 ”Node,RBAC”。Node 授权器主要用于各个 node 上的 kubelet 访问 apiserver 时使用的，其他一般均由 RBAC 授权器来授权。 这里我们着重介绍一下 RBAC RBAC(Role-Based Access Control)，它使用 “rbac.authorization.k8s.io” 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。在 RBAC API 中，一个角色(Role)包含了一组权限规则。 Role 有两种：Role 和 ClusterRole。一个Role对象只能用于授予对某一单一命名空间（namespace）中资源的访问权限。ClusterRole对象可以授予与Role对象相同的权限，但由于它们属于集群范围对象。 RoleBinding: 角色绑定则是定义了将一个角色的各种权限授予一个或者一组用户。 角色绑定包含了一组相关主体（即 subject, 包括用户——User、用户组——Group、或者服务账户——Service Account）以及对被授予角色的引用。 在命名空间中可以通过 RoleBinding 对象进行用户授权，而集群范围的用户授权则可以通过 ClusterRoleBinding对象完成。 在上面的基础上我们来创建具有特定权限的用户 我们创建一个用户 release 从名字可以看出他是干什么的，我们赋予这个用户可以在集群中发布服务的权限。 ","date":"2019-02-15","objectID":"/kubeconfig/:0:0","tags":["kubernetes","authentication","authorization"],"title":"kubectl 操作 Kubernetes 的权限控制","uri":"/kubeconfig/"},{"categories":null,"content":"创建具有对应权限的用户 创建 cluster role 示例如下 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-release rules: - apiGroups: [\"\", \"extensions\", \"apps\", \"batch\"] resources: [\"*\"] verbs: [\"update\", \"get\", \"watch\", \"list\", \"create\", \"patch\"] 绑定用户，创建 ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: cluster-release roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-release subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: release ","date":"2019-02-15","objectID":"/kubeconfig/:0:1","tags":["kubernetes","authentication","authorization"],"title":"kubectl 操作 Kubernetes 的权限控制","uri":"/kubeconfig/"},{"categories":null,"content":"创建 kubeconfig kubeconfig 其实就是集群的身份认证和鉴权的配置文件，它包含了集群信息，客户信息，上下文(context)等。并且根据不同的 context 可以实现多集群的访问。 这里我们用 tls client certificate 来作为身份验证的安全凭证，之前也说过也可以用 token 的方式。 1. 创建用户安全凭证(CA 证书) 给用户创建 private key(比如给用户release) openssl genrsa -out release.key 2048 说明 在申请数字证书之前，您必须先生成证书私钥和证书请求文件(CSR,Cerificate Signing Request),CSR是您的公钥证书原始文件，包含了您的服务器信息和您的单位信息，需要提交给CA认证中心。在生成CSR文件时会同时生成私钥文件，请妥善保管和备份您的私钥。 生成CSR文件时，一般需要输入以下信息(中文需要UTF8编码)： Organization Name(O)：申请单位名称法定名称，可以是中文或英文 Organization Unit(OU)：申请单位的所在部门，可以是中文或英文 Country Code(C)：申请单位所属国家，只能是两个字母的国家码，如中国只能是：CN State or Province(S)：申请单位所在省名或州名，可以是中文或英文 Locality(L)：申请单位所在城市名，可以是中文或英文 Common Name(CN)：申请SSL证书的具体网站域名 创建证书签名请求(csr) CN(这里是用户名) O(这里是组)要显示指明 openssl req -new -key release.key -out release.csr -subj \"/CN=release/O=k8s\" 给用户release签发证书 openssl x509 -req -in release.csr -CA /etc/kubernetes/ssl/ca.pem -CAkey /etc/kubernetes/ssl/ca-key.pem -CAcreateserial -out release.crt -days 365 至此我们应该生成了证书 release.crt 和 release.key。 2. 生成 kubeconfig 文件 先要指明 api-server 的地址 export KUBE_APISERVER=https://172.31.xx.xx:6443 设置k8s 集群信息 sudo kubectl config set-cluster cluster.local --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=release.kubeconfig 设置用户安全凭证 sudo kubectl config set-credentials release --client-certificate=release.crt --client-key=release.key --embed-certs=true --kubeconfig=release.kubeconfig 设置 context 参数 sudo kubectl config set-context release-context --cluster=cluster.local --user=release --kubeconfig=release.kubeconfig 设置默认 context sudo kubectl config use-context release-context --kubeconfig=release.kubeconfig 至此生成好了 kubeconfig 文件。 kubectl config view 可以查看生成好的内容。 ","date":"2019-02-15","objectID":"/kubeconfig/:0:2","tags":["kubernetes","authentication","authorization"],"title":"kubectl 操作 Kubernetes 的权限控制","uri":"/kubeconfig/"},{"categories":null,"content":"使用 kubeconfig 我们可以用生成好的 kubeconfig 文件来赋予 kubectl 操作集群的能力 示例如下 FROM ubuntu:16.04 COPY kubectl /usr/local/bin/ RUN chmod +x /usr/local/bin/kubectl RUN mkdir /root/.kube COPY config /root/.kube/ 下面这张图总结了我们的工作 ","date":"2019-02-15","objectID":"/kubeconfig/:0:3","tags":["kubernetes","authentication","authorization"],"title":"kubectl 操作 Kubernetes 的权限控制","uri":"/kubeconfig/"},{"categories":null,"content":"Kubernetes 集群的安全性的重要程度不用强调，当我们与集群交互时，不管是通过 kubernetes-dashborad 还是通过命令行工具 kubectl 都需要进行身份验证和鉴权。那么我们是怎么利用 k8s 本身提供的能力来做身份验证和鉴权的呢？ Kubernetes 集群的访问权限控制由 kube-apiserver 负责，kube-apiserver 的访问权限控制由身份验证(authentication)、授权(authorization)和准入控制（admission control）三步骤组成，这三步骤是按序进行的。 authentication k8s 支持 tls client certificate、basic auth、token 等方式对客户端发起的请求进行身份校验。APIServer 启动时，可以指定一种 Authentication 方法，也可以指定多种方法。如果指定了多种方法，那么 APIServer 将会逐个使用这些方法对客户端请求进行验证，只要请求数据通过其中一种方法的验证，APIServer 就会认为 Authentication 成功。 authorization APIServer 支持多种 authorization mode，包括 Node、RBAC、Webhook 等。APIServer 启动时，可以指定一种 authorization mode，也可以指定多种 authorization mode，如果是后者，只要 Request 通过了其中一种 mode 的授权，那么该环节的最终结果就是授权成功。authorization-mode 的默认配置是 ”Node,RBAC”。Node 授权器主要用于各个 node 上的 kubelet 访问 apiserver 时使用的，其他一般均由 RBAC 授权器来授权。 这里我们着重介绍一下 RBAC RBAC(Role-Based Access Control)，它使用 “rbac.authorization.k8s.io” 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。在 RBAC API 中，一个角色(Role)包含了一组权限规则。 Role 有两种：Role 和 ClusterRole。一个Role对象只能用于授予对某一单一命名空间（namespace）中资源的访问权限。ClusterRole对象可以授予与Role对象相同的权限，但由于它们属于集群范围对象。 RoleBinding: 角色绑定则是定义了将一个角色的各种权限授予一个或者一组用户。 角色绑定包含了一组相关主体（即 subject, 包括用户——User、用户组——Group、或者服务账户——Service Account）以及对被授予角色的引用。 在命名空间中可以通过 RoleBinding 对象进行用户授权，而集群范围的用户授权则可以通过 ClusterRoleBinding对象完成。 在上面的基础上我们来创建具有特定权限的用户 我们创建一个用户 release 从名字可以看出他是干什么的，我们赋予这个用户可以在集群中发布服务的权限。 ","date":"2019-02-15","objectID":"/kubeconfig/:0:0","tags":["kubernetes","authentication","authorization"],"title":"kubectl 操作 Kubernetes 的权限控制","uri":"/kubeconfig/"},{"categories":null,"content":"创建具有对应权限的用户 创建 cluster role 示例如下 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-release rules: - apiGroups: [\"\", \"extensions\", \"apps\", \"batch\"] resources: [\"*\"] verbs: [\"update\", \"get\", \"watch\", \"list\", \"create\", \"patch\"] 绑定用户，创建 ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: cluster-release roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-release subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: release ","date":"2019-02-15","objectID":"/kubeconfig/:0:1","tags":["kubernetes","authentication","authorization"],"title":"kubectl 操作 Kubernetes 的权限控制","uri":"/kubeconfig/"},{"categories":null,"content":"创建 kubeconfig kubeconfig 其实就是集群的身份认证和鉴权的配置文件，它包含了集群信息，客户信息，上下文(context)等。并且根据不同的 context 可以实现多集群的访问。 这里我们用 tls client certificate 来作为身份验证的安全凭证，之前也说过也可以用 token 的方式。 1. 创建用户安全凭证(CA 证书) 给用户创建 private key(比如给用户release) openssl genrsa -out release.key 2048 说明 在申请数字证书之前，您必须先生成证书私钥和证书请求文件(CSR,Cerificate Signing Request),CSR是您的公钥证书原始文件，包含了您的服务器信息和您的单位信息，需要提交给CA认证中心。在生成CSR文件时会同时生成私钥文件，请妥善保管和备份您的私钥。 生成CSR文件时，一般需要输入以下信息(中文需要UTF8编码)： Organization Name(O)：申请单位名称法定名称，可以是中文或英文 Organization Unit(OU)：申请单位的所在部门，可以是中文或英文 Country Code(C)：申请单位所属国家，只能是两个字母的国家码，如中国只能是：CN State or Province(S)：申请单位所在省名或州名，可以是中文或英文 Locality(L)：申请单位所在城市名，可以是中文或英文 Common Name(CN)：申请SSL证书的具体网站域名 创建证书签名请求(csr) CN(这里是用户名) O(这里是组)要显示指明 openssl req -new -key release.key -out release.csr -subj \"/CN=release/O=k8s\" 给用户release签发证书 openssl x509 -req -in release.csr -CA /etc/kubernetes/ssl/ca.pem -CAkey /etc/kubernetes/ssl/ca-key.pem -CAcreateserial -out release.crt -days 365 至此我们应该生成了证书 release.crt 和 release.key。 2. 生成 kubeconfig 文件 先要指明 api-server 的地址 export KUBE_APISERVER=https://172.31.xx.xx:6443 设置k8s 集群信息 sudo kubectl config set-cluster cluster.local --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true --server=${KUBE_APISERVER} --kubeconfig=release.kubeconfig 设置用户安全凭证 sudo kubectl config set-credentials release --client-certificate=release.crt --client-key=release.key --embed-certs=true --kubeconfig=release.kubeconfig 设置 context 参数 sudo kubectl config set-context release-context --cluster=cluster.local --user=release --kubeconfig=release.kubeconfig 设置默认 context sudo kubectl config use-context release-context --kubeconfig=release.kubeconfig 至此生成好了 kubeconfig 文件。 kubectl config view 可以查看生成好的内容。 ","date":"2019-02-15","objectID":"/kubeconfig/:0:2","tags":["kubernetes","authentication","authorization"],"title":"kubectl 操作 Kubernetes 的权限控制","uri":"/kubeconfig/"},{"categories":null,"content":"使用 kubeconfig 我们可以用生成好的 kubeconfig 文件来赋予 kubectl 操作集群的能力 示例如下 FROM ubuntu:16.04 COPY kubectl /usr/local/bin/ RUN chmod +x /usr/local/bin/kubectl RUN mkdir /root/.kube COPY config /root/.kube/ 下面这张图总结了我们的工作 ","date":"2019-02-15","objectID":"/kubeconfig/:0:3","tags":["kubernetes","authentication","authorization"],"title":"kubectl 操作 Kubernetes 的权限控制","uri":"/kubeconfig/"},{"categories":null,"content":"JIRA 是一个非常优秀的项目管理工具，可以帮助我们缺陷管理和任务追踪。\b关于 JIRA \b这里不\b多做介绍。JIRA 本身有完善的账号系统，但是因为很多公司包括我们自己有自己的单点登录系统，\b比如我们自己搭建的 CAS。所以如果能把 JIRA 接入 CAS 才是理想的方案。这里\b介绍一下 JIRA 集成 CAS 的方案。 这里我们\b是用 docker 部署的 \bJIRA 7.x, \b理论上 \bJIRA 7.x 不管是 docker 方式还是直接在宿主机上安装\b，我们的集成方案都是适用的。 ","date":"2019-01-31","objectID":"/jira-cas/:0:0","tags":["cas","jira"],"title":"JIRA 集成单点登录系统 CAS","uri":"/jira-cas/"},{"categories":null,"content":"集成方案 Step 1 下载两个 jar 包 cas-client-core-3.3.3.jar, cas-client-integration-atlassian-3.5.0-jira7.jar. 下载地址在这里 Step 2 \b修改 web.xml 如果是直接在宿主机（linux）上安装的方式, 修改 /opt/atlassian/jira/atlassian-jira/WEB-INF/web.xml(默认路径) 如果是 docker 方式， 我们可以先从容器中 copy 出 web.xml docker cp container_id:/opt/atlassian/jira/atlassian-jira/WEB-INF/web.xml . 编写好如下内容 \u003c!-- CAS:START - Java Client Filters --\u003e \u003cfilter\u003e \u003cfilter-name\u003eCasSingleSignOutFilter\u003c/filter-name\u003e \u003cfilter-class\u003eorg.jasig.cas.client.session.SingleSignOutFilter\u003c/filter-class\u003e \u003c/filter\u003e \u003cfilter\u003e \u003cfilter-name\u003eCasAuthenticationFilter\u003c/filter-name\u003e \u003cfilter-class\u003eorg.jasig.cas.client.authentication.AuthenticationFilter\u003c/filter-class\u003e \u003cinit-param\u003e \u003cparam-name\u003ecasServerLoginUrl\u003c/param-name\u003e \u003cparam-value\u003e Include your CAS login here \u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003cparam-name\u003eserverName\u003c/param-name\u003e \u003cparam-value\u003e include your JIRA url here \u003c/param-value\u003e \u003c/init-param\u003e \u003c/filter\u003e \u003cfilter\u003e \u003cfilter-name\u003eCasValidationFilter\u003c/filter-name\u003e \u003cfilter-class\u003eorg.jasig.cas.client.validation.Cas20ProxyReceivingTicketValidationFilter\u003c/filter-class\u003e \u003cinit-param\u003e \u003cparam-name\u003ecasServerUrlPrefix\u003c/param-name\u003e \u003cparam-value\u003eInclude your CAS address\u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003cparam-name\u003eserverName\u003c/param-name\u003e \u003cparam-value\u003einclude your JIRA url here\u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003cparam-name\u003eredirectAfterValidation\u003c/param-name\u003e \u003cparam-value\u003etrue\u003c/param-value\u003e \u003c/init-param\u003e \u003c/filter\u003e \u003c!--- CAS:END --\u003e 将上面的配置复制进 web.xml 大概380行的位置 THIS MUST BE THE LAST FILTER IN THE DEFINED CHAIN 的上面 在大概 640行的位置 \u003cfilter-mapping\u003e \u003cfilter-name\u003elogin\u003c/filter-name\u003e \u003curl-pattern\u003e/*\u003c/url-pattern\u003e \u003cdispatcher\u003eREQUEST\u003c/dispatcher\u003e \u003cdispatcher\u003eFORWARD\u003c/dispatcher\u003e \u003c!-- we want security/login to be applied after urlrewrites, for example --\u003e \u003c/filter-mapping\u003e 在这部分的上面复制\b以下的配置 \u003c!-- CAS:START - Java Client Filter Mappings --\u003e \u003cfilter-mapping\u003e \u003cfilter-name\u003eCasSingleSignOutFilter\u003c/filter-name\u003e \u003curl-pattern\u003e/*\u003c/url-pattern\u003e \u003c/filter-mapping\u003e \u003cfilter-mapping\u003e \u003cfilter-name\u003eCasAuthenticationFilter\u003c/filter-name\u003e \u003curl-pattern\u003e/default.jsp\u003c/url-pattern\u003e \u003c/filter-mapping\u003e \u003cfilter-mapping\u003e \u003cfilter-name\u003eCasValidationFilter\u003c/filter-name\u003e \u003curl-pattern\u003e/*\u003c/url-pattern\u003e \u003c/filter-mapping\u003e \u003c!-- CAS:END --\u003e ","date":"2019-01-31","objectID":"/jira-cas/:1:0","tags":["cas","jira"],"title":"JIRA 集成单点登录系统 CAS","uri":"/jira-cas/"},{"categories":null,"content":"Step 3 修改 seraph-config.xml 和 web.xml类似，只是路径变为 /opt/atlassian/jira/atlassian-jira/WEB-INF/classes/ 修改这部分内容 \u003cinit-param\u003e \u003c!-- The login URL to redirect to when the user tries to access a protected resource (rather than clicking on an explicit login link). Most of the time, this will be the same value as 'link.login.url'. - if the URL is absolute (contains '://'), then redirect that URL (for SSO applications) - else the context path will be prepended to this URL If '${originalurl}' is present in the URL, it will be replaced with the URL that the user requested. This gives SSO login pages the chance to redirect to the original page --\u003e \u003cparam-name\u003elogin.url\u003c/param-name\u003e \u003c!--\u003cparam-value\u003e/login.jsp?os_destination=${originalurl}\u003c/param-value\u003e--\u003e \u003cparam-value\u003eadd your CAS login URL here\u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003c!-- the URL to redirect to when the user explicitly clicks on a login link (rather than being redirected after trying to access a protected resource). Most of the time, this will be the same value as 'login.url'. - same properties as login.url above --\u003e \u003cparam-name\u003elink.login.url\u003c/param-name\u003e \u003c!--\u003cparam-value\u003e/login.jsp?os_destination=${originalurl}\u003c/param-value\u003e--\u003e \u003c!--\u003cparam-value\u003e/secure/Dashboard.jspa?os_destination=${originalurl}\u003c/param-value\u003e--\u003e \u003cparam-value\u003eadd your CAS login URL here\u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003c!-- URL for logging out. - If relative, Seraph just redirects to this URL, which is responsible for calling Authenticator.logout(). - If absolute (eg. SSO applications), Seraph calls Authenticator.logout() and redirects to the URL --\u003e \u003cparam-name\u003elogout.url\u003c/param-name\u003e \u003c!--\u003cparam-value\u003e/secure/Logout!default.jspa\u003c/param-value\u003e--\u003e \u003cparam-value\u003eadd your CAS LOGOUT URL here\u003c/param-value\u003e \u003c/init-param\u003e 大概 95 行的位置注释\b掉 SSOSeraphAuthenticator 和 JIRASeraphAuthenticator 的部分 用下面的部分替代 JIRASeraphAuthenticator \u003cauthenticator class=\"org.jasig.cas.client.integration.atlassian.Jira7CasAuthenticator\"\u003e \u003cinit-param\u003e \u003cparam-name\u003ecasServerUrlPrefix\u003c/param-name\u003e \u003cparam-value\u003einclude your cas server here\u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003cparam-name\u003eserverName\u003c/param-name\u003e \u003cparam-value\u003einclude your JIRA server URL\u003c/param-value\u003e \u003c/init-param\u003e \u003c/authenticator\u003e ","date":"2019-01-31","objectID":"/jira-cas/:1:1","tags":["cas","jira"],"title":"JIRA 集成单点登录系统 CAS","uri":"/jira-cas/"},{"categories":null,"content":"step 4 \b把准备好的 jar 包和配置文件复制进 JIRA 相应的目录 如果是宿主机安装方式直接复制\b即可 我们采用 docker 安装， \b所以修改一下 dockerfile COPY cas-client-core-3.3.3.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/ COPY cas-client-integration-atlassian-3.5.0-jira7.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/ COPY web.xml /opt/atlassian/jira/atlassian-jira/WEB-INF/ COPY seraph-config.xml /opt/atlassian/jira/atlassian-jira/WEB-INF/classes/ 至此，我们重启 JIRA \b加载新的 jar 包和配置文件，JIRA 与 CAS 的集成\b就完成了\b，应该就可以用 CAS 的账号登录 JIRA。 ","date":"2019-01-31","objectID":"/jira-cas/:1:2","tags":["cas","jira"],"title":"JIRA 集成单点登录系统 CAS","uri":"/jira-cas/"},{"categories":null,"content":"JIRA 是一个非常优秀的项目管理工具，可以帮助我们缺陷管理和任务追踪。\b关于 JIRA \b这里不\b多做介绍。JIRA 本身有完善的账号系统，但是因为很多公司包括我们自己有自己的单点登录系统，\b比如我们自己搭建的 CAS。所以如果能把 JIRA 接入 CAS 才是理想的方案。这里\b介绍一下 JIRA 集成 CAS 的方案。 这里我们\b是用 docker 部署的 \bJIRA 7.x, \b理论上 \bJIRA 7.x 不管是 docker 方式还是直接在宿主机上安装\b，我们的集成方案都是适用的。 ","date":"2019-01-31","objectID":"/jira-cas/:0:0","tags":["cas","jira"],"title":"JIRA 集成单点登录系统 CAS","uri":"/jira-cas/"},{"categories":null,"content":"集成方案 Step 1 下载两个 jar 包 cas-client-core-3.3.3.jar, cas-client-integration-atlassian-3.5.0-jira7.jar. 下载地址在这里 Step 2 \b修改 web.xml 如果是直接在宿主机（linux）上安装的方式, 修改 /opt/atlassian/jira/atlassian-jira/WEB-INF/web.xml(默认路径) 如果是 docker 方式， 我们可以先从容器中 copy 出 web.xml docker cp container_id:/opt/atlassian/jira/atlassian-jira/WEB-INF/web.xml . 编写好如下内容 \u003c!-- CAS:START - Java Client Filters --\u003e \u003cfilter\u003e \u003cfilter-name\u003eCasSingleSignOutFilter\u003c/filter-name\u003e \u003cfilter-class\u003eorg.jasig.cas.client.session.SingleSignOutFilter\u003c/filter-class\u003e \u003c/filter\u003e \u003cfilter\u003e \u003cfilter-name\u003eCasAuthenticationFilter\u003c/filter-name\u003e \u003cfilter-class\u003eorg.jasig.cas.client.authentication.AuthenticationFilter\u003c/filter-class\u003e \u003cinit-param\u003e \u003cparam-name\u003ecasServerLoginUrl\u003c/param-name\u003e \u003cparam-value\u003e Include your CAS login here \u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003cparam-name\u003eserverName\u003c/param-name\u003e \u003cparam-value\u003e include your JIRA url here \u003c/param-value\u003e \u003c/init-param\u003e \u003c/filter\u003e \u003cfilter\u003e \u003cfilter-name\u003eCasValidationFilter\u003c/filter-name\u003e \u003cfilter-class\u003eorg.jasig.cas.client.validation.Cas20ProxyReceivingTicketValidationFilter\u003c/filter-class\u003e \u003cinit-param\u003e \u003cparam-name\u003ecasServerUrlPrefix\u003c/param-name\u003e \u003cparam-value\u003eInclude your CAS address\u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003cparam-name\u003eserverName\u003c/param-name\u003e \u003cparam-value\u003einclude your JIRA url here\u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003cparam-name\u003eredirectAfterValidation\u003c/param-name\u003e \u003cparam-value\u003etrue\u003c/param-value\u003e \u003c/init-param\u003e \u003c/filter\u003e \u003c!--- CAS:END --\u003e 将上面的配置复制进 web.xml 大概380行的位置 THIS MUST BE THE LAST FILTER IN THE DEFINED CHAIN 的上面 在大概 640行的位置 \u003cfilter-mapping\u003e \u003cfilter-name\u003elogin\u003c/filter-name\u003e \u003curl-pattern\u003e/*\u003c/url-pattern\u003e \u003cdispatcher\u003eREQUEST\u003c/dispatcher\u003e \u003cdispatcher\u003eFORWARD\u003c/dispatcher\u003e \u003c!-- we want security/login to be applied after urlrewrites, for example --\u003e \u003c/filter-mapping\u003e 在这部分的上面复制\b以下的配置 \u003c!-- CAS:START - Java Client Filter Mappings --\u003e \u003cfilter-mapping\u003e \u003cfilter-name\u003eCasSingleSignOutFilter\u003c/filter-name\u003e \u003curl-pattern\u003e/*\u003c/url-pattern\u003e \u003c/filter-mapping\u003e \u003cfilter-mapping\u003e \u003cfilter-name\u003eCasAuthenticationFilter\u003c/filter-name\u003e \u003curl-pattern\u003e/default.jsp\u003c/url-pattern\u003e \u003c/filter-mapping\u003e \u003cfilter-mapping\u003e \u003cfilter-name\u003eCasValidationFilter\u003c/filter-name\u003e \u003curl-pattern\u003e/*\u003c/url-pattern\u003e \u003c/filter-mapping\u003e \u003c!-- CAS:END --\u003e ","date":"2019-01-31","objectID":"/jira-cas/:1:0","tags":["cas","jira"],"title":"JIRA 集成单点登录系统 CAS","uri":"/jira-cas/"},{"categories":null,"content":"Step 3 修改 seraph-config.xml 和 web.xml类似，只是路径变为 /opt/atlassian/jira/atlassian-jira/WEB-INF/classes/ 修改这部分内容 \u003cinit-param\u003e \u003c!-- The login URL to redirect to when the user tries to access a protected resource (rather than clicking on an explicit login link). Most of the time, this will be the same value as 'link.login.url'. - if the URL is absolute (contains '://'), then redirect that URL (for SSO applications) - else the context path will be prepended to this URL If '${originalurl}' is present in the URL, it will be replaced with the URL that the user requested. This gives SSO login pages the chance to redirect to the original page --\u003e \u003cparam-name\u003elogin.url\u003c/param-name\u003e \u003c!--\u003cparam-value\u003e/login.jsp?os_destination=${originalurl}\u003c/param-value\u003e--\u003e \u003cparam-value\u003eadd your CAS login URL here\u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003c!-- the URL to redirect to when the user explicitly clicks on a login link (rather than being redirected after trying to access a protected resource). Most of the time, this will be the same value as 'login.url'. - same properties as login.url above --\u003e \u003cparam-name\u003elink.login.url\u003c/param-name\u003e \u003c!--\u003cparam-value\u003e/login.jsp?os_destination=${originalurl}\u003c/param-value\u003e--\u003e \u003c!--\u003cparam-value\u003e/secure/Dashboard.jspa?os_destination=${originalurl}\u003c/param-value\u003e--\u003e \u003cparam-value\u003eadd your CAS login URL here\u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003c!-- URL for logging out. - If relative, Seraph just redirects to this URL, which is responsible for calling Authenticator.logout(). - If absolute (eg. SSO applications), Seraph calls Authenticator.logout() and redirects to the URL --\u003e \u003cparam-name\u003elogout.url\u003c/param-name\u003e \u003c!--\u003cparam-value\u003e/secure/Logout!default.jspa\u003c/param-value\u003e--\u003e \u003cparam-value\u003eadd your CAS LOGOUT URL here\u003c/param-value\u003e \u003c/init-param\u003e 大概 95 行的位置注释\b掉 SSOSeraphAuthenticator 和 JIRASeraphAuthenticator 的部分 用下面的部分替代 JIRASeraphAuthenticator \u003cauthenticator class=\"org.jasig.cas.client.integration.atlassian.Jira7CasAuthenticator\"\u003e \u003cinit-param\u003e \u003cparam-name\u003ecasServerUrlPrefix\u003c/param-name\u003e \u003cparam-value\u003einclude your cas server here\u003c/param-value\u003e \u003c/init-param\u003e \u003cinit-param\u003e \u003cparam-name\u003eserverName\u003c/param-name\u003e \u003cparam-value\u003einclude your JIRA server URL\u003c/param-value\u003e \u003c/init-param\u003e \u003c/authenticator\u003e ","date":"2019-01-31","objectID":"/jira-cas/:1:1","tags":["cas","jira"],"title":"JIRA 集成单点登录系统 CAS","uri":"/jira-cas/"},{"categories":null,"content":"step 4 \b把准备好的 jar 包和配置文件复制进 JIRA 相应的目录 如果是宿主机安装方式直接复制\b即可 我们采用 docker 安装， \b所以修改一下 dockerfile COPY cas-client-core-3.3.3.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/ COPY cas-client-integration-atlassian-3.5.0-jira7.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/ COPY web.xml /opt/atlassian/jira/atlassian-jira/WEB-INF/ COPY seraph-config.xml /opt/atlassian/jira/atlassian-jira/WEB-INF/classes/ 至此，我们重启 JIRA \b加载新的 jar 包和配置文件，JIRA 与 CAS 的集成\b就完成了\b，应该就可以用 CAS 的账号登录 JIRA。 ","date":"2019-01-31","objectID":"/jira-cas/:1:2","tags":["cas","jira"],"title":"JIRA 集成单点登录系统 CAS","uri":"/jira-cas/"},{"categories":null,"content":"在之前的文章中我们提到过, 服务拆分后，不同的服务之间必须进行交互才能实现特定的功能。在这里引入 service mesh 这个概念，service mesh 通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着服务的规模和复杂性的增长，service mesh 变的越来越不容易理解和管理。这个时候必须引入相应的解决方案，于是有了 Istio, Linkerd 等 service mesh 框架。以 Istio 为例，它旨在解决大量微服务的发现、连接、管理、监控以及安全等问题。Istio 对应用是透明的，不需要改动任何服务代码就可以实现透明的服务治理。 ","date":"2019-01-25","objectID":"/istio-init/:0:0","tags":["service mesh","istio","服务治理"],"title":"Istio 实践","uri":"/istio-init/"},{"categories":null,"content":"Istio 特性 Istio 只需要在我们的应用部署的环境中部署一个 sidecar 代理并使用 Istio 控制平面功能配置和管理代理，这个代理会拦截服务之间的通信。从而带来了以下特性 HTTP、gRPC、WebSocket 和 TCP 流量的自动负载均衡。 通过丰富的路由规则、重试、故障转移和故障注入，可以对流量行为进行细粒度控制。 可插入的策略层和配置 API，支持访问控制、速率限制和配额。 对出入集群入口和出口中所有流量的自动度量指标、日志记录和跟踪。 通过强大的基于身份的验证和授权，在集群中实现安全的服务间通信。 ","date":"2019-01-25","objectID":"/istio-init/:1:0","tags":["service mesh","istio","服务治理"],"title":"Istio 实践","uri":"/istio-init/"},{"categories":null,"content":"Istio 基本原理 Istio 服务网格逻辑上分为数据平面和控制平面。 数据平面由一组以 sidecar 方式部署的智能代理（Envoy）组成。这些代理可以调节和控制微服务及 Mixer 之间所有的网络通信。 控制平面负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。 下图显示了构成数据平面和控制平面的不同组件： 核心组件 Envoy Lyft 开源的高性能代理，用于调解服务网格中所有服务的入站和出站流量。它支持动态服务发现、负载均衡、TLS 终止、HTTP/2 和 gPRC 代理、熔断、健康检查、故障注入和性能测量等丰富的功能。Envoy 以 sidecar 的方式部署在相关的服务的 Pod 中，从而无需重新构建或重写代码。 Mixer 负责访问控制、执行策略并从 Envoy 代理中收集遥测数据。Mixer 支持灵活的插件模型，方便扩展（支持 GCP、AWS、Prometheus、Heapster 等多种后端）。 Pilot 动态管理 Envoy 实例的生命周期，提供服务发现、智能路由和弹性流量管理（如超时、重试）等功能。它将流量管理策略转化为 Envoy 数据平面配置，并传播到 sidecar 中。 Pilot 为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。它将控制流量行为的高级路由规则转换为特定于 Envoy 的配置，并在运行时将它们传播到 sidecar。Pilot 将服务发现机制抽象为符合 Envoy 数据平面 API 的标准格式，以便支持在多种环境下运行并保持流量管理的相同操作接口。 Citadel Citadel 通过内置身份和凭证管理提供服务间和最终用户的身份认证。支持基于角色的访问控制、基于服务标识的策略执行等。 Galle 是 Istio 1.0 中新引入的组件，在 Istio 中，承担配置的导入、处理和分发任务，为 Istio 提供了配置管理服务,提供在 k8s 服务端验证 Istio 的 CRD 资源的合法性的方法。 ","date":"2019-01-25","objectID":"/istio-init/:2:0","tags":["service mesh","istio","服务治理"],"title":"Istio 实践","uri":"/istio-init/"},{"categories":null,"content":"在 k8s 中搭建 Istio 使用 helm 方式 1. 下载 istio \b安装包 wget https://github.com/istio/istio/releases/download/1.0.4/istio-1.0.4-linux.tar.gz cd istio-1.0.4 export PATH=$PWD/bin:$PATH 2. 下载 helm wget https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz 解压 and mv linux-amd64/helm /usr/local/bin/helm 3. 安装 给 tiller 生成 cluste-admin的 serviceaccount kubectl apply -f install/kubernetes/helm/helm-service-account.yaml 部署 tiller helm init -i anjia0532/kubernetes-helm.tiller:v2.11.0 --service-account tiller 安装 istio（默认方式） helm install install/kubernetes/helm/istio --name istio --namespace istio-system 自定义方式 获取 cluster service 的 ip range, 使用这个 iprange pass \bcluster所有出口的\b请求 kubectl cluster-info dump | grep service-cluster-ip-range helm install install/kubernetes/helm/istio --name istio --namespace istio-system \\ --set global.proxy.includeIPRanges=\"10.233.0.0/18\" \\ --set gateways.istio-ingressgateway.type=NodePort \\ --set tracing.enabled=true \\ --set kiali.enabled=true \\ --set grafana.enabled=true 是否成功安装 helm ls --all kubeclt get pod -n istio-system kubectl get service -n istio-system ","date":"2019-01-25","objectID":"/istio-init/:3:0","tags":["service mesh","istio","服务治理"],"title":"Istio 实践","uri":"/istio-init/"},{"categories":null,"content":"使用 Istio 1. 部署服务 首先要把服务所在的 namespace 开启 Istio sidecar 自动注入 kubectl label namespace \u003cnamespace\u003e istio-injection=enabled 然后我们部署一个 hello-service 的服务, 这个服务我们部署两个不同的版本 v1 和 v2 示例的 service 和 deploymnent 配置如下 apiVersion: v1 kind: Service metadata: name: hello-service labels: app: hello-service spec: #type: NodePort ports: - port: 8888 name: http selector: app: hello-service --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: hello-service-v1 spec: revisionHistoryLimit: 3 strategy: type: RollingUpdate rollingUpdate: maxSurge: 3 maxUnavailable: 1 replicas: 1 template: metadata: labels: app: hello-service version: v1 spec: containers: - name: hello-service resources: requests: memory: \"64Mi\" cpu: \"125m\" limits: memory: \"128Mi\" cpu: \"250m\" image: kaka_server:v1 imagePullPolicy: Always ports: - containerPort: 8888 livenessProbe: httpGet: path: / port: 8888 initialDelaySeconds: 1 periodSeconds: 10 timeoutSeconds: 2 successThreshold: 1 failureThreshold: 3 volumeMounts: - mountPath: /home/kaka/logs name: hello volumes: - name: hello hostPath: path: /var/log/k8s type: DirectoryOrCreate --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: hello-service-v2 spec: revisionHistoryLimit: 3 replicas: 1 template: metadata: labels: app: hello-service version: v2 spec: containers: - name: hello-service image: kaka_server:v2 imagePullPolicy: Always ports: - containerPort: 8888 2. 创建 Istio CRD 首先了解下面的几个重要的 CRD，对我们服务的部署才能有更好的理解 Gateway：为网格配置网关，以允许一个服务可以被网格外部访问。 Virtualservice：用于定义路由规则，如根据来源或 Header 制定规则，或在不同服务版本之间分拆流量。 DestinationRule：定义目的服务的配置策略以及可路由子集。策略包括断路器、负载均衡以及 TLS 等。 ServiceEntry：用 ServiceEntry 可以向Istio中加入附加的服务条目，以使网格内可以向istio 服务网格之外的服务发出请求。 1. 创建 Istio Ingress Gateway 示例配置文件如下 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: hello-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" 这个 gateway 的作用作为我们整个 service mesh 的流量入口 2. 创建 Virtualservice 这里我们定义两个路由规则, 把 80% 的流量路由到服务版本 v1, 20% 的流量路由到服务版本 v2 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: hello-service spec: hosts: - \"hello-service.example.com\" gateways: - hello-gateway http: - match: - uri: prefix: / route: - destination: #port: #number: 8888 host: hello-service subset: v1 weight: 80 - destination: # port: # number: 8888 host: hello-service subset: v2 weight: 20 3. 创建 DestinationRule 定义服务的配置策略，示例把 不同的服务版本定义成了不同的 subset apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: hello-service spec: host: hello-service subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 以上完成以后，我们访问 http://hello-service.example.com 就能看到大部分请求访问到了 hello-service v1, 小部分请求访问到了 hello-service v2. 下面这张图显示了 hello-service 在 Istio service mesh 下的流量管理 ","date":"2019-01-25","objectID":"/istio-init/:4:0","tags":["service mesh","istio","服务治理"],"title":"Istio 实践","uri":"/istio-init/"},{"categories":null,"content":"在之前的文章中我们提到过, 服务拆分后，不同的服务之间必须进行交互才能实现特定的功能。在这里引入 service mesh 这个概念，service mesh 通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着服务的规模和复杂性的增长，service mesh 变的越来越不容易理解和管理。这个时候必须引入相应的解决方案，于是有了 Istio, Linkerd 等 service mesh 框架。以 Istio 为例，它旨在解决大量微服务的发现、连接、管理、监控以及安全等问题。Istio 对应用是透明的，不需要改动任何服务代码就可以实现透明的服务治理。 ","date":"2019-01-25","objectID":"/istio-init/:0:0","tags":["service mesh","istio","服务治理"],"title":"Istio 实践","uri":"/istio-init/"},{"categories":null,"content":"Istio 特性 Istio 只需要在我们的应用部署的环境中部署一个 sidecar 代理并使用 Istio 控制平面功能配置和管理代理，这个代理会拦截服务之间的通信。从而带来了以下特性 HTTP、gRPC、WebSocket 和 TCP 流量的自动负载均衡。 通过丰富的路由规则、重试、故障转移和故障注入，可以对流量行为进行细粒度控制。 可插入的策略层和配置 API，支持访问控制、速率限制和配额。 对出入集群入口和出口中所有流量的自动度量指标、日志记录和跟踪。 通过强大的基于身份的验证和授权，在集群中实现安全的服务间通信。 ","date":"2019-01-25","objectID":"/istio-init/:1:0","tags":["service mesh","istio","服务治理"],"title":"Istio 实践","uri":"/istio-init/"},{"categories":null,"content":"Istio 基本原理 Istio 服务网格逻辑上分为数据平面和控制平面。 数据平面由一组以 sidecar 方式部署的智能代理（Envoy）组成。这些代理可以调节和控制微服务及 Mixer 之间所有的网络通信。 控制平面负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。 下图显示了构成数据平面和控制平面的不同组件： 核心组件 Envoy Lyft 开源的高性能代理，用于调解服务网格中所有服务的入站和出站流量。它支持动态服务发现、负载均衡、TLS 终止、HTTP/2 和 gPRC 代理、熔断、健康检查、故障注入和性能测量等丰富的功能。Envoy 以 sidecar 的方式部署在相关的服务的 Pod 中，从而无需重新构建或重写代码。 Mixer 负责访问控制、执行策略并从 Envoy 代理中收集遥测数据。Mixer 支持灵活的插件模型，方便扩展（支持 GCP、AWS、Prometheus、Heapster 等多种后端）。 Pilot 动态管理 Envoy 实例的生命周期，提供服务发现、智能路由和弹性流量管理（如超时、重试）等功能。它将流量管理策略转化为 Envoy 数据平面配置，并传播到 sidecar 中。 Pilot 为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。它将控制流量行为的高级路由规则转换为特定于 Envoy 的配置，并在运行时将它们传播到 sidecar。Pilot 将服务发现机制抽象为符合 Envoy 数据平面 API 的标准格式，以便支持在多种环境下运行并保持流量管理的相同操作接口。 Citadel Citadel 通过内置身份和凭证管理提供服务间和最终用户的身份认证。支持基于角色的访问控制、基于服务标识的策略执行等。 Galle 是 Istio 1.0 中新引入的组件，在 Istio 中，承担配置的导入、处理和分发任务，为 Istio 提供了配置管理服务,提供在 k8s 服务端验证 Istio 的 CRD 资源的合法性的方法。 ","date":"2019-01-25","objectID":"/istio-init/:2:0","tags":["service mesh","istio","服务治理"],"title":"Istio 实践","uri":"/istio-init/"},{"categories":null,"content":"在 k8s 中搭建 Istio 使用 helm 方式 1. 下载 istio \b安装包 wget https://github.com/istio/istio/releases/download/1.0.4/istio-1.0.4-linux.tar.gz cd istio-1.0.4 export PATH=$PWD/bin:$PATH 2. 下载 helm wget https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz 解压 and mv linux-amd64/helm /usr/local/bin/helm 3. 安装 给 tiller 生成 cluste-admin的 serviceaccount kubectl apply -f install/kubernetes/helm/helm-service-account.yaml 部署 tiller helm init -i anjia0532/kubernetes-helm.tiller:v2.11.0 --service-account tiller 安装 istio（默认方式） helm install install/kubernetes/helm/istio --name istio --namespace istio-system 自定义方式 获取 cluster service 的 ip range, 使用这个 iprange pass \bcluster所有出口的\b请求 kubectl cluster-info dump | grep service-cluster-ip-range helm install install/kubernetes/helm/istio --name istio --namespace istio-system \\ --set global.proxy.includeIPRanges=\"10.233.0.0/18\" \\ --set gateways.istio-ingressgateway.type=NodePort \\ --set tracing.enabled=true \\ --set kiali.enabled=true \\ --set grafana.enabled=true 是否成功安装 helm ls --all kubeclt get pod -n istio-system kubectl get service -n istio-system ","date":"2019-01-25","objectID":"/istio-init/:3:0","tags":["service mesh","istio","服务治理"],"title":"Istio 实践","uri":"/istio-init/"},{"categories":null,"content":"使用 Istio 1. 部署服务 首先要把服务所在的 namespace 开启 Istio sidecar 自动注入 kubectl label namespace \u003cnamespace\u003e istio-injection=enabled 然后我们部署一个 hello-service 的服务, 这个服务我们部署两个不同的版本 v1 和 v2 示例的 service 和 deploymnent 配置如下 apiVersion: v1 kind: Service metadata: name: hello-service labels: app: hello-service spec: #type: NodePort ports: - port: 8888 name: http selector: app: hello-service --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: hello-service-v1 spec: revisionHistoryLimit: 3 strategy: type: RollingUpdate rollingUpdate: maxSurge: 3 maxUnavailable: 1 replicas: 1 template: metadata: labels: app: hello-service version: v1 spec: containers: - name: hello-service resources: requests: memory: \"64Mi\" cpu: \"125m\" limits: memory: \"128Mi\" cpu: \"250m\" image: kaka_server:v1 imagePullPolicy: Always ports: - containerPort: 8888 livenessProbe: httpGet: path: / port: 8888 initialDelaySeconds: 1 periodSeconds: 10 timeoutSeconds: 2 successThreshold: 1 failureThreshold: 3 volumeMounts: - mountPath: /home/kaka/logs name: hello volumes: - name: hello hostPath: path: /var/log/k8s type: DirectoryOrCreate --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: hello-service-v2 spec: revisionHistoryLimit: 3 replicas: 1 template: metadata: labels: app: hello-service version: v2 spec: containers: - name: hello-service image: kaka_server:v2 imagePullPolicy: Always ports: - containerPort: 8888 2. 创建 Istio CRD 首先了解下面的几个重要的 CRD，对我们服务的部署才能有更好的理解 Gateway：为网格配置网关，以允许一个服务可以被网格外部访问。 Virtualservice：用于定义路由规则，如根据来源或 Header 制定规则，或在不同服务版本之间分拆流量。 DestinationRule：定义目的服务的配置策略以及可路由子集。策略包括断路器、负载均衡以及 TLS 等。 ServiceEntry：用 ServiceEntry 可以向Istio中加入附加的服务条目，以使网格内可以向istio 服务网格之外的服务发出请求。 1. 创建 Istio Ingress Gateway 示例配置文件如下 apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: hello-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" 这个 gateway 的作用作为我们整个 service mesh 的流量入口 2. 创建 Virtualservice 这里我们定义两个路由规则, 把 80% 的流量路由到服务版本 v1, 20% 的流量路由到服务版本 v2 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: hello-service spec: hosts: - \"hello-service.example.com\" gateways: - hello-gateway http: - match: - uri: prefix: / route: - destination: #port: #number: 8888 host: hello-service subset: v1 weight: 80 - destination: # port: # number: 8888 host: hello-service subset: v2 weight: 20 3. 创建 DestinationRule 定义服务的配置策略，示例把 不同的服务版本定义成了不同的 subset apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: hello-service spec: host: hello-service subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 以上完成以后，我们访问 http://hello-service.example.com 就能看到大部分请求访问到了 hello-service v1, 小部分请求访问到了 hello-service v2. 下面这张图显示了 hello-service 在 Istio service mesh 下的流量管理 ","date":"2019-01-25","objectID":"/istio-init/:4:0","tags":["service mesh","istio","服务治理"],"title":"Istio 实践","uri":"/istio-init/"},{"categories":null,"content":"背景 任何一个公司都会有各种各样的内部系统，这些内部系统有可能是自建的也有可能是第三方的。这些系统很可能都带有一定的认证和鉴权，如果各个系统都需要一套用户名和密码，不仅给员工带来巨大的记忆负担，更重要的是会有很棘手的管理问题。在这中背景下我们就需要个 单点登录系统(SSO), 这样我们登录了单点系统后就能访问其他任意的系统。这篇文章的主题不是 SSO 的搭建， 所以这里不讲怎么建立一套单点登录系统。 我们建立自己的 SSO 系统使用的是 CAS(Central Authentication Service)协议。要把各种各样的系统与单点系统集成，会随着各系统不同的认证方式，集成的方案也不一样。这篇文章我们就讲一下怎么跟我们的基础设施提供者 AWS 的 web 管理后台集成。 AWS 支持用户使用外部身份登录，方案之一是利用身份提供商 (IdP)，可以管理 AWS 外部的用户身份，并向这些外部用户身份授予 AWS 资源的权限。 我们有自己的 CAS 但是不能作为 IdP 使用。 我们使用的方案总结下来就是 自建 SSO + AWS STS ","date":"2018-12-27","objectID":"/aws-cas/:1:0","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"AWS STS 先简单介绍一下 STS (很好很强大) AWS Security Token Service (AWS STS) 可以用来创建 AWS 资源的访问的临时安全凭证，并将这些凭证提供给可信用户。临时安全凭证的工作方式与 IAM 用户可使用的长期访问密钥凭证的工作方式几乎相同，仅存在以下差异： 顾名思义，临时安全凭证是短期凭证。可将这些凭证的有效时间配置几分钟到几小时。一旦这些凭证到期，AWS 将不再识别这些凭证或不再允许来自使用这些凭证发出的 API 请求的任何类型的访问。 临时安全凭证不随用户一起存储，而是动态生成并在用户提出请求时提供给用户。临时安全凭证到期时 (甚至之前)，用户可以请求新的凭证。 这些差异使得可利用临时凭证获得以下优势： 不必随应用程序分配或嵌入长期 AWS 安全凭证。 可允许用户访问 AWS 资源，而不必为这些用户定义 AWS 身份。临时凭证是角色和联合身份验证的基础。 临时安全凭证的使用期限有限，因此，在不需要这些证书时不必轮换或显式撤消这些证书。临时安全凭证到期后无法重复使用。可指定证书的有效期，但有最长限期。 ","date":"2018-12-27","objectID":"/aws-cas/:2:0","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"集成步骤 下面这张图展示了 CAS 与 AWS 集成后，登录 console 的流程 用户通过 CAS 认证和鉴权; 当通过 CAS 的验证后，CAS 会根据用户的权限向 AWS STS 发起请求申请临时的安全凭证; 获得STS 授予的安全凭证后，用安全凭证继续向 AWS STS 换取登录 token; 获得登录 token 后， CAS 会拼接完整的登录 url 并且把用户 redirect 到 AWS Console. 这样一个通过我们自己单点登录系统验证的用户就可以登录 AWS Consol ","date":"2018-12-27","objectID":"/aws-cas/:3:0","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"1.创建 IAM Role for STS access 使用 aws access key 和 aws access secret 本身就存在安全隐患，所以为了最大程度的安全，我们没有直接使用 access key + access secret与 AWS STS交互。 更加安全的方式是给 ec2 实例绑定 IAM role， 这样 ec2 会被授予相应 role 的权限，而我们不用考虑 access key 和 secret 的安全问题。 所以第一步创建一个 具有 STS assume role 权限的 IAM role { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"*\" } ] } ","date":"2018-12-27","objectID":"/aws-cas/:3:1","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"2. 创建 IAM role for user 这一步我们需要给用户创建具有一定权限的 IAM Role, 因为 STS 是根据这个 role 来生成对应的安全临时凭证。这一步创建 role 没有什么特别的，但是一定给要这个 role 的 trust relationship 添加我们 aws 的根账户, 这样我们在应用里才能跟 AWS STS 交互并且 assume role { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"monitoring.rds.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" }, # 下面的 trust entity 一定要添加 { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws-cn:iam::xxxxxx(根账号):root\" }, \"Action\": \"sts:AssumeRole\" } ] } ","date":"2018-12-27","objectID":"/aws-cas/:3:2","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"3. 绑定 STSAccessRole 到 ec2 实例 这一步比较简单，就是把我们在第一步中创建的 IAM Role 绑定到我们 CAS 的实例上。这样我们运行 CAS 的实例就被授权能与 AWS STS 交互 ","date":"2018-12-27","objectID":"/aws-cas/:3:3","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"4. 与 AWS STS 交互 下面的 Python 代码片段展示了我们怎样去跟 STS 交互 client = boto3.client('sts', region_name='cn-north-1') assumed_role = client.assume_role( RoleArn=role_arn, RoleSessionName=session_name, DurationSeconds=3600 ) temp_credentials = { 'sessionId': assumed_role['Credentials']['AccessKeyId'], 'sessionKey': assumed_role['Credentials']['SecretAccessKey'], 'sessionToken': assumed_role['Credentials']['SessionToken'] } temp_credentials = json.dumps(temp_credentials) signin_token_parameters = '?Action=getSigninToken\u0026SessionDuration=43200\u0026Session={}'.format( urllib.parse.quote_plus(temp_credentials)) signin_url = 'https://signin.amazonaws.cn/federation' get_token_url = signin_url + signin_token_parameters res = requests.get(get_token_url) if res.status_code == 200: signin_token = res.json() login_parameters = '?Action=login\u0026Issuer=zaihui\u0026Destination={}\u0026SigninToken={}'.format( urllib.parse.quote_plus('https://console.amazonaws.cn/'), signin_token['SigninToken'] ) login_url = signin_url + login_parameters 完成以上步骤后，如果我们顺利拿到 login_url 就可以拿着这个 url 愉快的登录 AWS Console 了 :) ","date":"2018-12-27","objectID":"/aws-cas/:3:4","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"背景 任何一个公司都会有各种各样的内部系统，这些内部系统有可能是自建的也有可能是第三方的。这些系统很可能都带有一定的认证和鉴权，如果各个系统都需要一套用户名和密码，不仅给员工带来巨大的记忆负担，更重要的是会有很棘手的管理问题。在这中背景下我们就需要个 单点登录系统(SSO), 这样我们登录了单点系统后就能访问其他任意的系统。这篇文章的主题不是 SSO 的搭建， 所以这里不讲怎么建立一套单点登录系统。 我们建立自己的 SSO 系统使用的是 CAS(Central Authentication Service)协议。要把各种各样的系统与单点系统集成，会随着各系统不同的认证方式，集成的方案也不一样。这篇文章我们就讲一下怎么跟我们的基础设施提供者 AWS 的 web 管理后台集成。 AWS 支持用户使用外部身份登录，方案之一是利用身份提供商 (IdP)，可以管理 AWS 外部的用户身份，并向这些外部用户身份授予 AWS 资源的权限。 我们有自己的 CAS 但是不能作为 IdP 使用。 我们使用的方案总结下来就是 自建 SSO + AWS STS ","date":"2018-12-27","objectID":"/aws-cas/:1:0","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"AWS STS 先简单介绍一下 STS (很好很强大) AWS Security Token Service (AWS STS) 可以用来创建 AWS 资源的访问的临时安全凭证，并将这些凭证提供给可信用户。临时安全凭证的工作方式与 IAM 用户可使用的长期访问密钥凭证的工作方式几乎相同，仅存在以下差异： 顾名思义，临时安全凭证是短期凭证。可将这些凭证的有效时间配置几分钟到几小时。一旦这些凭证到期，AWS 将不再识别这些凭证或不再允许来自使用这些凭证发出的 API 请求的任何类型的访问。 临时安全凭证不随用户一起存储，而是动态生成并在用户提出请求时提供给用户。临时安全凭证到期时 (甚至之前)，用户可以请求新的凭证。 这些差异使得可利用临时凭证获得以下优势： 不必随应用程序分配或嵌入长期 AWS 安全凭证。 可允许用户访问 AWS 资源，而不必为这些用户定义 AWS 身份。临时凭证是角色和联合身份验证的基础。 临时安全凭证的使用期限有限，因此，在不需要这些证书时不必轮换或显式撤消这些证书。临时安全凭证到期后无法重复使用。可指定证书的有效期，但有最长限期。 ","date":"2018-12-27","objectID":"/aws-cas/:2:0","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"集成步骤 下面这张图展示了 CAS 与 AWS 集成后，登录 console 的流程 用户通过 CAS 认证和鉴权; 当通过 CAS 的验证后，CAS 会根据用户的权限向 AWS STS 发起请求申请临时的安全凭证; 获得STS 授予的安全凭证后，用安全凭证继续向 AWS STS 换取登录 token; 获得登录 token 后， CAS 会拼接完整的登录 url 并且把用户 redirect 到 AWS Console. 这样一个通过我们自己单点登录系统验证的用户就可以登录 AWS Consol ","date":"2018-12-27","objectID":"/aws-cas/:3:0","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"1.创建 IAM Role for STS access 使用 aws access key 和 aws access secret 本身就存在安全隐患，所以为了最大程度的安全，我们没有直接使用 access key + access secret与 AWS STS交互。 更加安全的方式是给 ec2 实例绑定 IAM role， 这样 ec2 会被授予相应 role 的权限，而我们不用考虑 access key 和 secret 的安全问题。 所以第一步创建一个 具有 STS assume role 权限的 IAM role { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"*\" } ] } ","date":"2018-12-27","objectID":"/aws-cas/:3:1","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"2. 创建 IAM role for user 这一步我们需要给用户创建具有一定权限的 IAM Role, 因为 STS 是根据这个 role 来生成对应的安全临时凭证。这一步创建 role 没有什么特别的，但是一定给要这个 role 的 trust relationship 添加我们 aws 的根账户, 这样我们在应用里才能跟 AWS STS 交互并且 assume role { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"monitoring.rds.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" }, # 下面的 trust entity 一定要添加 { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws-cn:iam::xxxxxx(根账号):root\" }, \"Action\": \"sts:AssumeRole\" } ] } ","date":"2018-12-27","objectID":"/aws-cas/:3:2","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"3. 绑定 STSAccessRole 到 ec2 实例 这一步比较简单，就是把我们在第一步中创建的 IAM Role 绑定到我们 CAS 的实例上。这样我们运行 CAS 的实例就被授权能与 AWS STS 交互 ","date":"2018-12-27","objectID":"/aws-cas/:3:3","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"4. 与 AWS STS 交互 下面的 Python 代码片段展示了我们怎样去跟 STS 交互 client = boto3.client('sts', region_name='cn-north-1') assumed_role = client.assume_role( RoleArn=role_arn, RoleSessionName=session_name, DurationSeconds=3600 ) temp_credentials = { 'sessionId': assumed_role['Credentials']['AccessKeyId'], 'sessionKey': assumed_role['Credentials']['SecretAccessKey'], 'sessionToken': assumed_role['Credentials']['SessionToken'] } temp_credentials = json.dumps(temp_credentials) signin_token_parameters = '?Action=getSigninToken\u0026SessionDuration=43200\u0026Session={}'.format( urllib.parse.quote_plus(temp_credentials)) signin_url = 'https://signin.amazonaws.cn/federation' get_token_url = signin_url + signin_token_parameters res = requests.get(get_token_url) if res.status_code == 200: signin_token = res.json() login_parameters = '?Action=login\u0026Issuer=zaihui\u0026Destination={}\u0026SigninToken={}'.format( urllib.parse.quote_plus('https://console.amazonaws.cn/'), signin_token['SigninToken'] ) login_url = signin_url + login_parameters 完成以上步骤后，如果我们顺利拿到 login_url 就可以拿着这个 url 愉快的登录 AWS Console 了 :) ","date":"2018-12-27","objectID":"/aws-cas/:3:4","tags":["aws","sso"],"title":"如何把 AWS Console 登录集成到自己的 SSO","uri":"/aws-cas/"},{"categories":null,"content":"直接进入正题，这篇文章讲述生产环境如何搭建 kubernets cluster。 我们使用 kubespray 使用搭建 k8s cluster。 至于其他搭建工具 kops, kubeadm 感兴趣的大家可以自己研究一下，各有优势。 ","date":"2018-12-05","objectID":"/k8s-init/:0:0","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"环境: aws(cn-north-1) ubuntu 16.04 kubernetes v1.11.3 kubespray v2.7.0 ","date":"2018-12-05","objectID":"/k8s-init/:1:0","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"搭建 -\u003e ","date":"2018-12-05","objectID":"/k8s-init/:2:0","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"环境准备 准备实例 我们先准备5个实例，其中3个作为 master, 2个作为 node 分发公钥 选定其中一个作为我们的操作实例，因为后续操作都是通过 ssh 与各个节点交互, 先在操作节点上 ssh-keygen 生成公私钥并且把公钥id_rsa.pub分发到其他的节点上 kubespray环境 下载 kubespray wget https://github.com/kubernetes-incubator/kubespray/releases/tag/v2.7.0 解压并且按照依赖 tar xzvf v2.7.0.tar.gz \u0026\u0026 cd kubespray-2.7.0 pip install -r requirements.txt 生成清单 清单目录 cp -r inventory/sample inventory/my_cluster 声明实例地址 declare -a IPS=(172.31.xx.1 172.31.xx.2 172.31.xx.3 172.31.xx.4 172.31.xx.5) 生成目标机器的地址配置 CONFIG_FILE=inventory/my_cluster/hosts.ini python3 contrib/inventory_builder/inventory.py ${IPS[@]} ","date":"2018-12-05","objectID":"/k8s-init/:2:1","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"镜像地址修改 直接安装是无法成功的，因为我们在 cn 很多k8s需要的镜像无法直接 pull 下来。这一步我们就必须把镜像地址改成我们能拉倒的地址 主要是两类镜像 gco.io 和 quay.io，这两类镜像地址必须修改。 find . -name '*.yml' | xargs -n1 -I{} sed -i 's/quay\\.io/quay\\.mirrors\\.ustc\\.edu\\.cn/' {}` find . -name '*.yml' | xargs -n1 -I{} sed -i 's/gcr\\.io\\/google-containers\\//anjia0532\\/google-containers\\./' {}` find . -name '*.yml' | xargs -n1 -I{} sed -i 's/gcr\\.io\\/google_containers\\//anjia0532\\/google-containers\\./' {}` ","date":"2018-12-05","objectID":"/k8s-init/:2:2","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"修改 docker 本身的仓库 修改roles/docker/defaults/main.yml文件 docker_ubuntu_repo_base_url: \"http://mirrors.aliyun.com/docker-ce/linux/ubuntu\" docker_ubuntu_repo_gpgkey: \"http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg\" dockerproject_apt_repo_base_url: \"https://mirrors.tuna.tsinghua.edu.cn/docker/apt/repo\" dockerproject_apt_repo_gpgkey: \"https://mirrors.tuna.tsinghua.edu.cn/docker/apt/gpg\" ","date":"2018-12-05","objectID":"/k8s-init/:2:3","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"安装启动 在 kubesray 目录下 ansible-playbook -i inventory/zaihui_cluster/hosts.ini cluster.yml -b -v 由此安装开始… 如果此过程没有任何报错，最后能看到所有节点的状态是success。 那么应该就安装成功了！ 但是这个过程很可能还是会失败，最大的可能性就是上面我们修改的镜像地址有的可能还是拉不到镜像, 这个时候就要根据具体的镜像做修改。 比如 quay.mirrors.ustc.edu.cn/calico/cni:v3.1.3 拉取失败，我们可以去 docker hub 上看是否有相应版本的镜像。修改成 calico/cni:v3.1.3。 ","date":"2018-12-05","objectID":"/k8s-init/:2:4","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"kubernetes dashboard kubespray 安装的时候默认已经安装了 k8s dashboard。我们可以创建 service account 并赋予相应的权限来使用 token 访问dashboard。 比如我们给默认 namespace default 下的 serviceaccount default 分配权限。 RABC 配置文件 apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: default rules: - apiGroups: [\"\", \"extensions\", \"apps\", \"batch\"] resources: [\"*\"] verbs: [\"update\", \"get\", \"watch\", \"list\", \"create\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default-role-binding namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: default subjects: - kind: ServiceAccount name: default namespace: default 获取token kubectl -n default describe secret $(kubectl -n default get secret | grep default | awk '{print $1}') 使用 nginx 反向代理 api-server 示例配置 server { server_name location / { proxy_set_header Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass https://172.31.xx.xxx:6443; client_max_body_size 200m; } } ","date":"2018-12-05","objectID":"/k8s-init/:2:5","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"直接进入正题，这篇文章讲述生产环境如何搭建 kubernets cluster。 我们使用 kubespray 使用搭建 k8s cluster。 至于其他搭建工具 kops, kubeadm 感兴趣的大家可以自己研究一下，各有优势。 ","date":"2018-12-05","objectID":"/k8s-init/:0:0","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"环境: aws(cn-north-1) ubuntu 16.04 kubernetes v1.11.3 kubespray v2.7.0 ","date":"2018-12-05","objectID":"/k8s-init/:1:0","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"搭建 -\u003e ","date":"2018-12-05","objectID":"/k8s-init/:2:0","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"环境准备 准备实例 我们先准备5个实例，其中3个作为 master, 2个作为 node 分发公钥 选定其中一个作为我们的操作实例，因为后续操作都是通过 ssh 与各个节点交互, 先在操作节点上 ssh-keygen 生成公私钥并且把公钥id_rsa.pub分发到其他的节点上 kubespray环境 下载 kubespray wget https://github.com/kubernetes-incubator/kubespray/releases/tag/v2.7.0 解压并且按照依赖 tar xzvf v2.7.0.tar.gz \u0026\u0026 cd kubespray-2.7.0 pip install -r requirements.txt 生成清单 清单目录 cp -r inventory/sample inventory/my_cluster 声明实例地址 declare -a IPS=(172.31.xx.1 172.31.xx.2 172.31.xx.3 172.31.xx.4 172.31.xx.5) 生成目标机器的地址配置 CONFIG_FILE=inventory/my_cluster/hosts.ini python3 contrib/inventory_builder/inventory.py ${IPS[@]} ","date":"2018-12-05","objectID":"/k8s-init/:2:1","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"镜像地址修改 直接安装是无法成功的，因为我们在 cn 很多k8s需要的镜像无法直接 pull 下来。这一步我们就必须把镜像地址改成我们能拉倒的地址 主要是两类镜像 gco.io 和 quay.io，这两类镜像地址必须修改。 find . -name '*.yml' | xargs -n1 -I{} sed -i 's/quay\\.io/quay\\.mirrors\\.ustc\\.edu\\.cn/' {}` find . -name '*.yml' | xargs -n1 -I{} sed -i 's/gcr\\.io\\/google-containers\\//anjia0532\\/google-containers\\./' {}` find . -name '*.yml' | xargs -n1 -I{} sed -i 's/gcr\\.io\\/google_containers\\//anjia0532\\/google-containers\\./' {}` ","date":"2018-12-05","objectID":"/k8s-init/:2:2","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"修改 docker 本身的仓库 修改roles/docker/defaults/main.yml文件 docker_ubuntu_repo_base_url: \"http://mirrors.aliyun.com/docker-ce/linux/ubuntu\" docker_ubuntu_repo_gpgkey: \"http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg\" dockerproject_apt_repo_base_url: \"https://mirrors.tuna.tsinghua.edu.cn/docker/apt/repo\" dockerproject_apt_repo_gpgkey: \"https://mirrors.tuna.tsinghua.edu.cn/docker/apt/gpg\" ","date":"2018-12-05","objectID":"/k8s-init/:2:3","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"安装启动 在 kubesray 目录下 ansible-playbook -i inventory/zaihui_cluster/hosts.ini cluster.yml -b -v 由此安装开始… 如果此过程没有任何报错，最后能看到所有节点的状态是success。 那么应该就安装成功了！ 但是这个过程很可能还是会失败，最大的可能性就是上面我们修改的镜像地址有的可能还是拉不到镜像, 这个时候就要根据具体的镜像做修改。 比如 quay.mirrors.ustc.edu.cn/calico/cni:v3.1.3 拉取失败，我们可以去 docker hub 上看是否有相应版本的镜像。修改成 calico/cni:v3.1.3。 ","date":"2018-12-05","objectID":"/k8s-init/:2:4","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"kubernetes dashboard kubespray 安装的时候默认已经安装了 k8s dashboard。我们可以创建 service account 并赋予相应的权限来使用 token 访问dashboard。 比如我们给默认 namespace default 下的 serviceaccount default 分配权限。 RABC 配置文件 apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: default rules: - apiGroups: [\"\", \"extensions\", \"apps\", \"batch\"] resources: [\"*\"] verbs: [\"update\", \"get\", \"watch\", \"list\", \"create\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: default-role-binding namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: default subjects: - kind: ServiceAccount name: default namespace: default 获取token kubectl -n default describe secret $(kubectl -n default get secret | grep default | awk '{print $1}') 使用 nginx 反向代理 api-server 示例配置 server { server_name location / { proxy_set_header Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass https://172.31.xx.xxx:6443; client_max_body_size 200m; } } ","date":"2018-12-05","objectID":"/k8s-init/:2:5","tags":["kubernetes","容器","架构"],"title":"生产环境搭建 kubernetes(11.3)","uri":"/k8s-init/"},{"categories":null,"content":"背景 Celery 是一个强大的分布式异步任务处理和\b\b调度框架。基本上 Python 项目的异步任务，定时任务首先处理框架就是 Celery. 正因为 \bCelery 的处理时异步的并且是分布式的，当任务出现问题时追踪和调查就不是很容易。官方提供了 flower 可以用来查看任务的执行情况和执行时间。flower 带来的任务监控能力十分有限，最主要的是没有全局性的统计功能，只能查看单个任务；\b再者支持\b查找的时间段很短。\b随着我们异步任务量的和重要性的增加，增强对 celery 的监控\b变得十分必要。 ","date":"2018-11-21","objectID":"/celery-monitor/:1:0","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"监控方案 ","date":"2018-11-21","objectID":"/celery-monitor/:2:0","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"方案1 exporter + prometheus + grafana 优势 这种方案的\b好处是可以利用我们现有的基础设施 prometheus + grafana 劣势 没有\b可供\b部署的 exporter 使用，需要自己开发针对 celery 的exporter, 成本很高 ","date":"2018-11-21","objectID":"/celery-monitor/:2:1","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"方案2 celery signal + graphite + grafana 优势 \b 指标采集比较角度，利用 celery 提供的 hook就能\b\b做到 劣势 \b需要部署新的组件 \bgraphite, 具有一定的学习成本 经过比较，最终我选择方案2来完成。 ","date":"2018-11-21","objectID":"/celery-monitor/:2:2","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"\b原理介绍 Graphite Graphite是一个开源实时的、显示时间序列度量数据的图形系统。Graphite并不收集度量数据本身，而是像一个数据库，通过其后端接收度量数据，然后以实时方式查询、转换、组合这些度量数据。Graphite支持内建的Web界面，它允许用户浏览度量数据和图。 \b三个\b主要模块 Carbon (监控数据的 Twisted 守护进程) Carbon是基于Twisted实现,是Graphite的后端实现。 Carbon的主要作用，是接收被监控节点的连接，收集各个指标的数据，将这些数据写入缓存并最终持久化到whisper存储文件中去。Carbonr能保证Graphite web 绘制出实时接到的指标更新，其原理也很简单位，有点类似lucence，carbon接收到的数据会先存在缓存中，然后再一起写入whisper的硬盘存储。Graphite web通过向carbon-cache发起请求，会同时查询位于缓存与硬盘中的数据。 Whisper whisper 是一个固定大小的数据库，在设计上类似于RRD(round-robin-database)。它可以为随时间不断变化的数值型数据提供快速，可靠的存储。Whisper还可以把高精度的指标数据转换成低精度的指标数据以满足存储长期的历史数据的需求。比如说把按秒采集的指标转换成按分钟采集的指标，以减少数据量，进行长期存储。 Graphite-web Graphite web是基于Django实现的webapp，其主要功能自然是绘制报表与展示。但界面真的不好看（很少使用） Statsd statsd 其实就是一个监听UDP（默认）或者TCP的守护程序，根据简单的协议收集statsd客户端发送来的数据，聚合之后，定时推送给后端, 我们这里指的就是 graphite。 statsd 有多种语言的客户端实现，这里我们使用 python实现的 statsd 来采集 celery 指标。 下图是我们 celery 监控的架构 ","date":"2018-11-21","objectID":"/celery-monitor/:3:0","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"搭建 \bcelery hook 下面示例代码，\b使用 statsd \b接收 celery hook设置的指标并发送到 graphite #!/usr/bin/env python3 # -*- coding: utf-8 -*- import time import statsd from celery.signals import ( task_failure, task_postrun, task_prerun, task_success, ) from django.conf import settings statsd_conn = statsd.StatsClient( host=settings.GRAPHITE_HOST, port=8125, ) task_time = {} @task_prerun.connect def task_prerun_handler(task_id, task, *args, **kwargs): task_time[task_id] = time.time() statsd_conn.incr('{}.prerun'.format(task.name)) @task_postrun.connect def task_postrun_handler(task_id, task, *args, **kwargs): statsd_conn.incr('{}.postrun'.format(task.name)) delta_seconds = time.time() - task_time[task_id] # \b任务执行时间 statsd_conn.timing('{}.runtime'.format(task.name), int(1000 * delta_seconds)) @task_success.connect def task_success_handler(sender, result, **kwargs): # 成功任务数 statsd_conn.incr('{}.success'.format(sender.name)) @task_failure.connect def task_failure_handler(sender, result, **kwargs): # 失败\b任务数 statsd_conn.incr('{}.failure'.format(sender.name)) graphite \b部署 使用 docker 部署, Dockerfile 在这里 docker run -d\\ --name graphite\\ --restart=always\\ -p 8888:80\\ -p 2003-2004:2003-2004\\ -p 2023-2024:2023-2024\\ -p 8125:8125/udp\\ -p 8126:8126\\ graphiteapp/graphite-statsd 使用 nginx 反向代理 nginx 配置就非常简单了 server { server_name example.com; location / { proxy_set_header Host $host; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://xx.xx.xx.xx:8888; client_max_body_size 200m; } } 如果以上步骤全部完成， 访问 graphite的地址，应该已经能看到收集到的指标。 但就像我们说的还必须有数据可视化，所以下一步就是使用 Grafana \b从 Graphite 获取指标并在\u001d dashboard 上展示 Grafana 配置 添加 Graphite 数据源 dashoboard 配置 这里 dashboard 的配置我们不具体展开，说明一些比较重要的步骤。我们的异步任务目录层级比较多，为了更好的筛选必须配置各层的 Variables 如下图所示 \b指标配置 这里指标的配置比较多样，可以有各种aggregate，比如 min, sum, max, mean, mean90 下面展示一张示例的效果图 ","date":"2018-11-21","objectID":"/celery-monitor/:4:0","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"背景 Celery 是一个强大的分布式异步任务处理和\b\b调度框架。基本上 Python 项目的异步任务，定时任务首先处理框架就是 Celery. 正因为 \bCelery 的处理时异步的并且是分布式的，当任务出现问题时追踪和调查就不是很容易。官方提供了 flower 可以用来查看任务的执行情况和执行时间。flower 带来的任务监控能力十分有限，最主要的是没有全局性的统计功能，只能查看单个任务；\b再者支持\b查找的时间段很短。\b随着我们异步任务量的和重要性的增加，增强对 celery 的监控\b变得十分必要。 ","date":"2018-11-21","objectID":"/celery-monitor/:1:0","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"监控方案 ","date":"2018-11-21","objectID":"/celery-monitor/:2:0","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"方案1 exporter + prometheus + grafana 优势 这种方案的\b好处是可以利用我们现有的基础设施 prometheus + grafana 劣势 没有\b可供\b部署的 exporter 使用，需要自己开发针对 celery 的exporter, 成本很高 ","date":"2018-11-21","objectID":"/celery-monitor/:2:1","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"方案2 celery signal + graphite + grafana 优势 \b 指标采集比较角度，利用 celery 提供的 hook就能\b\b做到 劣势 \b需要部署新的组件 \bgraphite, 具有一定的学习成本 经过比较，最终我选择方案2来完成。 ","date":"2018-11-21","objectID":"/celery-monitor/:2:2","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"\b原理介绍 Graphite Graphite是一个开源实时的、显示时间序列度量数据的图形系统。Graphite并不收集度量数据本身，而是像一个数据库，通过其后端接收度量数据，然后以实时方式查询、转换、组合这些度量数据。Graphite支持内建的Web界面，它允许用户浏览度量数据和图。 \b三个\b主要模块 Carbon (监控数据的 Twisted 守护进程) Carbon是基于Twisted实现,是Graphite的后端实现。 Carbon的主要作用，是接收被监控节点的连接，收集各个指标的数据，将这些数据写入缓存并最终持久化到whisper存储文件中去。Carbonr能保证Graphite web 绘制出实时接到的指标更新，其原理也很简单位，有点类似lucence，carbon接收到的数据会先存在缓存中，然后再一起写入whisper的硬盘存储。Graphite web通过向carbon-cache发起请求，会同时查询位于缓存与硬盘中的数据。 Whisper whisper 是一个固定大小的数据库，在设计上类似于RRD(round-robin-database)。它可以为随时间不断变化的数值型数据提供快速，可靠的存储。Whisper还可以把高精度的指标数据转换成低精度的指标数据以满足存储长期的历史数据的需求。比如说把按秒采集的指标转换成按分钟采集的指标，以减少数据量，进行长期存储。 Graphite-web Graphite web是基于Django实现的webapp，其主要功能自然是绘制报表与展示。但界面真的不好看（很少使用） Statsd statsd 其实就是一个监听UDP（默认）或者TCP的守护程序，根据简单的协议收集statsd客户端发送来的数据，聚合之后，定时推送给后端, 我们这里指的就是 graphite。 statsd 有多种语言的客户端实现，这里我们使用 python实现的 statsd 来采集 celery 指标。 下图是我们 celery 监控的架构 ","date":"2018-11-21","objectID":"/celery-monitor/:3:0","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"搭建 \bcelery hook 下面示例代码，\b使用 statsd \b接收 celery hook设置的指标并发送到 graphite #!/usr/bin/env python3 # -*- coding: utf-8 -*- import time import statsd from celery.signals import ( task_failure, task_postrun, task_prerun, task_success, ) from django.conf import settings statsd_conn = statsd.StatsClient( host=settings.GRAPHITE_HOST, port=8125, ) task_time = {} @task_prerun.connect def task_prerun_handler(task_id, task, *args, **kwargs): task_time[task_id] = time.time() statsd_conn.incr('{}.prerun'.format(task.name)) @task_postrun.connect def task_postrun_handler(task_id, task, *args, **kwargs): statsd_conn.incr('{}.postrun'.format(task.name)) delta_seconds = time.time() - task_time[task_id] # \b任务执行时间 statsd_conn.timing('{}.runtime'.format(task.name), int(1000 * delta_seconds)) @task_success.connect def task_success_handler(sender, result, **kwargs): # 成功任务数 statsd_conn.incr('{}.success'.format(sender.name)) @task_failure.connect def task_failure_handler(sender, result, **kwargs): # 失败\b任务数 statsd_conn.incr('{}.failure'.format(sender.name)) graphite \b部署 使用 docker 部署, Dockerfile 在这里 docker run -d\\ --name graphite\\ --restart=always\\ -p 8888:80\\ -p 2003-2004:2003-2004\\ -p 2023-2024:2023-2024\\ -p 8125:8125/udp\\ -p 8126:8126\\ graphiteapp/graphite-statsd 使用 nginx 反向代理 nginx 配置就非常简单了 server { server_name example.com; location / { proxy_set_header Host $host; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://xx.xx.xx.xx:8888; client_max_body_size 200m; } } 如果以上步骤全部完成， 访问 graphite的地址，应该已经能看到收集到的指标。 但就像我们说的还必须有数据可视化，所以下一步就是使用 Grafana \b从 Graphite 获取指标并在\u001d dashboard 上展示 Grafana 配置 添加 Graphite 数据源 dashoboard 配置 这里 dashboard 的配置我们不具体展开，说明一些比较重要的步骤。我们的异步任务目录层级比较多，为了更好的筛选必须配置各层的 Variables 如下图所示 \b指标配置 这里指标的配置比较多样，可以有各种aggregate，比如 min, sum, max, mean, mean90 下面展示一张示例的效果图 ","date":"2018-11-21","objectID":"/celery-monitor/:4:0","tags":["celery","monitor","graphite","grafana"],"title":"Celery 监控方案落地","uri":"/celery-monitor/"},{"categories":null,"content":"Kubernetes 概述 Kubernetes 是一个跨主机集群的 开源的容器调度平台，它可以自动化应用容器的部署、扩展和操作 , 提供以容器为中心的基础架构。 使用 Kubernetes, 可以快速高效地响应客户需求: 快速、可预测地部署应用程序 拥有即时扩展应用程序的能力 不影响现有业务的情况下，无缝地发布新功能 优化硬件资源，降低成本 ","date":"2018-10-25","objectID":"/k8s-1/:1:0","tags":["kubernetes","container","service"],"title":"Kubernetes 基础概念","uri":"/k8s-1/"},{"categories":null,"content":"Kubernetes 组件 我们先从下面这张图来总体把握 k8s 的架构 ","date":"2018-10-25","objectID":"/k8s-1/:2:0","tags":["kubernetes","container","service"],"title":"Kubernetes 基础概念","uri":"/k8s-1/"},{"categories":null,"content":"Master 组件 Master 组件提供的集群控制。Master 组件对集群做出全局性决策(例如：调度)，以及检测和响应集群事件(副本控制器的replicas字段不满足时, 启动新的副本)。 下面这张图展示了 master 的构成 master 又由下面重要的组件来实现它的功能： kube-apiserver k8s API Server提供了k8s各类资源对象（pod,RC,Service等）的增删改查及watch等HTTP Rest接口，是整个系统的数据总线和数据中心。 kubernetes API Server的功能： 提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)； 提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）; 是资源配额控制的入口； 拥有完备的集群安全机制. kube-controller-manager Controller Manager作为集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。 每个Controller通过API Server提供的接口实时监控整个集群的每个资源对象的当前状态，当发生各种故障导致系统状态发生变化时，会尝试将系统状态修复到“期望状态”。 kube-scheduler Scheduler负责Pod调度。在整个系统中起\"承上启下\"作用，承上：负责接收Controller Manager创建的新的Pod，为其选择一个合适的Node；启下：Node上的kubelet接管Pod的生命周期。 Scheduler： 通过调度算法为待调度Pod列表的每个Pod从Node列表中选择一个最适合的Node，并将信息写入etcd中 kubelet通过API Server监听到kubernetes Scheduler产生的Pod绑定信息，然后获取对应的Pod清单，下载Image，并启动容器。 etcd etcd 是 kubernetes 存放集群状态和配置的地方，这是集群状态同步的关键，所有节点都是从 etcd 中获取集群中其他机器状态的；集群中所有容器的状态也是放在这里的。 ","date":"2018-10-25","objectID":"/k8s-1/:2:1","tags":["kubernetes","container","service"],"title":"Kubernetes 基础概念","uri":"/k8s-1/"},{"categories":null,"content":"Node 组件 Node是集群的工作负载节点，默认情况kubelet会向Master注册自己，一旦Node被纳入集群管理范围，kubelet会定时向Master汇报自身的情报，包括操作系统，Docker版本，机器资源情况等。 如果Node超过指定时间不上报信息，会被Master判断为“失联”，标记为Not Ready，随后Master会触发Pod转移。 下面\b这张图展示了 node 主要构成 node 主要组成： kubelet kubelet是主要的节点代理,它监测已分配给其节点的 Pod(通过 apiserver 或通过本地配置文件)，提供如下功能: 挂载 Pod 所需要的数据卷(Volume)。 下载 Pod 的 secrets。 通过 Docker 运行(或通过 rkt)运行 Pod 的容器。 周期性的对容器生命周期进行探测。 如果需要，通过创建镜像Pod（Mirror Pod)将 Pod 的状态报告回系统的其余部分。 将节点的状态报告回系统的其余部分。 kube-proxy kube-proxy 为 pod 提供代理服务。service是一组pod的服务抽象，相当于一组pod的LB，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP。 kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。 用\b下面这张图来展示 kube-proxy在 k8s 中的作用 docker \b\b容器的创建, 运行和管理 ","date":"2018-10-25","objectID":"/k8s-1/:2:2","tags":["kubernetes","container","service"],"title":"Kubernetes 基础概念","uri":"/k8s-1/"},{"categories":null,"content":"Kubernetes 基本的服务部署 我们先用下面这张图来展示一个部署在 k8s 中的服务的逻辑组成 Pod Pod 是Kubernetes的基本操作单元，也是应用运行的载体。整个Kubernetes系统都是围绕着Pod展开的，比如如何部署运行Pod、如何保证Pod的数量、如何访问Pod等。另外，Pod是一个或多个机关容器的集合，这可以说是一大创新点，提供了一种容器的组合的模型。 在Docker中，容器是最小的处理单元，增删改查的对象是容器，容器是一种虚拟化技术，容器之间是隔离的，隔离是基于Linux Namespace实现的。而在Kubernetes中，Pod包含一个或者多个相关的容器，Pod可以认为是容器的一种延伸扩展，一个Pod也是一个隔离体，而Pod内部包含的一组容器又是共享的（包括PID、Network、IPC、UTS）。除此之外，Pod中的容器可以访问共同的数据卷来实现文件系统的共享。 RelicaSet 在说 \bReplicaSet 之前我们先要介绍一下 Replication Controller(RC). RC 是Kubernetes中的另一个核心概念，应用托管在Kubernetes之后，Kubernetes需要保证应用能够持续运行，这是RC的工作内容，它会确保任何时间Kubernetes中都有指定数量的Pod在运行。在此基础上，RC还提供了一些更高级的特性，比如滚动升级、升级回滚等。 RC与Pod的关联是通过Label来实现的。Label机制是Kubernetes中的一个重要设计，通过Label进行对象的弱关联，可以灵活地进行分类和选择。对于Pod，需要设置其自身的Label来进行标识，Label是一系列的Key/value对，在Pod–\u003emetadata–\u003elabeks中进行设置。 RelicaSet 是 \bRC的升级\b，它与当前RC的唯一区别在于Replica Set支持基于集合的Label Selector(Set-based selector)，而旧版本RC只支持基于等式的Label Selector(equality-based selector)。 Kubernetes1.2以上版本通过Deployment来维护Replica Set而不是单独使用Replica Set。即控制流为：Delpoyment→Replica Set→Pod。即新版本的Deployment+Replica Set替代了RC的作用。 Deployment Kubernetes提供了一种更加简单的更新RC和Pod的机制，叫做Deployment。通过在Deployment中描述你所期望的集群状态，Deployment Controller会将现在的集群状态在一个可控的速度下逐步更新成你所期望的集群状态。Deployment主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller完全一样，可以看做新一代的Replication Controller。但是，它又具备了Replication Controller之外的新特性： Replication Controller全部功能 Deployment继承了上面描述的Replication Controller全部功能。 事件和状态查看 可以查看Deployment的升级详细进度和状态。 回滚 当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。 版本记录 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。 暂停和启动 对于每一次升级，都能够随时暂停和启动。 多种升级方案 Recreate—-删除所有已存在的pod,重新创建新的; RollingUpdate—-滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。 Service 在Kubernetes中，在受到RC调控的时候，Pod副本是变化的，对于的虚拟IP也是变化的，比如发生迁移或者伸缩的时候。这对于Pod的访问者来说是不可接受的。Kubernetes中的Service是一种抽象概念，它定义了一个Pod逻辑集合以及访问它们的策略，Service同Pod的关联同样是居于Label来完成的。Service的目标是提供一种桥梁， 它会为访问者提供一个固定访问地址，用于在访问时重定向到相应的后端，这使得非 Kubernetes原生应用程序，在无须为Kubemces编写特定代码的前提下，轻松访问后端。 Service同RC一样，都是通过Label来关联Pod的。当你在Service的yaml文件中定义了该Service的selector中的label为app:my-web，那么这个Service会将Pod–\u003emetadata–\u003elabeks中label为app:my-web的Pod作为分发请求的后端。当Pod发生变化时（增加、减少、重建等），Service会及时更新。这样一来，Service就可以作为Pod的访问入口，起到代理服务器的作用，而对于访问者来说，通过Service进行访问，无需直接感知Pod。 需要注意的是，Kubernetes分配给Service的固定IP是一个虚拟IP，并不是一个真实的IP，在外部是无法寻址的。真实的系统实现上，Kubernetes是通过Kube-proxy组件来实现的虚拟IP路由及转发。所以在之前集群部署的环节上，我们在每个Node上均部署了Proxy这个组件，从而实现了Kubernetes层级的虚拟转发网络。 我们用下面这张图来说明 service 的作用 ","date":"2018-10-25","objectID":"/k8s-1/:3:0","tags":["kubernetes","container","service"],"title":"Kubernetes 基础概念","uri":"/k8s-1/"},{"categories":null,"content":"Kubernetes 概述 Kubernetes 是一个跨主机集群的 开源的容器调度平台，它可以自动化应用容器的部署、扩展和操作 , 提供以容器为中心的基础架构。 使用 Kubernetes, 可以快速高效地响应客户需求: 快速、可预测地部署应用程序 拥有即时扩展应用程序的能力 不影响现有业务的情况下，无缝地发布新功能 优化硬件资源，降低成本 ","date":"2018-10-25","objectID":"/k8s-1/:1:0","tags":["kubernetes","container","service"],"title":"Kubernetes 基础概念","uri":"/k8s-1/"},{"categories":null,"content":"Kubernetes 组件 我们先从下面这张图来总体把握 k8s 的架构 ","date":"2018-10-25","objectID":"/k8s-1/:2:0","tags":["kubernetes","container","service"],"title":"Kubernetes 基础概念","uri":"/k8s-1/"},{"categories":null,"content":"Master 组件 Master 组件提供的集群控制。Master 组件对集群做出全局性决策(例如：调度)，以及检测和响应集群事件(副本控制器的replicas字段不满足时, 启动新的副本)。 下面这张图展示了 master 的构成 master 又由下面重要的组件来实现它的功能： kube-apiserver k8s API Server提供了k8s各类资源对象（pod,RC,Service等）的增删改查及watch等HTTP Rest接口，是整个系统的数据总线和数据中心。 kubernetes API Server的功能： 提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)； 提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）; 是资源配额控制的入口； 拥有完备的集群安全机制. kube-controller-manager Controller Manager作为集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。 每个Controller通过API Server提供的接口实时监控整个集群的每个资源对象的当前状态，当发生各种故障导致系统状态发生变化时，会尝试将系统状态修复到“期望状态”。 kube-scheduler Scheduler负责Pod调度。在整个系统中起\"承上启下\"作用，承上：负责接收Controller Manager创建的新的Pod，为其选择一个合适的Node；启下：Node上的kubelet接管Pod的生命周期。 Scheduler： 通过调度算法为待调度Pod列表的每个Pod从Node列表中选择一个最适合的Node，并将信息写入etcd中 kubelet通过API Server监听到kubernetes Scheduler产生的Pod绑定信息，然后获取对应的Pod清单，下载Image，并启动容器。 etcd etcd 是 kubernetes 存放集群状态和配置的地方，这是集群状态同步的关键，所有节点都是从 etcd 中获取集群中其他机器状态的；集群中所有容器的状态也是放在这里的。 ","date":"2018-10-25","objectID":"/k8s-1/:2:1","tags":["kubernetes","container","service"],"title":"Kubernetes 基础概念","uri":"/k8s-1/"},{"categories":null,"content":"Node 组件 Node是集群的工作负载节点，默认情况kubelet会向Master注册自己，一旦Node被纳入集群管理范围，kubelet会定时向Master汇报自身的情报，包括操作系统，Docker版本，机器资源情况等。 如果Node超过指定时间不上报信息，会被Master判断为“失联”，标记为Not Ready，随后Master会触发Pod转移。 下面\b这张图展示了 node 主要构成 node 主要组成： kubelet kubelet是主要的节点代理,它监测已分配给其节点的 Pod(通过 apiserver 或通过本地配置文件)，提供如下功能: 挂载 Pod 所需要的数据卷(Volume)。 下载 Pod 的 secrets。 通过 Docker 运行(或通过 rkt)运行 Pod 的容器。 周期性的对容器生命周期进行探测。 如果需要，通过创建镜像Pod（Mirror Pod)将 Pod 的状态报告回系统的其余部分。 将节点的状态报告回系统的其余部分。 kube-proxy kube-proxy 为 pod 提供代理服务。service是一组pod的服务抽象，相当于一组pod的LB，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP。 kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。 用\b下面这张图来展示 kube-proxy在 k8s 中的作用 docker \b\b容器的创建, 运行和管理 ","date":"2018-10-25","objectID":"/k8s-1/:2:2","tags":["kubernetes","container","service"],"title":"Kubernetes 基础概念","uri":"/k8s-1/"},{"categories":null,"content":"Kubernetes 基本的服务部署 我们先用下面这张图来展示一个部署在 k8s 中的服务的逻辑组成 Pod Pod 是Kubernetes的基本操作单元，也是应用运行的载体。整个Kubernetes系统都是围绕着Pod展开的，比如如何部署运行Pod、如何保证Pod的数量、如何访问Pod等。另外，Pod是一个或多个机关容器的集合，这可以说是一大创新点，提供了一种容器的组合的模型。 在Docker中，容器是最小的处理单元，增删改查的对象是容器，容器是一种虚拟化技术，容器之间是隔离的，隔离是基于Linux Namespace实现的。而在Kubernetes中，Pod包含一个或者多个相关的容器，Pod可以认为是容器的一种延伸扩展，一个Pod也是一个隔离体，而Pod内部包含的一组容器又是共享的（包括PID、Network、IPC、UTS）。除此之外，Pod中的容器可以访问共同的数据卷来实现文件系统的共享。 RelicaSet 在说 \bReplicaSet 之前我们先要介绍一下 Replication Controller(RC). RC 是Kubernetes中的另一个核心概念，应用托管在Kubernetes之后，Kubernetes需要保证应用能够持续运行，这是RC的工作内容，它会确保任何时间Kubernetes中都有指定数量的Pod在运行。在此基础上，RC还提供了一些更高级的特性，比如滚动升级、升级回滚等。 RC与Pod的关联是通过Label来实现的。Label机制是Kubernetes中的一个重要设计，通过Label进行对象的弱关联，可以灵活地进行分类和选择。对于Pod，需要设置其自身的Label来进行标识，Label是一系列的Key/value对，在Pod–\u003emetadata–\u003elabeks中进行设置。 RelicaSet 是 \bRC的升级\b，它与当前RC的唯一区别在于Replica Set支持基于集合的Label Selector(Set-based selector)，而旧版本RC只支持基于等式的Label Selector(equality-based selector)。 Kubernetes1.2以上版本通过Deployment来维护Replica Set而不是单独使用Replica Set。即控制流为：Delpoyment→Replica Set→Pod。即新版本的Deployment+Replica Set替代了RC的作用。 Deployment Kubernetes提供了一种更加简单的更新RC和Pod的机制，叫做Deployment。通过在Deployment中描述你所期望的集群状态，Deployment Controller会将现在的集群状态在一个可控的速度下逐步更新成你所期望的集群状态。Deployment主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller完全一样，可以看做新一代的Replication Controller。但是，它又具备了Replication Controller之外的新特性： Replication Controller全部功能 Deployment继承了上面描述的Replication Controller全部功能。 事件和状态查看 可以查看Deployment的升级详细进度和状态。 回滚 当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。 版本记录 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。 暂停和启动 对于每一次升级，都能够随时暂停和启动。 多种升级方案 Recreate—-删除所有已存在的pod,重新创建新的; RollingUpdate—-滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。 Service 在Kubernetes中，在受到RC调控的时候，Pod副本是变化的，对于的虚拟IP也是变化的，比如发生迁移或者伸缩的时候。这对于Pod的访问者来说是不可接受的。Kubernetes中的Service是一种抽象概念，它定义了一个Pod逻辑集合以及访问它们的策略，Service同Pod的关联同样是居于Label来完成的。Service的目标是提供一种桥梁， 它会为访问者提供一个固定访问地址，用于在访问时重定向到相应的后端，这使得非 Kubernetes原生应用程序，在无须为Kubemces编写特定代码的前提下，轻松访问后端。 Service同RC一样，都是通过Label来关联Pod的。当你在Service的yaml文件中定义了该Service的selector中的label为app:my-web，那么这个Service会将Pod–\u003emetadata–\u003elabeks中label为app:my-web的Pod作为分发请求的后端。当Pod发生变化时（增加、减少、重建等），Service会及时更新。这样一来，Service就可以作为Pod的访问入口，起到代理服务器的作用，而对于访问者来说，通过Service进行访问，无需直接感知Pod。 需要注意的是，Kubernetes分配给Service的固定IP是一个虚拟IP，并不是一个真实的IP，在外部是无法寻址的。真实的系统实现上，Kubernetes是通过Kube-proxy组件来实现的虚拟IP路由及转发。所以在之前集群部署的环节上，我们在每个Node上均部署了Proxy这个组件，从而实现了Kubernetes层级的虚拟转发网络。 我们用下面这张图来说明 service 的作用 ","date":"2018-10-25","objectID":"/k8s-1/:3:0","tags":["kubernetes","container","service"],"title":"Kubernetes 基础概念","uri":"/k8s-1/"},{"categories":null,"content":"如果业务场景不需要强力的事务支持及复杂的join, 数据模型变化频繁，数据需要落地，查询 QPS 超过200。 那么 Mongodb 作为数据库非常合适。 在我们的业务中我们就选用了 mongo 来存储账单，菜单，交易信息等数据。随着 mongodb 在我们的业务场景中应用的地方越来越广，mongo 必须是高可用的。 这里主要介绍一下 Mongodb 高可用方案以及其中的 replicaset(副本集)方案在生产上的搭建。 ","date":"2018-09-19","objectID":"/mongo-replicaset/:0:0","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"高可用方案 ","date":"2018-09-19","objectID":"/mongo-replicaset/:1:0","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"Master-Slave 主从架构 主从架构一般用于备份或者做读写分离。一般有一主一从设计和一主多从设计。 由两种角色构成： 主(Master) 可读可写，当数据有修改的时候，会将oplog同步到所有连接的salve上去。 从(Slave) 只读不可写，自动从Master同步数据。 特别的，对于 Mongodb 来说，并不推荐使用 Master-Slave 架构，因为 Master-Slave 其中 Master 宕机后不能自动恢复 在主从结构中，主节点的操作记录成为 oplog（operation log), oplog 存储在一个系统数据库local的集合oplog.$main中，这个集合的每个文档都代表主节点上执行的一个操作。 从服务器会定期从主服务器中获取 oplog 记录，然后在本机上执行。对于存储 oplog 的集合，MongoDB采用的是固定集合，也就是说随着操作过多，新的操作会覆盖旧的操作。 ","date":"2018-09-19","objectID":"/mongo-replicaset/:1:1","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"ReplicaSet(副本集) Mongodb的 ReplicaSet 即副本集方式主要有两个目的，一个是数据冗余做故障恢复使用，当发生硬件故障或者其它原因造成的宕机时，可以使用副本进行恢复。 另一个是做读写分离，读的请求分流到副本上，减轻主（Primary）的读压力。 Replica Set是mongod的实例集合，它们有着同样的数据内容。包含三类角色： 主节点（Primary） 接收所有的写请求，然后把修改同步到所有Secondary。一个Replica Set只能有一个Primary节点，当Primary挂掉后，其他Secondary或者Arbiter节点会重新选举出来一个主节点。 默认读请求也是发到 Primary 节点处理的，需要转发到 Secondary 需要客户端修改一下连接配置。 副本节点（Secondary） 副本节点同样使用 oplog 进行数据同步来与主节点保持同样的数据集。当主节点挂掉的时候，副本节点参与选主。 仲裁者（Arbiter） 不保有数据，不参与选主，只进行选主投票。使用Arbiter可以减轻数据存储的硬件需求，Arbiter跑起来几乎没什么大的硬件资源需求，但重要的一点是，在生产环境下它和其他数据节点不要部署在同一台机器上。 注意，一个自动failover的 ReplicaSet 节点数必须为奇数，目的是选主投票的时候要有一个大多数才能进行选主决策。 自动故障转移 当主节点与其他节点通信失联的时间超过选举超时时间（默认是10s）, 副本节点会提名自己成为主节点候选者。然后完成选主，集群则完成故障转移。 在故障转移过程中，写操作失败，副本节点仍然能正常的完成读操作。 ","date":"2018-09-19","objectID":"/mongo-replicaset/:1:2","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"Sharding(分片) 当数据量比较大的时候，我们需要把数据分片运行在不同的机器中，以降低CPU、内存和IO的压力，Sharding就是数据库分片。 MongodB 分片技术类似MySQL的水平切分和垂直切分，数据库主要由两种方式做 Sharding：垂直扩展和横向切分。 垂直扩展的方式就是进行集群扩展，添加更多的CPU，内存，磁盘空间等。 横向切分则是通过数据分片的方式，通过集群统一提供服务 Mongodb sharded cluster 架构图如下 Mongodb sharded cluster中的组件包含以下三大部分： shards 用来保存数据，保证数据的高可用性和一致性。可以是一个单独的mongod实例，也可以是一个副本集。 在生产环境下Shard一般是一个Replica Set，以防止该数据片的单点故障。 mongos mongos承担客户端请求路由的作用。客户端直接连接mongos，由mongos把读写请求路由到指定的Shard上去。 一个Sharding集群，可以有一个mongos，也可以有多mongos以减轻客户端请求的压力。 config server 保存集群的元数据（metadata），包含各个Shard的路由规则。 下面这张图是我对 Mongodb 分片架构重点的总结 ","date":"2018-09-19","objectID":"/mongo-replicaset/:1:3","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"ReplicaSet 搭建 这里我们根据自己的业务场景和数据量我们选取的 mongo 高可用架构是 包含arbiter的三节点副本集。 搭建环境是 Ubuntu, Mongodb 版本是4.0 配置国内的 deb 源 echo \"deb [ arch=amd64,arm64 ] http://mirrors.aliyun.com/mongodb/apt/ubuntu xenial/mongodb-org/4.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list 安装 mongo 4.0 sudo apt-get update sudo apt-get install -y mongodb-org=4.0.0 mongodb-org-server=4.0.0 mongodb-org-shell=4.0.0 mongodb-org-mongos=4.0.0 mongodb-org-tools=4.0.0 --allow-unauthenticated 修改配置 /etc/mongod.conf # mongod.conf # for documentation of all options, see: # http://docs.mongodb.org/manual/reference/configuration-options/ # Where and how to store data. storage: dbPath: /var/lib/mongodb # 重点，arbiter节点置为 false, 主副节点置为 true journal: enabled: false # engine: # mmapv1: # wiredTiger: # where to write logging data. systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log # network interfaces net: port: 27017 bindIp: 0.0.0.0 # 重点，副本集的名字 replication: replSetName: waimai_rs #processManagement: security: authorization: enabled keyFile: /home/zaihui/keys/mongodb/mongodb-keyfile 启动 mongo 实例 sudo service mongod start 如果 monod.service not found 使用 sudo systemctl unmask mongodb 重复上面的步骤，启动三个实例 初始化副本集 进入 mongo shell rs.initiate({_id:\"waimai_rs\",members:[ {_id:0,host:\"172.31.41.20:27017\",priority:1}, {_id:1,host:\"172.31.48.196:27017\",priority:0.5}, {_id:2, host:\"172.31.48.227:27017\",arbiterOnly:true} ]}) 在这里，我们把 priority 设置的不一样是为了指明主节点为 priority 最高的那个。 检查副本集\u001d配置 rs.conf() 看到如下配置 { \"_id\" : \"waimai_rs\", \"version\" : 1, \"protocolVersion\" : NumberLong(1), \"writeConcernMajorityJournalDefault\" : true, \"members\" : [ { \"_id\" : 0, \"host\" : \"172.31.41.20:27017\", \"arbiterOnly\" : false, \"buildIndexes\" : true, \"hidden\" : false, \"priority\" : 1, \"tags\" : { }, \"slaveDelay\" : NumberLong(0), \"votes\" : 1 }, .... 查看副本集状态 rs.status() 看到如下输出\b，检查各个节点是否正常 \"members\" : [ { \"_id\" : 0, \"name\" : \"172.31.41.20:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 1215722, \"optime\" : { \"ts\" : Timestamp(1537330095, 1), \"t\" : NumberLong(2) }, \"optimeDurable\" : { \"ts\" : Timestamp(1537330095, 1), \"t\" : NumberLong(2) }, \"optimeDate\" : ISODate(\"2018-09-19T04:08:15Z\"), \"optimeDurableDate\" : ISODate(\"2018-09-19T04:08:15Z\"), \"lastHeartbeat\" : ISODate(\"2018-09-19T04:08:19.593Z\"), \"lastHeartbeatRecv\" : ISODate(\"2018-09-19T04:08:18.644Z\"), \"pingMs\" : NumberLong(1), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"infoMessage\" : \"\", \"electionTime\" : Timestamp(1536114379, 1), \"electionDate\" : ISODate(\"2018-09-05T02:26:19Z\"), \"configVersion\" : 1 }, ... 验证 主从服务器数据是否同步，从服务器没有读写权限 向主节点写入数据 ok 后台自动同步到\b副本节点，副本节点有数据 向副本节点写入数据 false 副本节点不能写 主节点读取数据 ok 副本节点读取数据 false 副本节点不能读 配置副本节点可读 副本节点读取数据 ok 注意，遇到问题时要仔细查看日志信息，能帮助我们快速定位问题。 ","date":"2018-09-19","objectID":"/mongo-replicaset/:2:0","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"如果业务场景不需要强力的事务支持及复杂的join, 数据模型变化频繁，数据需要落地，查询 QPS 超过200。 那么 Mongodb 作为数据库非常合适。 在我们的业务中我们就选用了 mongo 来存储账单，菜单，交易信息等数据。随着 mongodb 在我们的业务场景中应用的地方越来越广，mongo 必须是高可用的。 这里主要介绍一下 Mongodb 高可用方案以及其中的 replicaset(副本集)方案在生产上的搭建。 ","date":"2018-09-19","objectID":"/mongo-replicaset/:0:0","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"高可用方案 ","date":"2018-09-19","objectID":"/mongo-replicaset/:1:0","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"Master-Slave 主从架构 主从架构一般用于备份或者做读写分离。一般有一主一从设计和一主多从设计。 由两种角色构成： 主(Master) 可读可写，当数据有修改的时候，会将oplog同步到所有连接的salve上去。 从(Slave) 只读不可写，自动从Master同步数据。 特别的，对于 Mongodb 来说，并不推荐使用 Master-Slave 架构，因为 Master-Slave 其中 Master 宕机后不能自动恢复 在主从结构中，主节点的操作记录成为 oplog（operation log), oplog 存储在一个系统数据库local的集合oplog.$main中，这个集合的每个文档都代表主节点上执行的一个操作。 从服务器会定期从主服务器中获取 oplog 记录，然后在本机上执行。对于存储 oplog 的集合，MongoDB采用的是固定集合，也就是说随着操作过多，新的操作会覆盖旧的操作。 ","date":"2018-09-19","objectID":"/mongo-replicaset/:1:1","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"ReplicaSet(副本集) Mongodb的 ReplicaSet 即副本集方式主要有两个目的，一个是数据冗余做故障恢复使用，当发生硬件故障或者其它原因造成的宕机时，可以使用副本进行恢复。 另一个是做读写分离，读的请求分流到副本上，减轻主（Primary）的读压力。 Replica Set是mongod的实例集合，它们有着同样的数据内容。包含三类角色： 主节点（Primary） 接收所有的写请求，然后把修改同步到所有Secondary。一个Replica Set只能有一个Primary节点，当Primary挂掉后，其他Secondary或者Arbiter节点会重新选举出来一个主节点。 默认读请求也是发到 Primary 节点处理的，需要转发到 Secondary 需要客户端修改一下连接配置。 副本节点（Secondary） 副本节点同样使用 oplog 进行数据同步来与主节点保持同样的数据集。当主节点挂掉的时候，副本节点参与选主。 仲裁者（Arbiter） 不保有数据，不参与选主，只进行选主投票。使用Arbiter可以减轻数据存储的硬件需求，Arbiter跑起来几乎没什么大的硬件资源需求，但重要的一点是，在生产环境下它和其他数据节点不要部署在同一台机器上。 注意，一个自动failover的 ReplicaSet 节点数必须为奇数，目的是选主投票的时候要有一个大多数才能进行选主决策。 自动故障转移 当主节点与其他节点通信失联的时间超过选举超时时间（默认是10s）, 副本节点会提名自己成为主节点候选者。然后完成选主，集群则完成故障转移。 在故障转移过程中，写操作失败，副本节点仍然能正常的完成读操作。 ","date":"2018-09-19","objectID":"/mongo-replicaset/:1:2","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"Sharding(分片) 当数据量比较大的时候，我们需要把数据分片运行在不同的机器中，以降低CPU、内存和IO的压力，Sharding就是数据库分片。 MongodB 分片技术类似MySQL的水平切分和垂直切分，数据库主要由两种方式做 Sharding：垂直扩展和横向切分。 垂直扩展的方式就是进行集群扩展，添加更多的CPU，内存，磁盘空间等。 横向切分则是通过数据分片的方式，通过集群统一提供服务 Mongodb sharded cluster 架构图如下 Mongodb sharded cluster中的组件包含以下三大部分： shards 用来保存数据，保证数据的高可用性和一致性。可以是一个单独的mongod实例，也可以是一个副本集。 在生产环境下Shard一般是一个Replica Set，以防止该数据片的单点故障。 mongos mongos承担客户端请求路由的作用。客户端直接连接mongos，由mongos把读写请求路由到指定的Shard上去。 一个Sharding集群，可以有一个mongos，也可以有多mongos以减轻客户端请求的压力。 config server 保存集群的元数据（metadata），包含各个Shard的路由规则。 下面这张图是我对 Mongodb 分片架构重点的总结 ","date":"2018-09-19","objectID":"/mongo-replicaset/:1:3","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"ReplicaSet 搭建 这里我们根据自己的业务场景和数据量我们选取的 mongo 高可用架构是 包含arbiter的三节点副本集。 搭建环境是 Ubuntu, Mongodb 版本是4.0 配置国内的 deb 源 echo \"deb [ arch=amd64,arm64 ] http://mirrors.aliyun.com/mongodb/apt/ubuntu xenial/mongodb-org/4.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.0.list 安装 mongo 4.0 sudo apt-get update sudo apt-get install -y mongodb-org=4.0.0 mongodb-org-server=4.0.0 mongodb-org-shell=4.0.0 mongodb-org-mongos=4.0.0 mongodb-org-tools=4.0.0 --allow-unauthenticated 修改配置 /etc/mongod.conf # mongod.conf # for documentation of all options, see: # http://docs.mongodb.org/manual/reference/configuration-options/ # Where and how to store data. storage: dbPath: /var/lib/mongodb # 重点，arbiter节点置为 false, 主副节点置为 true journal: enabled: false # engine: # mmapv1: # wiredTiger: # where to write logging data. systemLog: destination: file logAppend: true path: /var/log/mongodb/mongod.log # network interfaces net: port: 27017 bindIp: 0.0.0.0 # 重点，副本集的名字 replication: replSetName: waimai_rs #processManagement: security: authorization: enabled keyFile: /home/zaihui/keys/mongodb/mongodb-keyfile 启动 mongo 实例 sudo service mongod start 如果 monod.service not found 使用 sudo systemctl unmask mongodb 重复上面的步骤，启动三个实例 初始化副本集 进入 mongo shell rs.initiate({_id:\"waimai_rs\",members:[ {_id:0,host:\"172.31.41.20:27017\",priority:1}, {_id:1,host:\"172.31.48.196:27017\",priority:0.5}, {_id:2, host:\"172.31.48.227:27017\",arbiterOnly:true} ]}) 在这里，我们把 priority 设置的不一样是为了指明主节点为 priority 最高的那个。 检查副本集\u001d配置 rs.conf() 看到如下配置 { \"_id\" : \"waimai_rs\", \"version\" : 1, \"protocolVersion\" : NumberLong(1), \"writeConcernMajorityJournalDefault\" : true, \"members\" : [ { \"_id\" : 0, \"host\" : \"172.31.41.20:27017\", \"arbiterOnly\" : false, \"buildIndexes\" : true, \"hidden\" : false, \"priority\" : 1, \"tags\" : { }, \"slaveDelay\" : NumberLong(0), \"votes\" : 1 }, .... 查看副本集状态 rs.status() 看到如下输出\b，检查各个节点是否正常 \"members\" : [ { \"_id\" : 0, \"name\" : \"172.31.41.20:27017\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 1215722, \"optime\" : { \"ts\" : Timestamp(1537330095, 1), \"t\" : NumberLong(2) }, \"optimeDurable\" : { \"ts\" : Timestamp(1537330095, 1), \"t\" : NumberLong(2) }, \"optimeDate\" : ISODate(\"2018-09-19T04:08:15Z\"), \"optimeDurableDate\" : ISODate(\"2018-09-19T04:08:15Z\"), \"lastHeartbeat\" : ISODate(\"2018-09-19T04:08:19.593Z\"), \"lastHeartbeatRecv\" : ISODate(\"2018-09-19T04:08:18.644Z\"), \"pingMs\" : NumberLong(1), \"lastHeartbeatMessage\" : \"\", \"syncingTo\" : \"\", \"syncSourceHost\" : \"\", \"syncSourceId\" : -1, \"infoMessage\" : \"\", \"electionTime\" : Timestamp(1536114379, 1), \"electionDate\" : ISODate(\"2018-09-05T02:26:19Z\"), \"configVersion\" : 1 }, ... 验证 主从服务器数据是否同步，从服务器没有读写权限 向主节点写入数据 ok 后台自动同步到\b副本节点，副本节点有数据 向副本节点写入数据 false 副本节点不能写 主节点读取数据 ok 副本节点读取数据 false 副本节点不能读 配置副本节点可读 副本节点读取数据 ok 注意，遇到问题时要仔细查看日志信息，能帮助我们快速定位问题。 ","date":"2018-09-19","objectID":"/mongo-replicaset/:2:0","tags":["Mongodb","ha","数据库"],"title":"Mongodb 高可用方案及副本集搭建","uri":"/mongo-replicaset/"},{"categories":null,"content":"注意：Jaeger是德语单词，猎人的意思，读做“耶格” ","date":"2018-08-15","objectID":"/jaeger/:0:0","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"链路追踪背景 微服务的流行程度不需要我们多说，随着业务的扩张和规模的扩大，单体架构的支撑能力越来越有限。所以服务拆分成了必须的选择，而随着服务的增多，以前单体应用里的函数调用都变成了服务之间的请求与调用。随之而来的就是运维和问题定位难度也会变的很大。所以，我们就需要一个工具来帮助我们排查系统性能瓶颈和定位问题, 称他为Tracing吧，tracing能记录每次调用的过程和耗时。 ","date":"2018-08-15","objectID":"/jaeger/:0:1","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"Tracing简介 Tracing 在90年代就已经出现了，真正的老大是Google的 Dapper.随后出现了不少比较不错的 tracing 软件。比如StackDriver Trace (Google),Zipkin(twitter),鹰眼(taobao) 等等。 一般 tracing 系统核心组成都有：代码打点；数据发送；数据存储；数据查询展示。 在数据采集过程中，需要在代码中打点，并且不同系统的 API 并不兼容，这就导致了如果希望切换追踪系统，往往会带来较大改动成本。 ","date":"2018-08-15","objectID":"/jaeger/:0:2","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"Opentracing 为了解决不同的分布式追踪系统 API 不兼容的问题，诞生了 OpenTracing 规范。OpenTracing 是一个轻量级的标准化层，它位于应用程序/类库和追踪或日志分析程序之间。 opentracing 优点 opentracing 进入了CNCF，为分布式追踪，提供统一的概念和数据标准。 opentracing 通过提供平台无关、厂商无关的 API，使得开发人员能够方便的添加（或更换）追踪系统的实现。 opentracing 数据定义 两个核心组成 trace 和 span trace 是一次调用的统称（一条调用链），经过的各个服务生成一个 span, 多个 span 组成一个 trace。 span 与 span形成链式关系。 下图展示了两者的关系 span组成部分 An operation name，操作名称 A start timestamp，起始时间 A finish timestamp，结束时间 Span Tag，一组键值对构成的Span标签集合。键值对中，键必须为string，值可以是字符串，布尔，或者数字类型。 Span Log，一组span的日志集合。 每次log操作包含一个键值对，以及一个时间戳。 键值对中，键必须为string，值可以是任意类型。 但是需要注意，不是所有的支持OpenTracing的Tracer,都需要支持所有的值类型。 SpanContext，Span上下文对象 (下面会详细说明) References(Span间关系)，相关的零个或者多个Span（Span间通过SpanContext建立这种关系） 每一个SpanContext包含以下状态： 任何一个OpenTracing的实现，都需要将当前调用链的状态（例如：trace和span的id），依赖一个独特的Span去跨进程边界传输 Baggage Items，Trace的随行数据，是一个键值对集合，它存在于trace中，也需要跨进程边界传输 关于 OpenTracing 更多语义，请参考 OpenTracing语义标准。 ","date":"2018-08-15","objectID":"/jaeger/:0:3","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"Jaeger架构 在 OpenTracing的实现中，Zipkin和 Jaeger是比较留下的方案。 在 \bJaeger 和 Zipkin的对比中，我认为Jaeger的优势在： 更加cloud native（docker环境搭建更加方便，\b对\bkubernetes支持的更好） 支持的客户端更多，并且我觉得代码（python客户端）易读和\b清晰 组成架构更加\b科学（我喜欢） Jaeger 主要由以下几部分组成。 Jaeger Client - 为不同语言实现了符合 OpenTracing 标准的 SDK。应用程序通过 API 写入数据，client library 把 trace 信息按照应用程序指定的采样策略传递给 jaeger-agent。 Agent - 它是一个监听在 UDP 端口上接收 span 数据的网络守护进程，它会将数据批量发送给 collector。它被设计成一个基础组件，部署到所有的宿主机上。Agent 将 client library 和 collector 解耦，为 client library 屏蔽了路由和发现 collector 的细节。 Collector - 接收 jaeger-agent 发送来的数据，然后将数据写入后端存储。Collector 被设计成无状态的组件，因此您可以同时运行任意数量的 jaeger-collector。 Data Store - 后端存储被设计成一个可插拔的组件，支持将数据写入 cassandra、elastic search。 Query - 接收查询请求，然后从后端存储系统中检索 trace 并通过 UI 进行展示。Query 是无状态的，您可以启动多个实例，把它们部署在 nginx 这样的负载均衡器后面。 下图是 \bJaeger官方文档的架构图 ","date":"2018-08-15","objectID":"/jaeger/:0:4","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"Jaeger搭建 本地测试 我们使用官方的 all-in-one image就可以\b运行一个完整的链路追踪系统。\b这种方式数据存在内存中，仅供我们用来本地开发和测试。 运行方式 docker run -d --name jaeger \\ -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\ -p 5775:5775/udp \\ -p 6831:6831/udp \\ -p 6832:6832/udp \\ -p 5778:5778 \\ -p 16686:16686 \\ -p 14268:14268 \\ -p 9411:9411 \\ jaegertracing/all-in-one:latest 访问 http://localhost:16686就能看到 jaeger的数据查询页 正式环境搭建 Jaeger\b目前支持的后代存储有 Cassandra 和 Elasticsearch, 因为我们已经有\b搭建好的 ES， 所以\b自然存储选择使用 ES. agent 运行方式 ``` version: \"3\" services: jaeger-agent: image: jaegertracing/jaeger-agent hostname: jaeger-agent command: [\"--collector.host-port=collector-host:14267\"] ports: - \"5775:5775/udp\" - \"6831:6831/udp\" - \"6832:6832/udp\" - \"5778:5778\" networks: - default restart: on-failure environment: - SPAN_STORAGE_TYPE=elasticsearch ``` collector 和 query 可以搭建在同一个实例上,运行方式 version: \"3\" services: jaeger-collector: image: jaegertracing/jaeger-collector ports: - \"14269:14269\" - \"14268:14268\" - \"14267:14267\" - \"9411:9411\" networks: - default restart: on-failure environment: - SPAN_STORAGE_TYPE=elasticsearch command: [ \"--es.server-urls=http:es-host:9200\", \"--log-level=debug\" ] #depends_on: # - elasticsearch jaeger-query: image: jaegertracing/jaeger-query environment: - SPAN_STORAGE_TYPE=elasticsearch - no_proxy=localhost ports: - \"16686:16686\" - \"16687:16687\" networks: - default restart: on-failure command: [ \"--es.server-urls=http://es-host:9200\", \"--span-storage.type=elasticsearch\", \"--log-level=debug\", #\"--query.static-files=/go/jaeger-ui/\" ] depends_on: - jaeger-collector networks: elastic-jaeger: driver: bridge 数据简单展示图例 链路依赖关系图 ","date":"2018-08-15","objectID":"/jaeger/:0:5","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"Django接入 我们开发了自己的 jaeger-python 包（huipy），可以非常简单的在 \bDjango 项目中使用。 接入方式 在中间件中引入 MIDDLEWARE = [ 'huipy.tracer.middleware.TraceMiddleware', # 其他中间件 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'corsheaders.middleware.CorsMiddleware', 'django.middleware.common.CommonMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] settings.SERVICE_NAME = 'atlas' # 其他配置 ... \b在发送请求时\u001d引入 from huipy.tracer.httpclient import HttpClient HttpClient(url='http://httpbin.org/get').get() ","date":"2018-08-15","objectID":"/jaeger/:0:6","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"线上部署问题 我们使用 uWSGI 作为 Django app 的容器, 默认的启动模式是 preforking \b在 uWSGI启动的时候，首先主进程会初始化并且load app, 然后会fork出指定数目的子进程。 使用fork函数得到的子进程从父进程的继承了整个进程的地址空间，包括：进程上下文、进程堆栈、内存信息、打开的文件描述符、信号控制设置、进程优先级、进程组号、当前工作目录、根目录、资源限制、控制终端等。 这里提一下linux fork 使用的机制是 copy-on-write(inux系统为了提高系统性能和资源利用率，for出一个新进程时，系统并没有真正复制一个副本。如果多个进程要读取它们自己的那部分资源的副本，那么复制是不必要的。每个进程只要保存一个指向这个资源的指针就可以了。如果一个进程要修改自己的那份资源的“副本”，那么就会复制那份资源) 在绝大多数场景下这种方式不会有问题， 但是当主进程本身是多线程的时候可能就会造成问题。 我们的 tracer初始化后会启动一个后台线程向agent 发送udp数据包，而这个过程在主进程\bload app的时候就完成了。fork子进程的时候这个后台线程当然是不会被fork的， 所以当子进程真正处理请求时，没有后台线程来发送数据。早造成的后果就是我们始终看不到我们\b请求的trace. 我们可以通过查看特定进程的系统调用来查看到信息： 首先是 preforking模式，我们查看某一个 uWSGI进程的调用情况 # mac上使用 dtruss, linux使用 strace sudo dtruss -p 1310 # 输出 SYSCALL(args) = return lazy-apps模式 sudo dtruss -p 1509 # 输出 SYSCALL(args) = return gettimeofday(0x7000050F58E8, 0x0, 0x0) = 0 0 psynch_cvwait(0x105FABF80, 0xC2901000C2A00, 0x5B700) = -1 Err#316 gettimeofday(0x7000050F58E8, 0x0, 0x0) = 0 0 psynch_cvwait(0x105FABF80, 0xC2A01000C2B00, 0x5B700) = -1 Err#316 gettimeofday(0x7000050F58E8, 0x0, 0x0) = 0 0 psynch_cvwait(0x105FABF80, 0xC2B01000C2C00, 0x5B700) = -1 Err#316 gettimeofday(0x7000050F58E8, 0x0, 0x0) = 0 0 psynch_cvwait(0x105FABF80, 0xC2C01000C2D00, 0x5B700) = -1 Err#316 gettimeofday(0x7000050F58E8, 0x0, 0x0) = 0 0 很明显\b在 lazy-apps模式下一直有线程在监听事件，而前者没有这样的线程 解决方案 lazy-apps uWSGI可以使用 lazy-apps模式启动，在主进程fork\b子进程后，每个子进程再初始化和load app。 这样可以保证每个进程独立启动，\b保证了更好的的隔离性。\b在我们的场景中这样每个子进程会启动自己的后台线程。 这个方案的缺点\b是： 启动时间会稍微变长，但是有copy-on-write其实影响不大 占用的内存的会变多 延迟初始化tracer 重构我们的实现，在 middleware执行到 process_request的时候再进行全局的初始化 def process_request(self, request): from huipy.tracer.initial_tracer import initialize_global_tracer self._tracer = initialize_global_tracer() ... ","date":"2018-08-15","objectID":"/jaeger/:0:7","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"注意：Jaeger是德语单词，猎人的意思，读做“耶格” ","date":"2018-08-15","objectID":"/jaeger/:0:0","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"链路追踪背景 微服务的流行程度不需要我们多说，随着业务的扩张和规模的扩大，单体架构的支撑能力越来越有限。所以服务拆分成了必须的选择，而随着服务的增多，以前单体应用里的函数调用都变成了服务之间的请求与调用。随之而来的就是运维和问题定位难度也会变的很大。所以，我们就需要一个工具来帮助我们排查系统性能瓶颈和定位问题, 称他为Tracing吧，tracing能记录每次调用的过程和耗时。 ","date":"2018-08-15","objectID":"/jaeger/:0:1","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"Tracing简介 Tracing 在90年代就已经出现了，真正的老大是Google的 Dapper.随后出现了不少比较不错的 tracing 软件。比如StackDriver Trace (Google),Zipkin(twitter),鹰眼(taobao) 等等。 一般 tracing 系统核心组成都有：代码打点；数据发送；数据存储；数据查询展示。 在数据采集过程中，需要在代码中打点，并且不同系统的 API 并不兼容，这就导致了如果希望切换追踪系统，往往会带来较大改动成本。 ","date":"2018-08-15","objectID":"/jaeger/:0:2","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"Opentracing 为了解决不同的分布式追踪系统 API 不兼容的问题，诞生了 OpenTracing 规范。OpenTracing 是一个轻量级的标准化层，它位于应用程序/类库和追踪或日志分析程序之间。 opentracing 优点 opentracing 进入了CNCF，为分布式追踪，提供统一的概念和数据标准。 opentracing 通过提供平台无关、厂商无关的 API，使得开发人员能够方便的添加（或更换）追踪系统的实现。 opentracing 数据定义 两个核心组成 trace 和 span trace 是一次调用的统称（一条调用链），经过的各个服务生成一个 span, 多个 span 组成一个 trace。 span 与 span形成链式关系。 下图展示了两者的关系 span组成部分 An operation name，操作名称 A start timestamp，起始时间 A finish timestamp，结束时间 Span Tag，一组键值对构成的Span标签集合。键值对中，键必须为string，值可以是字符串，布尔，或者数字类型。 Span Log，一组span的日志集合。 每次log操作包含一个键值对，以及一个时间戳。 键值对中，键必须为string，值可以是任意类型。 但是需要注意，不是所有的支持OpenTracing的Tracer,都需要支持所有的值类型。 SpanContext，Span上下文对象 (下面会详细说明) References(Span间关系)，相关的零个或者多个Span（Span间通过SpanContext建立这种关系） 每一个SpanContext包含以下状态： 任何一个OpenTracing的实现，都需要将当前调用链的状态（例如：trace和span的id），依赖一个独特的Span去跨进程边界传输 Baggage Items，Trace的随行数据，是一个键值对集合，它存在于trace中，也需要跨进程边界传输 关于 OpenTracing 更多语义，请参考 OpenTracing语义标准。 ","date":"2018-08-15","objectID":"/jaeger/:0:3","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"Jaeger架构 在 OpenTracing的实现中，Zipkin和 Jaeger是比较留下的方案。 在 \bJaeger 和 Zipkin的对比中，我认为Jaeger的优势在： 更加cloud native（docker环境搭建更加方便，\b对\bkubernetes支持的更好） 支持的客户端更多，并且我觉得代码（python客户端）易读和\b清晰 组成架构更加\b科学（我喜欢） Jaeger 主要由以下几部分组成。 Jaeger Client - 为不同语言实现了符合 OpenTracing 标准的 SDK。应用程序通过 API 写入数据，client library 把 trace 信息按照应用程序指定的采样策略传递给 jaeger-agent。 Agent - 它是一个监听在 UDP 端口上接收 span 数据的网络守护进程，它会将数据批量发送给 collector。它被设计成一个基础组件，部署到所有的宿主机上。Agent 将 client library 和 collector 解耦，为 client library 屏蔽了路由和发现 collector 的细节。 Collector - 接收 jaeger-agent 发送来的数据，然后将数据写入后端存储。Collector 被设计成无状态的组件，因此您可以同时运行任意数量的 jaeger-collector。 Data Store - 后端存储被设计成一个可插拔的组件，支持将数据写入 cassandra、elastic search。 Query - 接收查询请求，然后从后端存储系统中检索 trace 并通过 UI 进行展示。Query 是无状态的，您可以启动多个实例，把它们部署在 nginx 这样的负载均衡器后面。 下图是 \bJaeger官方文档的架构图 ","date":"2018-08-15","objectID":"/jaeger/:0:4","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"Jaeger搭建 本地测试 我们使用官方的 all-in-one image就可以\b运行一个完整的链路追踪系统。\b这种方式数据存在内存中，仅供我们用来本地开发和测试。 运行方式 docker run -d --name jaeger \\ -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\ -p 5775:5775/udp \\ -p 6831:6831/udp \\ -p 6832:6832/udp \\ -p 5778:5778 \\ -p 16686:16686 \\ -p 14268:14268 \\ -p 9411:9411 \\ jaegertracing/all-in-one:latest 访问 http://localhost:16686就能看到 jaeger的数据查询页 正式环境搭建 Jaeger\b目前支持的后代存储有 Cassandra 和 Elasticsearch, 因为我们已经有\b搭建好的 ES， 所以\b自然存储选择使用 ES. agent 运行方式 ``` version: \"3\" services: jaeger-agent: image: jaegertracing/jaeger-agent hostname: jaeger-agent command: [\"--collector.host-port=collector-host:14267\"] ports: - \"5775:5775/udp\" - \"6831:6831/udp\" - \"6832:6832/udp\" - \"5778:5778\" networks: - default restart: on-failure environment: - SPAN_STORAGE_TYPE=elasticsearch ``` collector 和 query 可以搭建在同一个实例上,运行方式 version: \"3\" services: jaeger-collector: image: jaegertracing/jaeger-collector ports: - \"14269:14269\" - \"14268:14268\" - \"14267:14267\" - \"9411:9411\" networks: - default restart: on-failure environment: - SPAN_STORAGE_TYPE=elasticsearch command: [ \"--es.server-urls=http:es-host:9200\", \"--log-level=debug\" ] #depends_on: # - elasticsearch jaeger-query: image: jaegertracing/jaeger-query environment: - SPAN_STORAGE_TYPE=elasticsearch - no_proxy=localhost ports: - \"16686:16686\" - \"16687:16687\" networks: - default restart: on-failure command: [ \"--es.server-urls=http://es-host:9200\", \"--span-storage.type=elasticsearch\", \"--log-level=debug\", #\"--query.static-files=/go/jaeger-ui/\" ] depends_on: - jaeger-collector networks: elastic-jaeger: driver: bridge 数据简单展示图例 链路依赖关系图 ","date":"2018-08-15","objectID":"/jaeger/:0:5","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"Django接入 我们开发了自己的 jaeger-python 包（huipy），可以非常简单的在 \bDjango 项目中使用。 接入方式 在中间件中引入 MIDDLEWARE = [ 'huipy.tracer.middleware.TraceMiddleware', # 其他中间件 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'corsheaders.middleware.CorsMiddleware', 'django.middleware.common.CommonMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', ] settings.SERVICE_NAME = 'atlas' # 其他配置 ... \b在发送请求时\u001d引入 from huipy.tracer.httpclient import HttpClient HttpClient(url='http://httpbin.org/get').get() ","date":"2018-08-15","objectID":"/jaeger/:0:6","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"线上部署问题 我们使用 uWSGI 作为 Django app 的容器, 默认的启动模式是 preforking \b在 uWSGI启动的时候，首先主进程会初始化并且load app, 然后会fork出指定数目的子进程。 使用fork函数得到的子进程从父进程的继承了整个进程的地址空间，包括：进程上下文、进程堆栈、内存信息、打开的文件描述符、信号控制设置、进程优先级、进程组号、当前工作目录、根目录、资源限制、控制终端等。 这里提一下linux fork 使用的机制是 copy-on-write(inux系统为了提高系统性能和资源利用率，for出一个新进程时，系统并没有真正复制一个副本。如果多个进程要读取它们自己的那部分资源的副本，那么复制是不必要的。每个进程只要保存一个指向这个资源的指针就可以了。如果一个进程要修改自己的那份资源的“副本”，那么就会复制那份资源) 在绝大多数场景下这种方式不会有问题， 但是当主进程本身是多线程的时候可能就会造成问题。 我们的 tracer初始化后会启动一个后台线程向agent 发送udp数据包，而这个过程在主进程\bload app的时候就完成了。fork子进程的时候这个后台线程当然是不会被fork的， 所以当子进程真正处理请求时，没有后台线程来发送数据。早造成的后果就是我们始终看不到我们\b请求的trace. 我们可以通过查看特定进程的系统调用来查看到信息： 首先是 preforking模式，我们查看某一个 uWSGI进程的调用情况 # mac上使用 dtruss, linux使用 strace sudo dtruss -p 1310 # 输出 SYSCALL(args) = return lazy-apps模式 sudo dtruss -p 1509 # 输出 SYSCALL(args) = return gettimeofday(0x7000050F58E8, 0x0, 0x0) = 0 0 psynch_cvwait(0x105FABF80, 0xC2901000C2A00, 0x5B700) = -1 Err#316 gettimeofday(0x7000050F58E8, 0x0, 0x0) = 0 0 psynch_cvwait(0x105FABF80, 0xC2A01000C2B00, 0x5B700) = -1 Err#316 gettimeofday(0x7000050F58E8, 0x0, 0x0) = 0 0 psynch_cvwait(0x105FABF80, 0xC2B01000C2C00, 0x5B700) = -1 Err#316 gettimeofday(0x7000050F58E8, 0x0, 0x0) = 0 0 psynch_cvwait(0x105FABF80, 0xC2C01000C2D00, 0x5B700) = -1 Err#316 gettimeofday(0x7000050F58E8, 0x0, 0x0) = 0 0 很明显\b在 lazy-apps模式下一直有线程在监听事件，而前者没有这样的线程 解决方案 lazy-apps uWSGI可以使用 lazy-apps模式启动，在主进程fork\b子进程后，每个子进程再初始化和load app。 这样可以保证每个进程独立启动，\b保证了更好的的隔离性。\b在我们的场景中这样每个子进程会启动自己的后台线程。 这个方案的缺点\b是： 启动时间会稍微变长，但是有copy-on-write其实影响不大 占用的内存的会变多 延迟初始化tracer 重构我们的实现，在 middleware执行到 process_request的时候再进行全局的初始化 def process_request(self, request): from huipy.tracer.initial_tracer import initialize_global_tracer self._tracer = initialize_global_tracer() ... ","date":"2018-08-15","objectID":"/jaeger/:0:7","tags":["backend","jaeger","django"],"title":"Jaeger in Django","uri":"/jaeger/"},{"categories":null,"content":"HTTP 协议是整个WEB世界的基石，而我们现在站绝对统治地位的HTTP/1.1从诞生之初到现在已经过去19年了。当时的互联网规模和现在相比无论是从量级还是复杂程度上来说，都是天壤之别。自然HTTP/1.1的许多问题，阻碍了web技术的发展，急需新的方案来解决web发展中出现的问题，HTTP2就是这个背景下诞生的。我们从实践角度来学习一下 HTTTP/2是如何解决当前的一些问题。 ","date":"2018-07-01","objectID":"/http2/:0:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"HTTP发展历史 HTTP/0.9 (1989) 它只有一个 GET 方法, 没有首部, 设计目标是获取HTML(没有图片, 只有文本) HTTP/1.0 (1996) 在 HTTP/0.9 版本的基础上新增了大量内容，加入了许多重要概念: 首部 响应码 重定向 错误 条件请求 内容编码(压缩) 更多的请求方法 … HTTP/1.1 (1999) 在 HTTP/1.0 版本的基础上添加: 强制要求客户端提供 Host 首部, 这使虚拟主机托管成为可能(在一个IP上提供多个web服务)。 当使用新的连接指令时, Web 服务器不需要在每个响应之后关闭连接。添加的变更如下: 缓存相关首部的扩展 OPTIONS 方法 Upgrade 首部 Range 请求 压缩和传输编码(Transfer-encoding) 管道化(pipelining) SPDY (2009) Google 工程师提出了一种替代 HTTP 的方案: SPDY 不是第一个希望替代 HTTP 的方案, 但它是其中最重要的一个, 因为它带来了显而易见的性能提升. 它是 HTTP/2 的基础. HTTP/2 (2015) RFC 7540 在 2015 年 5 月 14 日发布了HTTP/2的正式协议. 期望: 相比于使用 TCP 的 HTTP/1.1, 最终用户可感知的多数延迟都有能够量化的显著改善 解决 HTTP 中的队头阻塞问题 并行的实现机制不依赖与服务器建立多个连接, 从而提升 TCP 连接的利用率, 特别是在拥塞控制方面 保留 HTTP/1.1 的语义, 可以利用已有的文档资源, 包括(但不限于) HTTP 方法, 状态码, URI 和首部字段 明确定义 HTTP/2.0 和 HTTP/1.x 交互的方法, 特别是通过中介时的方法(双向) 明确指出它们可以被合理使用的新的扩展点和策略 ","date":"2018-07-01","objectID":"/http2/:1:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"解决的问题 队头阻塞 服务端收到多个管道请求后，需要按接收顺序逐个响应。如果恰好第一个请求特别慢，后续所有响应都会跟着被阻 TCP利用低效 h1 并不支持多路复用, 所以浏览器一般会针对指定域名开启6个并发连接。这意味着 多个TCP连接（同域名6个）\b\bdns\b开销；TCP拥塞控制经过多个RTT才能达到\b理想的\b吞吐量 消息头冗余 HTTP/1 协议头部使用纯文本格式，没有任何压缩，且包含很多冗余信息（例如 Cookie、UserAgent 每次都会携带），所以一个页面的请求数越多，头部带来的额外开销就越大（\b使用短小和多个域名） 优先级设置受限 浏览器为了先请求优先级高的资源, 会推迟请求其他资源. 但优先级高的资源获取后, 在处理的过程中, 浏览器并不会发起新的资源请求, 所以服务器无法利用这段时间发送优先级低的资源, 总的页面下载时间因此延长了； 一个高优先级资源被浏览器发现后, 但受制于浏览器处理的方式, 它被排在了一个正在获取的低优先级资源之后 ","date":"2018-07-01","objectID":"/http2/:2:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"HTTP2核心 我们先介绍一下HTTP2的核心概念便于更好的理解h2的特性。 HTTP/2 将 HTTP 协议通信分解为二进制编码帧的交换，这些帧对应着特定数据流中的消息。所有这些都在一个 TCP 连接内复用。这是 HTTP/2 协议所有其他功能和性能优化的基础。 数据流stream：已建立的连接内的双向字节流，可以承载一条或多条消息。 消息message：与逻辑请求或响应消息对应的完整的一系列帧。 帧frame：HTTP/2 通信的最小单位，每个帧都包含帧头，至少也会标识出当前帧所属的数据流。 这些概念的关系总结如下： 所有通信都在一个 TCP 连接上完成，此连接可以承载任意数量的双向数据流。 每个数据流都有一个唯一的标识符和可选的优先级信息，用于承载双向消息。 每条消息都是一条逻辑 HTTP 消息（例如请求或响应），包含一个或多个帧。 帧是最小的通信单位，承载着特定类型的数据，例如 HTTP 标头、消息负载，等等。 来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。 下图是我们用Wireshark抓到的h2数据包, 图中箭头指出了帧的组成部分 用表格的形式说明h2的帧结构 ","date":"2018-07-01","objectID":"/http2/:3:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"HTTP2 重要特性 ","date":"2018-07-01","objectID":"/http2/:4:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"多路复用 什么是连接，抽象的说是HTTP依赖的可靠的传输层的连接，一般来说指的是一个TCP连接。 HTTP/2 中引入了多路复用的概念，对于同一个域名的多个请求，会复用同一个连接。 不同请求的的帧就是用 Stream Identifier 来区分。 下图展示了h2中的数据交互过程 ","date":"2018-07-01","objectID":"/http2/:4:1","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"流量控制 h2使得客户端和服务端都具备调整传输速度的能力，用 WINDOW_UPDATE帧来指示流量\b控制信息。每个帧告诉对方，发送方想要接受多少\b字节。当一端接收并处理被发送的数据时，它也会发出一个*WINDOW_UPDATE帧来更新数据接收窗口。 下图展示了流量控制\b帧调整数据收发窗口 ","date":"2018-07-01","objectID":"/http2/:4:2","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"优先级设置 将 HTTP 消息分解为很多独立的帧之后，我们就可以复用多个数据流中的帧，客户端和服务器交错发送和传输这些帧的顺序就成为关键的性能决定因素。为了做到这一点，HTTP/2 标准允许每个数据流都有一个关联的权重和依赖关系： 可以向每个数据流分配一个介于 1 至 256 之间的整数。 每个数据流与其他数据流之间可以存在显式依赖关系。 用下面这个示意图来描述优先级设置的原理 HTTP/2 内的数据流依赖关系通过将另一个数据流的唯一标识符作为父项引用进行声明；如果忽略标识符，相应数据流将依赖于“根数据流”。声明数据流依赖关系指出，应尽可能先向父数据流分配资源，然后再向其依赖项分配资源。换句话说，“请先处理和传输响应 D，然后再处理和传输响应 C”。 共享相同父项的数据流（即，同级数据流）应按其权重比例分配资源。 例如，如果数据流 A 的权重为 12，其同级数据流 B 的权重为 4，那么要确定每个数据流应接收的资源比例，请执行以下操作： 将所有权重求和：4 + 12 = 16 将每个数据流权重除以总权重：A = 12/16, B = 4/16因此，数据流 A 应获得四分之三的可用资源，数据流 B 应获得四分之一的可用资源；数据流 B 获得的资源是数据流 A 所获资源的三分之一。 抓包观察PRIORITY帧 ","date":"2018-07-01","objectID":"/http2/:4:3","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"服务端推送 在h1中我们在网页中内联 CSS、JavaScript，或者通过数据 URI 内联过其他资源，就是为了减少请求次数来降低页面响应时间。对于将资源手动内联到文档中的过程，我们实际上是在将资源推送给客户端，而不是等待客户端请求。使用 HTTP/2，我们不仅可以实现相同结果，还会获得其他性能优势。 服务器可以对一个客户端请求发送多个响应。 换句话说，除了对最初请求的响应外，服务器还可以向客户端推送额外响应，而无需客户端明确地请求。 所有服务器推送数据流都由 PUSH_PROMISE 帧发起，表明了服务器向客户端推送所述资源的意图，并且需要先于请求推送资源的响应数据传输。这种传输顺序非常重要：客户端需要了解服务器打算推送哪些资源，以免为这些资源创建重复请求。满足此要求的最简单策略是先于父响应（即，DATA 帧）发送所有 PUSH_PROMISE 帧，其中包含所承诺资源的 HTTP header。下图\b展示了服务端推送的流程 抓包观察服务端推送 ","date":"2018-07-01","objectID":"/http2/:4:4","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"头部压缩 每个 HTTP 传输都承载一组header，这些header说明了传输的资源及其属性。 在 HTTP/1.x 中, headwer始终以纯文本形式，通常会给每个传输增加 500–800 字节的开销。如果使用 HTTP Cookie，增加的开销有时会达到上千字节。为了减少此开销和提升性能，HTTP/2 使用 HPACK 压缩格式压缩请求和响应标头元数据，这种格式采用两种简单但是强大的技术： 通过静态 Huffman 代码对传输的标头字段进行编码，从而减小了各个传输的大小。 客户端和服务器同时维护和更新一个包含之前见过的header的索引列表（换句话说，它可以建立一个共享的压缩上下文），此列表随后会用作参考，对之前传输的值进行有效编码。 利用 Huffman 编码，可以在传输时对各个值进行压缩，而利用之前传输值的索引列表，我们可以通过传输索引值的方式对重复值进行编码，索引值可用于有效查询和重构完整的header键值对。HPACK 压缩上下文包含一个静态表和一个动态表：静态表在规范中定义，并提供了一个包含所有连接都可能使用的常用 HTTP 标头字段（例如，有效标头名称）的列表；动态表最初为空，将根据在特定连接内交换的值进行更新。每个动态表只针对一个连接，每个连接的压缩解压缩的上下文有且仅有一个动态表。 HPACK示意 我们抓包观察压缩效果 ","date":"2018-07-01","objectID":"/http2/:4:5","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"存在的问题 与H1的兼容 域名拆分： 多个\bTCP连接浪费资源（cpu和内存） 破坏HTTP2 stream 优先级；头部压缩\b效果变差 资源内联： 无法利用缓存；多页面公用的资源内联浪费\b 资源合并： 缓存内用率低，消耗更多的CPU和\b内存 TCP队头阻塞 构建于TCP上层协议的H2多路复用，一旦发生出现TCP队头阻塞，需要小心对待多路的业务数据发送 服务端推送，优先级设置之类的新特性还处于发展阶段，不够完善 ","date":"2018-07-01","objectID":"/http2/:5:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"HTTP 协议是整个WEB世界的基石，而我们现在站绝对统治地位的HTTP/1.1从诞生之初到现在已经过去19年了。当时的互联网规模和现在相比无论是从量级还是复杂程度上来说，都是天壤之别。自然HTTP/1.1的许多问题，阻碍了web技术的发展，急需新的方案来解决web发展中出现的问题，HTTP2就是这个背景下诞生的。我们从实践角度来学习一下 HTTTP/2是如何解决当前的一些问题。 ","date":"2018-07-01","objectID":"/http2/:0:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"HTTP发展历史 HTTP/0.9 (1989) 它只有一个 GET 方法, 没有首部, 设计目标是获取HTML(没有图片, 只有文本) HTTP/1.0 (1996) 在 HTTP/0.9 版本的基础上新增了大量内容，加入了许多重要概念: 首部 响应码 重定向 错误 条件请求 内容编码(压缩) 更多的请求方法 … HTTP/1.1 (1999) 在 HTTP/1.0 版本的基础上添加: 强制要求客户端提供 Host 首部, 这使虚拟主机托管成为可能(在一个IP上提供多个web服务)。 当使用新的连接指令时, Web 服务器不需要在每个响应之后关闭连接。添加的变更如下: 缓存相关首部的扩展 OPTIONS 方法 Upgrade 首部 Range 请求 压缩和传输编码(Transfer-encoding) 管道化(pipelining) SPDY (2009) Google 工程师提出了一种替代 HTTP 的方案: SPDY 不是第一个希望替代 HTTP 的方案, 但它是其中最重要的一个, 因为它带来了显而易见的性能提升. 它是 HTTP/2 的基础. HTTP/2 (2015) RFC 7540 在 2015 年 5 月 14 日发布了HTTP/2的正式协议. 期望: 相比于使用 TCP 的 HTTP/1.1, 最终用户可感知的多数延迟都有能够量化的显著改善 解决 HTTP 中的队头阻塞问题 并行的实现机制不依赖与服务器建立多个连接, 从而提升 TCP 连接的利用率, 特别是在拥塞控制方面 保留 HTTP/1.1 的语义, 可以利用已有的文档资源, 包括(但不限于) HTTP 方法, 状态码, URI 和首部字段 明确定义 HTTP/2.0 和 HTTP/1.x 交互的方法, 特别是通过中介时的方法(双向) 明确指出它们可以被合理使用的新的扩展点和策略 ","date":"2018-07-01","objectID":"/http2/:1:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"解决的问题 队头阻塞 服务端收到多个管道请求后，需要按接收顺序逐个响应。如果恰好第一个请求特别慢，后续所有响应都会跟着被阻 TCP利用低效 h1 并不支持多路复用, 所以浏览器一般会针对指定域名开启6个并发连接。这意味着 多个TCP连接（同域名6个）\b\bdns\b开销；TCP拥塞控制经过多个RTT才能达到\b理想的\b吞吐量 消息头冗余 HTTP/1 协议头部使用纯文本格式，没有任何压缩，且包含很多冗余信息（例如 Cookie、UserAgent 每次都会携带），所以一个页面的请求数越多，头部带来的额外开销就越大（\b使用短小和多个域名） 优先级设置受限 浏览器为了先请求优先级高的资源, 会推迟请求其他资源. 但优先级高的资源获取后, 在处理的过程中, 浏览器并不会发起新的资源请求, 所以服务器无法利用这段时间发送优先级低的资源, 总的页面下载时间因此延长了； 一个高优先级资源被浏览器发现后, 但受制于浏览器处理的方式, 它被排在了一个正在获取的低优先级资源之后 ","date":"2018-07-01","objectID":"/http2/:2:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"HTTP2核心 我们先介绍一下HTTP2的核心概念便于更好的理解h2的特性。 HTTP/2 将 HTTP 协议通信分解为二进制编码帧的交换，这些帧对应着特定数据流中的消息。所有这些都在一个 TCP 连接内复用。这是 HTTP/2 协议所有其他功能和性能优化的基础。 数据流stream：已建立的连接内的双向字节流，可以承载一条或多条消息。 消息message：与逻辑请求或响应消息对应的完整的一系列帧。 帧frame：HTTP/2 通信的最小单位，每个帧都包含帧头，至少也会标识出当前帧所属的数据流。 这些概念的关系总结如下： 所有通信都在一个 TCP 连接上完成，此连接可以承载任意数量的双向数据流。 每个数据流都有一个唯一的标识符和可选的优先级信息，用于承载双向消息。 每条消息都是一条逻辑 HTTP 消息（例如请求或响应），包含一个或多个帧。 帧是最小的通信单位，承载着特定类型的数据，例如 HTTP 标头、消息负载，等等。 来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。 下图是我们用Wireshark抓到的h2数据包, 图中箭头指出了帧的组成部分 用表格的形式说明h2的帧结构 ","date":"2018-07-01","objectID":"/http2/:3:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"HTTP2 重要特性 ","date":"2018-07-01","objectID":"/http2/:4:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"多路复用 什么是连接，抽象的说是HTTP依赖的可靠的传输层的连接，一般来说指的是一个TCP连接。 HTTP/2 中引入了多路复用的概念，对于同一个域名的多个请求，会复用同一个连接。 不同请求的的帧就是用 Stream Identifier 来区分。 下图展示了h2中的数据交互过程 ","date":"2018-07-01","objectID":"/http2/:4:1","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"流量控制 h2使得客户端和服务端都具备调整传输速度的能力，用 WINDOW_UPDATE帧来指示流量\b控制信息。每个帧告诉对方，发送方想要接受多少\b字节。当一端接收并处理被发送的数据时，它也会发出一个*WINDOW_UPDATE帧来更新数据接收窗口。 下图展示了流量控制\b帧调整数据收发窗口 ","date":"2018-07-01","objectID":"/http2/:4:2","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"优先级设置 将 HTTP 消息分解为很多独立的帧之后，我们就可以复用多个数据流中的帧，客户端和服务器交错发送和传输这些帧的顺序就成为关键的性能决定因素。为了做到这一点，HTTP/2 标准允许每个数据流都有一个关联的权重和依赖关系： 可以向每个数据流分配一个介于 1 至 256 之间的整数。 每个数据流与其他数据流之间可以存在显式依赖关系。 用下面这个示意图来描述优先级设置的原理 HTTP/2 内的数据流依赖关系通过将另一个数据流的唯一标识符作为父项引用进行声明；如果忽略标识符，相应数据流将依赖于“根数据流”。声明数据流依赖关系指出，应尽可能先向父数据流分配资源，然后再向其依赖项分配资源。换句话说，“请先处理和传输响应 D，然后再处理和传输响应 C”。 共享相同父项的数据流（即，同级数据流）应按其权重比例分配资源。 例如，如果数据流 A 的权重为 12，其同级数据流 B 的权重为 4，那么要确定每个数据流应接收的资源比例，请执行以下操作： 将所有权重求和：4 + 12 = 16 将每个数据流权重除以总权重：A = 12/16, B = 4/16因此，数据流 A 应获得四分之三的可用资源，数据流 B 应获得四分之一的可用资源；数据流 B 获得的资源是数据流 A 所获资源的三分之一。 抓包观察PRIORITY帧 ","date":"2018-07-01","objectID":"/http2/:4:3","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"服务端推送 在h1中我们在网页中内联 CSS、JavaScript，或者通过数据 URI 内联过其他资源，就是为了减少请求次数来降低页面响应时间。对于将资源手动内联到文档中的过程，我们实际上是在将资源推送给客户端，而不是等待客户端请求。使用 HTTP/2，我们不仅可以实现相同结果，还会获得其他性能优势。 服务器可以对一个客户端请求发送多个响应。 换句话说，除了对最初请求的响应外，服务器还可以向客户端推送额外响应，而无需客户端明确地请求。 所有服务器推送数据流都由 PUSH_PROMISE 帧发起，表明了服务器向客户端推送所述资源的意图，并且需要先于请求推送资源的响应数据传输。这种传输顺序非常重要：客户端需要了解服务器打算推送哪些资源，以免为这些资源创建重复请求。满足此要求的最简单策略是先于父响应（即，DATA 帧）发送所有 PUSH_PROMISE 帧，其中包含所承诺资源的 HTTP header。下图\b展示了服务端推送的流程 抓包观察服务端推送 ","date":"2018-07-01","objectID":"/http2/:4:4","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"头部压缩 每个 HTTP 传输都承载一组header，这些header说明了传输的资源及其属性。 在 HTTP/1.x 中, headwer始终以纯文本形式，通常会给每个传输增加 500–800 字节的开销。如果使用 HTTP Cookie，增加的开销有时会达到上千字节。为了减少此开销和提升性能，HTTP/2 使用 HPACK 压缩格式压缩请求和响应标头元数据，这种格式采用两种简单但是强大的技术： 通过静态 Huffman 代码对传输的标头字段进行编码，从而减小了各个传输的大小。 客户端和服务器同时维护和更新一个包含之前见过的header的索引列表（换句话说，它可以建立一个共享的压缩上下文），此列表随后会用作参考，对之前传输的值进行有效编码。 利用 Huffman 编码，可以在传输时对各个值进行压缩，而利用之前传输值的索引列表，我们可以通过传输索引值的方式对重复值进行编码，索引值可用于有效查询和重构完整的header键值对。HPACK 压缩上下文包含一个静态表和一个动态表：静态表在规范中定义，并提供了一个包含所有连接都可能使用的常用 HTTP 标头字段（例如，有效标头名称）的列表；动态表最初为空，将根据在特定连接内交换的值进行更新。每个动态表只针对一个连接，每个连接的压缩解压缩的上下文有且仅有一个动态表。 HPACK示意 我们抓包观察压缩效果 ","date":"2018-07-01","objectID":"/http2/:4:5","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"存在的问题 与H1的兼容 域名拆分： 多个\bTCP连接浪费资源（cpu和内存） 破坏HTTP2 stream 优先级；头部压缩\b效果变差 资源内联： 无法利用缓存；多页面公用的资源内联浪费\b 资源合并： 缓存内用率低，消耗更多的CPU和\b内存 TCP队头阻塞 构建于TCP上层协议的H2多路复用，一旦发生出现TCP队头阻塞，需要小心对待多路的业务数据发送 服务端推送，优先级设置之类的新特性还处于发展阶段，不够完善 ","date":"2018-07-01","objectID":"/http2/:5:0","tags":["HTTP2","Web"],"title":"从实践出发理解HTTP2","uri":"/http2/"},{"categories":null,"content":"监控报警一般都是基于日志数据来做，总结起来就是根据监控指标查询一定时间段内的该指标的变化情况，当该指标超过设定的阈值时则触发报警规则，发出报警信息包括邮件，短信，微信甚至自动拨打电话等。 \b要\b实现监控报警的功能应该几步来做： 指定日志规范，生成日志 收集日志 查询分析日志 触发报警规则 报警 其中\b第二步\b日志收集就是一项不小的工作。一般会有这\b三部分组成 Agent : 部署在各个应用服务器收集应用的日志 Collector: 日志收集中心，把分散在Agent的日志统一收集到日志中心 Storage: 存储层，日志收集之后的存储 当\b\b数据量大\b的时候，可以采用 kafaka,redis这样的中间件来缓冲日志发送量，提高系统的可靠性。 可以采用的方案有: Flume日志收集，ELK\b方案等 我们因为已经搭建了ELK\b日志收集平台，已经完成了第二步，\b要做监控报警自然就\b基于ELK来做。 ","date":"2018-06-16","objectID":"/elasalert/:0:0","tags":["es","alert"],"title":"基于elastalert的监控报警系统","uri":"/elasalert/"},{"categories":null,"content":"方案选择 先简单分析一下我们\b要做的事情 设置报警规则 查询ES中的\b特定指标 分析是否达到阈值 发报警邮件，微信 经过调研，有这么几个选择： 自己\b开发 ELK提供了XPack, 具有报警的作用 第三方开源方案 首先试用了 Xpack本身的功能还是挺吸引\b人的 权限控制 + ELK集群监控，\b但因为监控报警功能不\b够灵活，\b达不到自己的需求且是收费的（穷.jpg); yelp开源了Elastalert, yelp也是先有了自己的ELK, 而后需要搭建监控报警系统的时候，开发了 Elastalert。跟我们的路线和需求非常匹配，那果断放弃重复造轮子。Elastalert非常灵活，除了通过配置进行报警规则设置，还可以自己开发\b独有的报警规则。 经过在测试系统的对比，决定采用 Elastalert做\b作为监控报警系统的核心。 下图展示了我们监控系统的组件及架构\b简图 服务搭建 安装依赖 \b新建 pyhton2.7 虚拟环境 \u0026 sudo apt install -y virtualenv \u0026 sudo apt install -y python-dev libffi-dev libssl-dev \u0026 virtualenv env 激活env 安装\b pip $ sudo apt-get install python-setuptools python-dev build-essential $ sudo easy_install pip 安装 elastalert 先安装这个版本，解决依赖包boto3的问题, error: python-dateutil 2.7.0 is installed but python-dateutil\u003c2.7.0,\u003e=2.1 is required by set(['botocore']) $ pip install \"python-dateutil\u003c2.7.0,\u003e=2.1\" \u0026 pip install elastalert \u0026 pip install elasticsearch 如果报错 permission则 denied pip install –user elastalert 设置 elasticsearch 把查询和报警的信息和元数据存\b到 elasticsearch 中。这样可以在 kibana 中查询到\b报警的错误，异常和执行情况。 $ elastalert-create-index New index name (Default elastalert_status) Name of existing index to copy (Default None) New index elastalert_status created Done! \b配置和设置规则 把公共的配置写到 config.yaml vim config.yaml 示例: # The Elasticsearch hostname for metadata writeback # Note that every rule can have its own Elasticsearch host es_host: localhost # The Elasticsearch port es_port: 9200 创建报警规则 vim alert_rules/alert_frquency.yaml 示例: name: Example rule type: frequency index: access num_events: 50 timeframe: minutes: 5 filter: - term: some_field: \"some_value\" alert: - \"email\" email: - \"elastalert@example.com\" 测试报警规则 $ elastalert-test-rule alert_rules/alert_frequency.yam 启动服务 $ python -m elastalert.elastalert --verbose --rule alert_frequency.yaml 用supervisor管理监控服务 安装 supervisor sudo apt install -y supervisor 配置elastalert [program:elastalert] command=/home/xxx/elastalert/env/bin/elastalert --config /home/xxx/elastalert/config.yaml --verbose autostart=true autorestart=true stdout_logfile=/var/log/supervisor/elastalert.log stderr_logfile=/var/log/supervisor/elastalert_error.log stopsignal=INT stderr_logfile_maxbytes=5MB stdout_logfile_maxbytes=5MB 启动服务 sudo supervisorctl start elastalert ","date":"2018-06-16","objectID":"/elasalert/:1:0","tags":["es","alert"],"title":"基于elastalert的监控报警系统","uri":"/elasalert/"},{"categories":null,"content":"开发报警规则和报警方式 报警类型 class AwesomeNewRule(RuleType): # ... def add_data(self, data): # ... def get_match_str(self, match): # ... def garbage_collect(self, timestamp): # ... 报警方式 class AwesomeNewAlerter(Alerter): required_options = set(['some_config_option']) def alert(self, matches): ... def get_info(self): ... \b报警 on-the-fly class MyEnhancement(BaseEnhancement): def process(self, match): # Drops a match if \"field_1\" == \"field_2\" if match['field_1'] == match['field_2']: raise DropMatchException() ","date":"2018-06-16","objectID":"/elasalert/:2:0","tags":["es","alert"],"title":"基于elastalert的监控报警系统","uri":"/elasalert/"},{"categories":null,"content":"监控报警一般都是基于日志数据来做，总结起来就是根据监控指标查询一定时间段内的该指标的变化情况，当该指标超过设定的阈值时则触发报警规则，发出报警信息包括邮件，短信，微信甚至自动拨打电话等。 \b要\b实现监控报警的功能应该几步来做： 指定日志规范，生成日志 收集日志 查询分析日志 触发报警规则 报警 其中\b第二步\b日志收集就是一项不小的工作。一般会有这\b三部分组成 Agent : 部署在各个应用服务器收集应用的日志 Collector: 日志收集中心，把分散在Agent的日志统一收集到日志中心 Storage: 存储层，日志收集之后的存储 当\b\b数据量大\b的时候，可以采用 kafaka,redis这样的中间件来缓冲日志发送量，提高系统的可靠性。 可以采用的方案有: Flume日志收集，ELK\b方案等 我们因为已经搭建了ELK\b日志收集平台，已经完成了第二步，\b要做监控报警自然就\b基于ELK来做。 ","date":"2018-06-16","objectID":"/elasalert/:0:0","tags":["es","alert"],"title":"基于elastalert的监控报警系统","uri":"/elasalert/"},{"categories":null,"content":"方案选择 先简单分析一下我们\b要做的事情 设置报警规则 查询ES中的\b特定指标 分析是否达到阈值 发报警邮件，微信 经过调研，有这么几个选择： 自己\b开发 ELK提供了XPack, 具有报警的作用 第三方开源方案 首先试用了 Xpack本身的功能还是挺吸引\b人的 权限控制 + ELK集群监控，\b但因为监控报警功能不\b够灵活，\b达不到自己的需求且是收费的（穷.jpg); yelp开源了Elastalert, yelp也是先有了自己的ELK, 而后需要搭建监控报警系统的时候，开发了 Elastalert。跟我们的路线和需求非常匹配，那果断放弃重复造轮子。Elastalert非常灵活，除了通过配置进行报警规则设置，还可以自己开发\b独有的报警规则。 经过在测试系统的对比，决定采用 Elastalert做\b作为监控报警系统的核心。 下图展示了我们监控系统的组件及架构\b简图 服务搭建 安装依赖 \b新建 pyhton2.7 虚拟环境 \u0026 sudo apt install -y virtualenv \u0026 sudo apt install -y python-dev libffi-dev libssl-dev \u0026 virtualenv env 激活env 安装\b pip $ sudo apt-get install python-setuptools python-dev build-essential $ sudo easy_install pip 安装 elastalert 先安装这个版本，解决依赖包boto3的问题, error: python-dateutil 2.7.0 is installed but python-dateutil\u003c2.7.0,\u003e=2.1 is required by set(['botocore']) $ pip install \"python-dateutil\u003c2.7.0,\u003e=2.1\" \u0026 pip install elastalert \u0026 pip install elasticsearch 如果报错 permission则 denied pip install –user elastalert 设置 elasticsearch 把查询和报警的信息和元数据存\b到 elasticsearch 中。这样可以在 kibana 中查询到\b报警的错误，异常和执行情况。 $ elastalert-create-index New index name (Default elastalert_status) Name of existing index to copy (Default None) New index elastalert_status created Done! \b配置和设置规则 把公共的配置写到 config.yaml vim config.yaml 示例: # The Elasticsearch hostname for metadata writeback # Note that every rule can have its own Elasticsearch host es_host: localhost # The Elasticsearch port es_port: 9200 创建报警规则 vim alert_rules/alert_frquency.yaml 示例: name: Example rule type: frequency index: access num_events: 50 timeframe: minutes: 5 filter: - term: some_field: \"some_value\" alert: - \"email\" email: - \"elastalert@example.com\" 测试报警规则 $ elastalert-test-rule alert_rules/alert_frequency.yam 启动服务 $ python -m elastalert.elastalert --verbose --rule alert_frequency.yaml 用supervisor管理监控服务 安装 supervisor sudo apt install -y supervisor 配置elastalert [program:elastalert] command=/home/xxx/elastalert/env/bin/elastalert --config /home/xxx/elastalert/config.yaml --verbose autostart=true autorestart=true stdout_logfile=/var/log/supervisor/elastalert.log stderr_logfile=/var/log/supervisor/elastalert_error.log stopsignal=INT stderr_logfile_maxbytes=5MB stdout_logfile_maxbytes=5MB 启动服务 sudo supervisorctl start elastalert ","date":"2018-06-16","objectID":"/elasalert/:1:0","tags":["es","alert"],"title":"基于elastalert的监控报警系统","uri":"/elasalert/"},{"categories":null,"content":"开发报警规则和报警方式 报警类型 class AwesomeNewRule(RuleType): # ... def add_data(self, data): # ... def get_match_str(self, match): # ... def garbage_collect(self, timestamp): # ... 报警方式 class AwesomeNewAlerter(Alerter): required_options = set(['some_config_option']) def alert(self, matches): ... def get_info(self): ... \b报警 on-the-fly class MyEnhancement(BaseEnhancement): def process(self, match): # Drops a match if \"field_1\" == \"field_2\" if match['field_1'] == match['field_2']: raise DropMatchException() ","date":"2018-06-16","objectID":"/elasalert/:2:0","tags":["es","alert"],"title":"基于elastalert的监控报警系统","uri":"/elasalert/"},{"categories":null,"content":"AWS RDS主从同步延迟调查 ","date":"2018-05-04","objectID":"/replica-research/:1:0","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"背景 我们的关系型数据库用的是AWS的RDS，mysql主从延迟这个问题之前是知道的，简单怀疑是AWS的主从同步机制问题但是一直没有重视起来去研究一下问题的根源。终于，隐患\b暴露处来— 一次alter table和主从延迟曾大发生的时间点重合，从而加剧主从延迟。造成从库远远落后于主库\b，而数据不一致。必须找到延迟过大的根本原因，解决这个问题。\b 在分析问题之前我们先简要\b介绍一下\bmysql主从同步机制 ","date":"2018-05-04","objectID":"/replica-research/:1:1","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"主从同步机制 mysql在主库上记录binlog。在准备提交事务\b完成数据更新前，主库把数据更新的事件记录到binlog中。 mysql通过\b上面说的binlog用3个线程来实现主从同步，master上的Binlog Dump Thread, slave上的Slave I/O Thread和Slave SQL Thread Slave I/O Thread: I/O\b线程跟主库建立一个普通的客户端连接，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容, 存到relaylog中, 然后将读取到的主库binlog的文件名和位置记录到master-info文件中 Binlog Dump Thread: 读取主库上的binlog中的事件，根据请求信息读取制定日志指定位置之后的日志信息，返回给Slave I/O Thread。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的binlog文件的名称以及binlog的位置。 Slave SQL Thread: 从relaylog中读取事件\b并且在从库执行，从而实现从库的更新 \b\b通过观察从库的\b\b同步延迟曲线发现每天固定的三个时间点从库都会报警主从延迟超过阈值。 ","date":"2018-05-04","objectID":"/replica-research/:1:2","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"首先从AWS\b官方文档上寻找主从同步延迟的办法 可通过多种方式来减少对源数据库实例的更新与对只读副本的后续更新之间的滞后，例如： 将只读副本的存储大小和数据库实例类调整到与源数据库实例类似。 确保源数据库实例和只读副本使用的数据库参数组中的参数设置相兼容。有关更多信息和示例，请参阅本部分后面的有关 max_allowed_packet 参数的讨论。 Amazon RDS 监控只读副本的复制状态，如果由于任何原因停止复制，则将只读副本实例的 Replication State 字段更新为 Error。可能会有这样的例子，在您的只读副本上运行的 DML 查询与对源数据库实例的更新冲突。 您可通过查看 Replication Error 字段，检查 MySQL 或 MariaDB 引擎引发的关联错误的详细信息。还生成指示只读副本状态的事件，包括 RDS-EVENT-0045、RDS-EVENT-0046 和 RDS-EVENT-0047。有关这些事件和事件订阅的详细信息，请参阅 使用 Amazon RDS 事件通知。如果返回 MySQL 错误消息，则检查 MySQL 错误消息文档中的错误编号。如果返回 MariaDB 错误消息，则检查 MariaDB 错误消息文档中的错误。 一个可导致复制出错的常见问题是只读副本的 max_allowed_packet 参数的值小于源数据库实例的 max_allowed_packet 参数的值。max_allowed_packet 参数是可在数据库参数组中进行设置的自定义参数，用于指定可在数据库上执行的最大 DML 代码大小。有时候，与源数据库实例关联的数据库参数组中的 max_allowed_packet 参数值，要小于与源的只读副本关联的数据库参数组中的 max_allowed_packet 参数值。在这些情况下，复制过程可能会引发错误 (数据包大于 ‘max_allowed_packet’ 字节) 并停止复制。可通过让源和只读副本使用 max_allowed_packet 参数值相同的数据库参数组来纠正该错误。 按照官网文档的建议尝试，发现并不是以上\b所说问题。必须\b\b深入分析，找到原因。 \b ","date":"2018-05-04","objectID":"/replica-research/:1:3","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"分析 \b是否在延迟报警的时间点有\b大量的写操作 通过代码排查，在这些\b时间点并没有类似的操作 在延迟发生的时间点观察执行状态 主库show PROCESSLIST，并没有很多的写操作同时也印证了代码的排查的结论 从库show PROCESSLIST，看到十分重要的\b信息 发生了\b锁表并且有很多\b写入在排队 观察innodb_trx和innodb_lock 查看是否有大的事务\b拿着锁不放，结果是没有 通过以上信息，判读不是我们自己的写操作造成主从同步延迟 继续寻找线索 \b查看发生延迟时间点的慢查询日志 查看到\b这个时候有很多类似的查询 SELECT /*!40001 SQL_NO_CACHE */ * FROM 先来看这个查询的意义： /*! */ 这是mysql里的语法，表示达到条件会执行相应的语句。 !后面是版本号， 如果本数据库等于或大于此版本号，那么语句会执行。 那么这句话的意思是 如果版本号大于或等于4，会执行 sql_no_cache, 就是不用缓存数据。 而并非说本次查询不作为下次查询的缓存。 在备份操作时Mysql 会自动调用此语法。该语句会查询到表中所有数据，在备份文件中会生成相应的insert语句。 查看\b数据库备份 每隔8小时执行一次 mysqldump -u ${user} -p${password} -h ${host} ${name} \u003e ${filename} 可以看到时间点与主从同步延迟的报警时间完全吻合 ","date":"2018-05-04","objectID":"/replica-research/:1:4","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"原因 通过\b一系列列大胆假设\b和小心求证,可以断定是我们\b在执行自动备份的时候大量锁表，造成很多同步的写入操作被阻塞 mysqldumb 有个很关键的点是为了保证同步后数据的一致性，\b\b会采用 lock-tables, lock-all-tables, single-transaction不同的机制。 lock-tables: 默认会选择这个机制，把所有需要dump的表都会加锁，当然写入表的操作就被阻塞了 lock-all-tables: dump\b期间锁定所有数据库中的所有表，以保证数据的一致性。这是一个全局锁定，并且自动关闭–single-transaction 和–lock-tables 选项。这个参数副作用比较大，这是全库锁定，备份执行过程中，该库无法进行读写操作 single-transaction: 这个机制把事务隔离级别设置为可重复读，在dump之前提交一个START TRANSACTION语句，不会阻塞任何应用程序且能保证导出时数据库的一致性状态。它只适用于多版本存储引擎(MVCC)，仅InnoDB。本选项和–lock-tables 选项是互斥的，因为LOCK TABLES 会使任何挂起的事务隐含提交，使用参数–single-transaction会自动关闭该选项。 ","date":"2018-05-04","objectID":"/replica-research/:1:5","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"解决方案 因为我们的db引擎是innodb, 最简单的方式就是修改mysqldump保证\b数据一致性的机制，采用single-transaction mysqldump -u ${user} -p${password} -h ${host} ${name} --single-transaction --quick \u003e ${filename} single-transaction只适用于innodb, 在innodb导出时会建立一致性快照，在保证导出数据的一致性前提下，又不会堵塞其他会话的读写操作，相比–lock-tables参数来说锁定粒度要低，造成的影响也要小很多。指定这个参数后，其他连接不能执行ALTER TABLE、DROP TABLE 、RENAME TABLE、TRUNCATE TABLE这类语句，事务的隔离级别无法控制DDL(Data Definition Languages)语句。所以改\b表的时候，要注意查看是否有数据库备份执行 \b再起一个从库，\b单独在此从库上执行备份 因为数据库备份的时候占用不少资源，而同时我们的从库也有大量的读的操作，会互相影响。\b 不用mysqldump选择别的备份方案, 比如XtraBackup 下图是\b\b执行解决方案后从库延迟\b的曲线图，可以看到之前定时出现的主从延迟(心头大患)已经消失了(开心.jpg) \b\b\b主要参考：https://dev.mysql.com/doc/refman/5.6/en/replication.html。 ","date":"2018-05-04","objectID":"/replica-research/:1:6","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"总结 多学习多思考，\b对数据库的操作包括基本的增删改查\b要有敬畏之心。 附加部分 ","date":"2018-05-04","objectID":"/replica-research/:1:7","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"数据库查询优化 数据库主从同步延迟报警的时间点，\b还有主库CPU和连接数报警，在一开始因为时间点基本一致，而\b认为两者一定时相关的，误导了我对根本原因的判读。\b更重要的是针对这种报警必须优化。下面就是一个优化的例子。 ","date":"2018-05-04","objectID":"/replica-research/:2:0","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"1. 分析原因 根据数据库报警信息 CPU和connections超过阈值报警，找到对应时间点的慢查询 根据报警时间点，\b找到\b对应的定时任务 根据时间，\b\b查\b看慢查询日志 AWS RDS开启slow_query_log，设置三个关键参数: slow_query_log（\b1）, long_query_time（比如10秒）, log_output（file）\b 设置完成后可以用show variables like '参数名称'来查看是否设置成功 # Time: 180504 0:28:20 # User@Host: xxx(脱敏) Id: 164478982 # Query_time: 3.542192 Lock_time: 0.000056 Rows_sent: 1 Rows_examined: 76281 SET timestamp=1525393700; SELECT COUNT(*) AS `__count` FROM `core_membership` WHERE (`core_membership`.`business_group_id` = 2908 AND `core_membership`.`last_visited_at` \u003e '2018-05-04 23:59:59.999999' AND `core_membership`.`last_visited_at` \u003c= '2018-02-03 23:59:59.999999'); # 这个慢查询必须优化 # Time: 180504 0:27:31 # User@Host: xxx(脱敏) Id: 164477983 # Query_time: 45.261294 Lock_time: 0.000043 Rows_sent: 1 Rows_examined: 8583 SET timestamp=1525393651; SELECT COUNT(*) AS `__count` FROM `core_membership` WHERE (`core_membership`.`business_group_id` = 1976 AND `core_membership`.`level` = 1); 慢查询和\b应用中的定时任务要执行的\b查询完全一致 ","date":"2018-05-04","objectID":"/replica-research/:2:1","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"2. 优化 分析慢查询日志确定拖垮db的\b根本原因是上面的第二条查询 通过explain分析\b上面的查询语句，虽然用到了索引，但依然很慢 原因：level为1的会员占了绝大多数，\b即使用了索引，这样的查询扫索引依然很多行，所以慢 下面的执行结果非常明显 sql\u003e SELECT COUNT(*) AS `__count` FROM `core_membership` WHERE (`core_membership`.`business_group_id` = 2708 AND `core_membership`.`level` = 1) [2018-05-04 13:16:31] 1 row retrieved starting from 1 in 2s 987ms (execution: 2s 964ms, fetching: 23ms) sql\u003e SELECT COUNT(*) AS `__count` FROM `core_membership` WHERE (`core_membership`.`business_group_id` = 2708 AND `core_membership`.`level` \u003e 1) [2018-05-04 13:17:16] 1 row retrieved starting from 1 in 598ms (execution: 587ms, fetching: 11ms) 优化方案：\blevel大于1的会员为少数，最简单有效的方法就是只查询高等级的会员，这样\b查询索引的时候\b自然就快。然后再用总数跟高等级的人数做减法，来避免level 1的查询 下图是优化后的主库CPU占用曲线\b图，可以看到优化后峰值明显减少 图上剩下的峰值就是下一步的优化对象。 \b更新时间：2018-05-15 ","date":"2018-05-04","objectID":"/replica-research/:2:2","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"死锁排查 ","date":"2018-05-04","objectID":"/replica-research/:3:0","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"问题发现 sentry上出现报错 An error occurred in the current transaction. You can't execute queries until the end of the 'atomic' block. 具体查看就是 'Deadlock found when trying to get lock; try restarting transaction' 问题十分明显，并发的情况下多个事务出现了**死锁* 这里对于 mysql 的死锁及相关概念就不展开，类似的介绍网上很多。 ","date":"2018-05-04","objectID":"/replica-research/:3:1","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"排查原因 查看死锁信息的重要命令 SHOW ENGINE INNODB STATUS; 此命令还展示了很多其他信息，比如外键错误，这里就省略，只列出死锁相关的重要信息 ------------------------ LATEST DETECTED DEADLOCK ------------------------ 2018-05-15 02:53:14 2b2b78d00700 *** (1) TRANSACTION: TRANSACTION 5052291242, ACTIVE 26 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 10 lock struct(s), heap size 1184, 3 row lock(s), undo log entries 3 MySQL thread id 197230223, OS thread handle 0x2b3239e46700, query id xxx(脱敏） update INSERT INTO `servant_servantorder` (`uid`, `visit_id`, `dish_id`, `account_id`, `online_order_id`, `name`, `unit_price`, `member_price`, `amount`, `unit`, `main_category`, `discountable`, `created_at`, `updated_at`, `deactivated_at`) VALUES ('7064c4991bd24f71bcf8b65367eff21d', 1475092, 65990, NULL, NULL, '小毛巾', '1.50', '1.50', '10.00', '条', '一次性用品', 0, '2018-05-15 10:52:48.496575', '2018-05-15 10:52:48.496827', NULL) *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 1361 page no 677 n bits 160 index `PRIMARY` of table `xxx`.`servant_servantdish` trx id 5052291242 lock mode S locks rec but not gap waiting Record lock, heap no 83 PHYSICAL RECORD: n_fields 29; compact format; info bits 0 0: len 4; hex 800101c6; asc ;; 1: len 6; hex 00012d23d88e; asc -# ;; ... *** (2) TRANSACTION: TRANSACTION 5052291214, ACTIVE 26 sec inserting mysql tables in use 1, locked 1 57 lock struct(s), heap size 6544, 4113 row lock(s), undo log entries 4541 MySQL thread id 197230216, OS thread handle 0x2b2b78d00700, query id xxx(脱敏 update INSERT INTO `servant_servantdish` (`uid`, `servant_id`, `code`, `name`, `unit_price`, `member_price`, `minimum_amount`, `cost`, `unit`, `size`, `english_name`, `description`, `image`, `main_category`, `sub_category`, `sales_category`, `available`, `discountable`, `minimum_chargeable`, `in_dish_set`, `can_order`, `erasable`, `created_at`, `updated_at`, `deactivated_at`, `json_data`) VALUES ('8a42e8b35d1841818d53eb1c80035fb2', 161, '722', '农家红烧杂鱼', '48.00', '48.00', '0.00', '0.00', '例', '', 'NJHSZY', '', NULL, '海鲜鱼类', '鱼类', '加工成品', 1, 0, 1, 0, 0, 0, '2018-05-15 10:53:14.148526', '2018-05-15 10:53:14.148801', NULL, NULL) *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 1361 page no 677 n bits 168 index `PRIMARY` of table `xxx`.`servant_servantdish` trx id 5052291214 lock_mode X locks rec but not gap Record lock, heap no 14 PHYSICAL RECORD: n_fields 29; compact format; info bits 0 0: len 4; hex 80010180; asc ;; 1: len 6; hex 00012d23d88e; asc -# ;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 907 page no 14 n bits 112 index `PRIMARY` of table `xxx`.`servant_servant` trx id 5052291214 lock mode S locks rec but not gap waiting Record lock, heap no 13 PHYSICAL RECORD: n_fields 10; compact format; info bits 0 0: len 4; hex 800000a1; asc ;; 1: len 6; hex 00012d23d8aa; asc -# ;; 2: len 7; hex 6e00002bcf0c02; asc n + ;; 3: len 8; hex 999fdead2e091650; asc . P;; 4: len 4; hex 80001874; asc t;; 5: len 8; hex 999fdead300729c8; asc 0 ) ;; 6: SQL NULL; 7: len 4; hex 80000052; asc R;; 8: len 4; hex 80000052; asc R;; 9: len 30; hex 7b2276656e646f72223a20227b5c22446973706c61794e616d655c223a5c; asc {\"vendor\": \"{\\\"DisplayName\\\":\\; (total 538 bytes); ... *** WE ROLL BACK TRANSACTION (1) ","date":"2018-05-04","objectID":"/replica-research/:3:2","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"分析日志 通过分析第一部分，可以看到事务1在往表 servantorder 里插入一条记录，在等待servantdish主键(page 677)的 S 锁(共享锁) *** (1) TRANSACTION: TRANSACTION 5052291242, ACTIVE 26 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 10 lock struct(s), heap size 1184, 3 row lock(s), undo log entries 3 MySQL thread id 197230223, OS thread handle 0x2b3239e46700, query id xxx(脱敏）update INSERT INTO `servant_servantorder` (`uid`, `visit_id`, `dish_id`, `account_id`, `online_order_id`, `name`, `unit_price`, `member_price`, `amount`, `unit`, `main_category`, `discountable`, `created_at`, `updated_at`, `deactivated_at`) VALUES ('7064c4991bd24f71bcf8b65367eff21d', 1475092, 65990, NULL, NULL, '小毛巾', '1.50', '1.50', '10.00', '条', '一次性用品', 0, '2018-05-15 10:52:48.496575', '2018-05-15 10:52:48.496827', NULL) *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 1361 page no 677 n bits 160 index `PRIMARY` of table `zaihui`.`servant_servantdish` trx id 5052291242 lock mode S locks rec but not gap waiting 通过分析第二部分可以看到事务2持有 servantdish(page 677）的 X 锁(排它锁)，正是事务1在等待的锁。而同时事务2还在等待servant(page 14)的 S 锁 *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 1361 page no 677 n bits 168 index `PRIMARY` of table `xxx`.`servant_servantdish` trx id 5052291214 lock_mode X locks rec but not gap Record lock, heap no 14 PHYSICAL RECORD: n_fields 29; compact format; info bits 0 0: len 4; hex 80010180; asc ;; 1: len 6; hex 00012d23d88e; asc -# ;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 907 page no 14 n bits 112 index `PRIMARY` of table `xxx`.`servant_servant` trx id 5052291214 lock mode S locks rec but not gap waiting 确实发生了死锁，但其实我们从日志获得的信息还不够。因为第一部分并没有明确显示，事务1持有servant的锁。实际上show engine innodb status并不会显示事务的全部信息。所以需要去发生死锁的业务逻辑里找到证据。 # 事务1 with transaction.atomic(): # 获取 servant 的 X 锁 servant.last_api_call = datetime.datetime.now() servant.save() 最后因为mysql检测到了死锁, 事务1权重较小，被回滚。 下面通过一个表格的形式，梳理一下整个流程。 步骤 事务1 事务2 1 begin 2 INSERT INTO servant_servantdish (…) VALUES (…); 持有servantdish的 X 锁 3 begin 4 更新servant, 获得 servant的 X 锁 5 \b\u001d查询servant, 等待servant的 S 锁 6 INSERT INTO servant_servantorder (…) VALUES (…); 等待servant_dish的 S\b 锁 7 事务1和事务2互相等待对方的\b释放锁，发生死锁 8 监测到死锁，\b权重较小的事务1被回滚 ","date":"2018-05-04","objectID":"/replica-research/:3:3","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"总结 发生死锁时，一定要及时排查死锁原因，从业务层面进行解决。首先根据死锁信息分析循环等待的场景，然后根据当前各个事务执行的SQL分析出加锁类型以及顺序，逆向推断出如何形成循环等待，\b必要时要\b具体的查看业务层的代码，看具体执行了哪些操作，这样就能找到死锁产生的原因了。 ","date":"2018-05-04","objectID":"/replica-research/:3:4","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"AWS RDS主从同步延迟调查 ","date":"2018-05-04","objectID":"/replica-research/:1:0","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"背景 我们的关系型数据库用的是AWS的RDS，mysql主从延迟这个问题之前是知道的，简单怀疑是AWS的主从同步机制问题但是一直没有重视起来去研究一下问题的根源。终于，隐患\b暴露处来— 一次alter table和主从延迟曾大发生的时间点重合，从而加剧主从延迟。造成从库远远落后于主库\b，而数据不一致。必须找到延迟过大的根本原因，解决这个问题。\b 在分析问题之前我们先简要\b介绍一下\bmysql主从同步机制 ","date":"2018-05-04","objectID":"/replica-research/:1:1","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"主从同步机制 mysql在主库上记录binlog。在准备提交事务\b完成数据更新前，主库把数据更新的事件记录到binlog中。 mysql通过\b上面说的binlog用3个线程来实现主从同步，master上的Binlog Dump Thread, slave上的Slave I/O Thread和Slave SQL Thread Slave I/O Thread: I/O\b线程跟主库建立一个普通的客户端连接，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容, 存到relaylog中, 然后将读取到的主库binlog的文件名和位置记录到master-info文件中 Binlog Dump Thread: 读取主库上的binlog中的事件，根据请求信息读取制定日志指定位置之后的日志信息，返回给Slave I/O Thread。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的binlog文件的名称以及binlog的位置。 Slave SQL Thread: 从relaylog中读取事件\b并且在从库执行，从而实现从库的更新 \b\b通过观察从库的\b\b同步延迟曲线发现每天固定的三个时间点从库都会报警主从延迟超过阈值。 ","date":"2018-05-04","objectID":"/replica-research/:1:2","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"首先从AWS\b官方文档上寻找主从同步延迟的办法 可通过多种方式来减少对源数据库实例的更新与对只读副本的后续更新之间的滞后，例如： 将只读副本的存储大小和数据库实例类调整到与源数据库实例类似。 确保源数据库实例和只读副本使用的数据库参数组中的参数设置相兼容。有关更多信息和示例，请参阅本部分后面的有关 max_allowed_packet 参数的讨论。 Amazon RDS 监控只读副本的复制状态，如果由于任何原因停止复制，则将只读副本实例的 Replication State 字段更新为 Error。可能会有这样的例子，在您的只读副本上运行的 DML 查询与对源数据库实例的更新冲突。 您可通过查看 Replication Error 字段，检查 MySQL 或 MariaDB 引擎引发的关联错误的详细信息。还生成指示只读副本状态的事件，包括 RDS-EVENT-0045、RDS-EVENT-0046 和 RDS-EVENT-0047。有关这些事件和事件订阅的详细信息，请参阅 使用 Amazon RDS 事件通知。如果返回 MySQL 错误消息，则检查 MySQL 错误消息文档中的错误编号。如果返回 MariaDB 错误消息，则检查 MariaDB 错误消息文档中的错误。 一个可导致复制出错的常见问题是只读副本的 max_allowed_packet 参数的值小于源数据库实例的 max_allowed_packet 参数的值。max_allowed_packet 参数是可在数据库参数组中进行设置的自定义参数，用于指定可在数据库上执行的最大 DML 代码大小。有时候，与源数据库实例关联的数据库参数组中的 max_allowed_packet 参数值，要小于与源的只读副本关联的数据库参数组中的 max_allowed_packet 参数值。在这些情况下，复制过程可能会引发错误 (数据包大于 ‘max_allowed_packet’ 字节) 并停止复制。可通过让源和只读副本使用 max_allowed_packet 参数值相同的数据库参数组来纠正该错误。 按照官网文档的建议尝试，发现并不是以上\b所说问题。必须\b\b深入分析，找到原因。 \b ","date":"2018-05-04","objectID":"/replica-research/:1:3","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"分析 \b是否在延迟报警的时间点有\b大量的写操作 通过代码排查，在这些\b时间点并没有类似的操作 在延迟发生的时间点观察执行状态 主库show PROCESSLIST，并没有很多的写操作同时也印证了代码的排查的结论 从库show PROCESSLIST，看到十分重要的\b信息 发生了\b锁表并且有很多\b写入在排队 观察innodb_trx和innodb_lock 查看是否有大的事务\b拿着锁不放，结果是没有 通过以上信息，判读不是我们自己的写操作造成主从同步延迟 继续寻找线索 \b查看发生延迟时间点的慢查询日志 查看到\b这个时候有很多类似的查询 SELECT /*!40001 SQL_NO_CACHE */ * FROM 先来看这个查询的意义： /*! */ 这是mysql里的语法，表示达到条件会执行相应的语句。 !后面是版本号， 如果本数据库等于或大于此版本号，那么语句会执行。 那么这句话的意思是 如果版本号大于或等于4，会执行 sql_no_cache, 就是不用缓存数据。 而并非说本次查询不作为下次查询的缓存。 在备份操作时Mysql 会自动调用此语法。该语句会查询到表中所有数据，在备份文件中会生成相应的insert语句。 查看\b数据库备份 每隔8小时执行一次 mysqldump -u ${user} -p${password} -h ${host} ${name} \u003e ${filename} 可以看到时间点与主从同步延迟的报警时间完全吻合 ","date":"2018-05-04","objectID":"/replica-research/:1:4","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"原因 通过\b一系列列大胆假设\b和小心求证,可以断定是我们\b在执行自动备份的时候大量锁表，造成很多同步的写入操作被阻塞 mysqldumb 有个很关键的点是为了保证同步后数据的一致性，\b\b会采用 lock-tables, lock-all-tables, single-transaction不同的机制。 lock-tables: 默认会选择这个机制，把所有需要dump的表都会加锁，当然写入表的操作就被阻塞了 lock-all-tables: dump\b期间锁定所有数据库中的所有表，以保证数据的一致性。这是一个全局锁定，并且自动关闭–single-transaction 和–lock-tables 选项。这个参数副作用比较大，这是全库锁定，备份执行过程中，该库无法进行读写操作 single-transaction: 这个机制把事务隔离级别设置为可重复读，在dump之前提交一个START TRANSACTION语句，不会阻塞任何应用程序且能保证导出时数据库的一致性状态。它只适用于多版本存储引擎(MVCC)，仅InnoDB。本选项和–lock-tables 选项是互斥的，因为LOCK TABLES 会使任何挂起的事务隐含提交，使用参数–single-transaction会自动关闭该选项。 ","date":"2018-05-04","objectID":"/replica-research/:1:5","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"解决方案 因为我们的db引擎是innodb, 最简单的方式就是修改mysqldump保证\b数据一致性的机制，采用single-transaction mysqldump -u ${user} -p${password} -h ${host} ${name} --single-transaction --quick \u003e ${filename} single-transaction只适用于innodb, 在innodb导出时会建立一致性快照，在保证导出数据的一致性前提下，又不会堵塞其他会话的读写操作，相比–lock-tables参数来说锁定粒度要低，造成的影响也要小很多。指定这个参数后，其他连接不能执行ALTER TABLE、DROP TABLE 、RENAME TABLE、TRUNCATE TABLE这类语句，事务的隔离级别无法控制DDL(Data Definition Languages)语句。所以改\b表的时候，要注意查看是否有数据库备份执行 \b再起一个从库，\b单独在此从库上执行备份 因为数据库备份的时候占用不少资源，而同时我们的从库也有大量的读的操作，会互相影响。\b 不用mysqldump选择别的备份方案, 比如XtraBackup 下图是\b\b执行解决方案后从库延迟\b的曲线图，可以看到之前定时出现的主从延迟(心头大患)已经消失了(开心.jpg) \b\b\b主要参考：https://dev.mysql.com/doc/refman/5.6/en/replication.html。 ","date":"2018-05-04","objectID":"/replica-research/:1:6","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"总结 多学习多思考，\b对数据库的操作包括基本的增删改查\b要有敬畏之心。 附加部分 ","date":"2018-05-04","objectID":"/replica-research/:1:7","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"数据库查询优化 数据库主从同步延迟报警的时间点，\b还有主库CPU和连接数报警，在一开始因为时间点基本一致，而\b认为两者一定时相关的，误导了我对根本原因的判读。\b更重要的是针对这种报警必须优化。下面就是一个优化的例子。 ","date":"2018-05-04","objectID":"/replica-research/:2:0","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"1. 分析原因 根据数据库报警信息 CPU和connections超过阈值报警，找到对应时间点的慢查询 根据报警时间点，\b找到\b对应的定时任务 根据时间，\b\b查\b看慢查询日志 AWS RDS开启slow_query_log，设置三个关键参数: slow_query_log（\b1）, long_query_time（比如10秒）, log_output（file）\b 设置完成后可以用show variables like '参数名称'来查看是否设置成功 # Time: 180504 0:28:20 # User@Host: xxx(脱敏) Id: 164478982 # Query_time: 3.542192 Lock_time: 0.000056 Rows_sent: 1 Rows_examined: 76281 SET timestamp=1525393700; SELECT COUNT(*) AS `__count` FROM `core_membership` WHERE (`core_membership`.`business_group_id` = 2908 AND `core_membership`.`last_visited_at` \u003e '2018-05-04 23:59:59.999999' AND `core_membership`.`last_visited_at` \u003c= '2018-02-03 23:59:59.999999'); # 这个慢查询必须优化 # Time: 180504 0:27:31 # User@Host: xxx(脱敏) Id: 164477983 # Query_time: 45.261294 Lock_time: 0.000043 Rows_sent: 1 Rows_examined: 8583 SET timestamp=1525393651; SELECT COUNT(*) AS `__count` FROM `core_membership` WHERE (`core_membership`.`business_group_id` = 1976 AND `core_membership`.`level` = 1); 慢查询和\b应用中的定时任务要执行的\b查询完全一致 ","date":"2018-05-04","objectID":"/replica-research/:2:1","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"2. 优化 分析慢查询日志确定拖垮db的\b根本原因是上面的第二条查询 通过explain分析\b上面的查询语句，虽然用到了索引，但依然很慢 原因：level为1的会员占了绝大多数，\b即使用了索引，这样的查询扫索引依然很多行，所以慢 下面的执行结果非常明显 sql\u003e SELECT COUNT(*) AS `__count` FROM `core_membership` WHERE (`core_membership`.`business_group_id` = 2708 AND `core_membership`.`level` = 1) [2018-05-04 13:16:31] 1 row retrieved starting from 1 in 2s 987ms (execution: 2s 964ms, fetching: 23ms) sql\u003e SELECT COUNT(*) AS `__count` FROM `core_membership` WHERE (`core_membership`.`business_group_id` = 2708 AND `core_membership`.`level` \u003e 1) [2018-05-04 13:17:16] 1 row retrieved starting from 1 in 598ms (execution: 587ms, fetching: 11ms) 优化方案：\blevel大于1的会员为少数，最简单有效的方法就是只查询高等级的会员，这样\b查询索引的时候\b自然就快。然后再用总数跟高等级的人数做减法，来避免level 1的查询 下图是优化后的主库CPU占用曲线\b图，可以看到优化后峰值明显减少 图上剩下的峰值就是下一步的优化对象。 \b更新时间：2018-05-15 ","date":"2018-05-04","objectID":"/replica-research/:2:2","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"死锁排查 ","date":"2018-05-04","objectID":"/replica-research/:3:0","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"问题发现 sentry上出现报错 An error occurred in the current transaction. You can't execute queries until the end of the 'atomic' block. 具体查看就是 'Deadlock found when trying to get lock; try restarting transaction' 问题十分明显，并发的情况下多个事务出现了**死锁* 这里对于 mysql 的死锁及相关概念就不展开，类似的介绍网上很多。 ","date":"2018-05-04","objectID":"/replica-research/:3:1","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"排查原因 查看死锁信息的重要命令 SHOW ENGINE INNODB STATUS; 此命令还展示了很多其他信息，比如外键错误，这里就省略，只列出死锁相关的重要信息 ------------------------ LATEST DETECTED DEADLOCK ------------------------ 2018-05-15 02:53:14 2b2b78d00700 *** (1) TRANSACTION: TRANSACTION 5052291242, ACTIVE 26 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 10 lock struct(s), heap size 1184, 3 row lock(s), undo log entries 3 MySQL thread id 197230223, OS thread handle 0x2b3239e46700, query id xxx(脱敏） update INSERT INTO `servant_servantorder` (`uid`, `visit_id`, `dish_id`, `account_id`, `online_order_id`, `name`, `unit_price`, `member_price`, `amount`, `unit`, `main_category`, `discountable`, `created_at`, `updated_at`, `deactivated_at`) VALUES ('7064c4991bd24f71bcf8b65367eff21d', 1475092, 65990, NULL, NULL, '小毛巾', '1.50', '1.50', '10.00', '条', '一次性用品', 0, '2018-05-15 10:52:48.496575', '2018-05-15 10:52:48.496827', NULL) *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 1361 page no 677 n bits 160 index `PRIMARY` of table `xxx`.`servant_servantdish` trx id 5052291242 lock mode S locks rec but not gap waiting Record lock, heap no 83 PHYSICAL RECORD: n_fields 29; compact format; info bits 0 0: len 4; hex 800101c6; asc ;; 1: len 6; hex 00012d23d88e; asc -# ;; ... *** (2) TRANSACTION: TRANSACTION 5052291214, ACTIVE 26 sec inserting mysql tables in use 1, locked 1 57 lock struct(s), heap size 6544, 4113 row lock(s), undo log entries 4541 MySQL thread id 197230216, OS thread handle 0x2b2b78d00700, query id xxx(脱敏 update INSERT INTO `servant_servantdish` (`uid`, `servant_id`, `code`, `name`, `unit_price`, `member_price`, `minimum_amount`, `cost`, `unit`, `size`, `english_name`, `description`, `image`, `main_category`, `sub_category`, `sales_category`, `available`, `discountable`, `minimum_chargeable`, `in_dish_set`, `can_order`, `erasable`, `created_at`, `updated_at`, `deactivated_at`, `json_data`) VALUES ('8a42e8b35d1841818d53eb1c80035fb2', 161, '722', '农家红烧杂鱼', '48.00', '48.00', '0.00', '0.00', '例', '', 'NJHSZY', '', NULL, '海鲜鱼类', '鱼类', '加工成品', 1, 0, 1, 0, 0, 0, '2018-05-15 10:53:14.148526', '2018-05-15 10:53:14.148801', NULL, NULL) *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 1361 page no 677 n bits 168 index `PRIMARY` of table `xxx`.`servant_servantdish` trx id 5052291214 lock_mode X locks rec but not gap Record lock, heap no 14 PHYSICAL RECORD: n_fields 29; compact format; info bits 0 0: len 4; hex 80010180; asc ;; 1: len 6; hex 00012d23d88e; asc -# ;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 907 page no 14 n bits 112 index `PRIMARY` of table `xxx`.`servant_servant` trx id 5052291214 lock mode S locks rec but not gap waiting Record lock, heap no 13 PHYSICAL RECORD: n_fields 10; compact format; info bits 0 0: len 4; hex 800000a1; asc ;; 1: len 6; hex 00012d23d8aa; asc -# ;; 2: len 7; hex 6e00002bcf0c02; asc n + ;; 3: len 8; hex 999fdead2e091650; asc . P;; 4: len 4; hex 80001874; asc t;; 5: len 8; hex 999fdead300729c8; asc 0 ) ;; 6: SQL NULL; 7: len 4; hex 80000052; asc R;; 8: len 4; hex 80000052; asc R;; 9: len 30; hex 7b2276656e646f72223a20227b5c22446973706c61794e616d655c223a5c; asc {\"vendor\": \"{\\\"DisplayName\\\":\\; (total 538 bytes); ... *** WE ROLL BACK TRANSACTION (1) ","date":"2018-05-04","objectID":"/replica-research/:3:2","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"分析日志 通过分析第一部分，可以看到事务1在往表 servantorder 里插入一条记录，在等待servantdish主键(page 677)的 S 锁(共享锁) *** (1) TRANSACTION: TRANSACTION 5052291242, ACTIVE 26 sec inserting mysql tables in use 1, locked 1 LOCK WAIT 10 lock struct(s), heap size 1184, 3 row lock(s), undo log entries 3 MySQL thread id 197230223, OS thread handle 0x2b3239e46700, query id xxx(脱敏）update INSERT INTO `servant_servantorder` (`uid`, `visit_id`, `dish_id`, `account_id`, `online_order_id`, `name`, `unit_price`, `member_price`, `amount`, `unit`, `main_category`, `discountable`, `created_at`, `updated_at`, `deactivated_at`) VALUES ('7064c4991bd24f71bcf8b65367eff21d', 1475092, 65990, NULL, NULL, '小毛巾', '1.50', '1.50', '10.00', '条', '一次性用品', 0, '2018-05-15 10:52:48.496575', '2018-05-15 10:52:48.496827', NULL) *** (1) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 1361 page no 677 n bits 160 index `PRIMARY` of table `zaihui`.`servant_servantdish` trx id 5052291242 lock mode S locks rec but not gap waiting 通过分析第二部分可以看到事务2持有 servantdish(page 677）的 X 锁(排它锁)，正是事务1在等待的锁。而同时事务2还在等待servant(page 14)的 S 锁 *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 1361 page no 677 n bits 168 index `PRIMARY` of table `xxx`.`servant_servantdish` trx id 5052291214 lock_mode X locks rec but not gap Record lock, heap no 14 PHYSICAL RECORD: n_fields 29; compact format; info bits 0 0: len 4; hex 80010180; asc ;; 1: len 6; hex 00012d23d88e; asc -# ;; *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 907 page no 14 n bits 112 index `PRIMARY` of table `xxx`.`servant_servant` trx id 5052291214 lock mode S locks rec but not gap waiting 确实发生了死锁，但其实我们从日志获得的信息还不够。因为第一部分并没有明确显示，事务1持有servant的锁。实际上show engine innodb status并不会显示事务的全部信息。所以需要去发生死锁的业务逻辑里找到证据。 # 事务1 with transaction.atomic(): # 获取 servant 的 X 锁 servant.last_api_call = datetime.datetime.now() servant.save() 最后因为mysql检测到了死锁, 事务1权重较小，被回滚。 下面通过一个表格的形式，梳理一下整个流程。 步骤 事务1 事务2 1 begin 2 INSERT INTO servant_servantdish (…) VALUES (…); 持有servantdish的 X 锁 3 begin 4 更新servant, 获得 servant的 X 锁 5 \b\u001d查询servant, 等待servant的 S 锁 6 INSERT INTO servant_servantorder (…) VALUES (…); 等待servant_dish的 S\b 锁 7 事务1和事务2互相等待对方的\b释放锁，发生死锁 8 监测到死锁，\b权重较小的事务1被回滚 ","date":"2018-05-04","objectID":"/replica-research/:3:3","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"总结 发生死锁时，一定要及时排查死锁原因，从业务层面进行解决。首先根据死锁信息分析循环等待的场景，然后根据当前各个事务执行的SQL分析出加锁类型以及顺序，逆向推断出如何形成循环等待，\b必要时要\b具体的查看业务层的代码，看具体执行了哪些操作，这样就能找到死锁产生的原因了。 ","date":"2018-05-04","objectID":"/replica-research/:3:4","tags":["mysql","RDS"],"title":"MySQL主从同步延迟调查","uri":"/replica-research/"},{"categories":null,"content":"Sentry是我们查看线上bug最适用和好用的工具。\b因为sentry能帮我们完整记录bug\b或异常的上下文，能有效帮助我们定位和分析问题，查看方便并且具有通知功能，受到我们开发者\b的热烈拥护。具体\b在Django中使用sentry还是比较简单的，官方文档清晰明了。 在我们的项目中，\b只要是Exception都能被sentry捕获并且发送至sentry服务器，那么代码中抛出的\bException是怎么被sentry捕获到的呢？ \b这篇小记就是分析记录sentry的客户端raven在Django中\u001d的工作原理。 在我们的项目中包含两类Exception 意料之中的\b或者说已知的Exception, 即try, except\b\b捕获的 未知的Exception,即\bbug或未知的异常 ","date":"2018-04-13","objectID":"/analyse-sentry-capture-md/:0:0","tags":["Django","后端","sentry"],"title":"Django中sentry怎么抓取异常","uri":"/analyse-sentry-capture-md/"},{"categories":null,"content":"\b\b捕获\b已知的异常 对于已知的并且需要记录的Exception，我们一般会调用python自带日志模块logging处理，此时raven就发挥作用了 代码如下 # 代码位置 raven.handlers.logging.py # SentryHandler继承自loggging.Handler,并且覆写了emit()方法 class SentryHandler(logging.Handler, object): def __init__(self, *args, **kwargs): client = kwargs.get('client_cls', Client) if len(args) == 1: arg = args[0] if isinstance(arg, string_types): self.client = client(dsn=arg, **kwargs) elif isinstance(arg, Client): self.client = arg else: raise ValueError('The first argument to %s must be either a ' 'Client instance or a DSN, got %r instead.' % (self.__class__.__name__, arg,)) elif 'client' in kwargs: self.client = kwargs['client'] else: self.client = client(*args, **kwargs) self.tags = kwargs.pop('tags', None) logging.Handler.__init__(self, level=kwargs.get('level', logging.NOTSET)) def emit(self, record): # 其他部分略去，只列出关键流程 ... self._emit(record) def _emit(record, **kwargs): ... # 这个\b方法里sentry会把拿到的\bexception进行处理（build_msg方法）并且发送到服务器（send方法） return self.client.capture( event_type, stack=stack, data=data, extra=extra, date=date, sample_rate=sample_rate, **kwargs) 一句话就是说当我们用logging日志模块处理异常时, 只要我们正确配置了sentry，\b改异常同时会被raven捕获进行处理并发送到sentry服务器。 ","date":"2018-04-13","objectID":"/analyse-sentry-capture-md/:1:0","tags":["Django","后端","sentry"],"title":"Django中sentry怎么抓取异常","uri":"/analyse-sentry-capture-md/"},{"categories":null,"content":"捕获未知的异常 代码中的bug,\b 我们没法做预处理, 所以也就不可能用logging模块提前去记录。而raven\b捕获处理这种异常的流程如下: 1. 在django工程初识化时，会调用\binstall()起一个Signal实例\b绑定raven的异常处理handler # \b代码位置 raven.contrib.django.models.py def install(self): request_started.connect(self.before_request, weak=False) # \bSignal实例绑定一个\breceiver(self.exception_handler) got_request_exception.connect(self.exception_handler, weak=False) if self.has_celery: try: self.install_celery() except Exception: logger.exception('Failed to install Celery error handler') 2.在业务中未被捕获的异常在Django处理请求的入口BaseHandler也是请求返回的出口会被最终捕获，在这里会通过signal发送给receiver # 代码位置django.core.handlers.base.py class BaseHandler(object): ... def get_response(self, request): ... # Signal实例把异常发送给receiver signals.got_request_exception.send(sender=self.__class__, request=request) response = self.handle_uncaught_exception(request, resolver, sys.exc_info()) 3. receiver接收到异常，处理异常并发送到sentry\b服务器 # 代码位置raven.contrib.django.models.py # raven的receiver def exception_handler(self, request=None, **kwargs): try: # 处理异常 self.client.captureException(exc_info=sys.exc_info(), request=request) except Exception as exc: try: logger.exception('Unable to process log entry: %s' % (exc,)) except Exception as exc: warnings.warn('Unable to process log entry: %s' % (exc,)) # 代码位置raven.base.py def capture(self, event_type, data=None, date=None, time_spent=None, extra=None, stack=None, tags=None, sample_rate=None, **kwargs): if not self.is_enabled(): return exc_info = kwargs.get('exc_info') if exc_info is not None: if self.skip_error_for_logging(exc_info): return elif not self.should_capture(exc_info): self.logger.info( 'Not capturing exception due to filters: %s', exc_info[0], exc_info=sys.exc_info()) return self.record_exception_seen(exc_info) # \b\b组装拼接\b异常信息,构造成一个dict data = self.build_msg( event_type, data, date, time_spent, extra, stack, tags=tags, **kwargs) # should this event be sampled? if sample_rate is None: sample_rate = self.sample_rate # 序列化，编码,压缩信息并发送到服务器器 if self._random.random() \u003c sample_rate: self.send(**data) self._local_state.last_event_id = data['event_id'] return data['event_id'] 上面就是raven处理\b未知异常的主流程。\b一句话总结: Exception–\u003e (通过Signal) –\u003e exception_handler –\u003e capture –\u003e build\u0026send ","date":"2018-04-13","objectID":"/analyse-sentry-capture-md/:2:0","tags":["Django","后端","sentry"],"title":"Django中sentry怎么抓取异常","uri":"/analyse-sentry-capture-md/"},{"categories":null,"content":"Sentry是我们查看线上bug最适用和好用的工具。\b因为sentry能帮我们完整记录bug\b或异常的上下文，能有效帮助我们定位和分析问题，查看方便并且具有通知功能，受到我们开发者\b的热烈拥护。具体\b在Django中使用sentry还是比较简单的，官方文档清晰明了。 在我们的项目中，\b只要是Exception都能被sentry捕获并且发送至sentry服务器，那么代码中抛出的\bException是怎么被sentry捕获到的呢？ \b这篇小记就是分析记录sentry的客户端raven在Django中\u001d的工作原理。 在我们的项目中包含两类Exception 意料之中的\b或者说已知的Exception, 即try, except\b\b捕获的 未知的Exception,即\bbug或未知的异常 ","date":"2018-04-13","objectID":"/analyse-sentry-capture-md/:0:0","tags":["Django","后端","sentry"],"title":"Django中sentry怎么抓取异常","uri":"/analyse-sentry-capture-md/"},{"categories":null,"content":"\b\b捕获\b已知的异常 对于已知的并且需要记录的Exception，我们一般会调用python自带日志模块logging处理，此时raven就发挥作用了 代码如下 # 代码位置 raven.handlers.logging.py # SentryHandler继承自loggging.Handler,并且覆写了emit()方法 class SentryHandler(logging.Handler, object): def __init__(self, *args, **kwargs): client = kwargs.get('client_cls', Client) if len(args) == 1: arg = args[0] if isinstance(arg, string_types): self.client = client(dsn=arg, **kwargs) elif isinstance(arg, Client): self.client = arg else: raise ValueError('The first argument to %s must be either a ' 'Client instance or a DSN, got %r instead.' % (self.__class__.__name__, arg,)) elif 'client' in kwargs: self.client = kwargs['client'] else: self.client = client(*args, **kwargs) self.tags = kwargs.pop('tags', None) logging.Handler.__init__(self, level=kwargs.get('level', logging.NOTSET)) def emit(self, record): # 其他部分略去，只列出关键流程 ... self._emit(record) def _emit(record, **kwargs): ... # 这个\b方法里sentry会把拿到的\bexception进行处理（build_msg方法）并且发送到服务器（send方法） return self.client.capture( event_type, stack=stack, data=data, extra=extra, date=date, sample_rate=sample_rate, **kwargs) 一句话就是说当我们用logging日志模块处理异常时, 只要我们正确配置了sentry，\b改异常同时会被raven捕获进行处理并发送到sentry服务器。 ","date":"2018-04-13","objectID":"/analyse-sentry-capture-md/:1:0","tags":["Django","后端","sentry"],"title":"Django中sentry怎么抓取异常","uri":"/analyse-sentry-capture-md/"},{"categories":null,"content":"捕获未知的异常 代码中的bug,\b 我们没法做预处理, 所以也就不可能用logging模块提前去记录。而raven\b捕获处理这种异常的流程如下: 1. 在django工程初识化时，会调用\binstall()起一个Signal实例\b绑定raven的异常处理handler # \b代码位置 raven.contrib.django.models.py def install(self): request_started.connect(self.before_request, weak=False) # \bSignal实例绑定一个\breceiver(self.exception_handler) got_request_exception.connect(self.exception_handler, weak=False) if self.has_celery: try: self.install_celery() except Exception: logger.exception('Failed to install Celery error handler') 2.在业务中未被捕获的异常在Django处理请求的入口BaseHandler也是请求返回的出口会被最终捕获，在这里会通过signal发送给receiver # 代码位置django.core.handlers.base.py class BaseHandler(object): ... def get_response(self, request): ... # Signal实例把异常发送给receiver signals.got_request_exception.send(sender=self.__class__, request=request) response = self.handle_uncaught_exception(request, resolver, sys.exc_info()) 3. receiver接收到异常，处理异常并发送到sentry\b服务器 # 代码位置raven.contrib.django.models.py # raven的receiver def exception_handler(self, request=None, **kwargs): try: # 处理异常 self.client.captureException(exc_info=sys.exc_info(), request=request) except Exception as exc: try: logger.exception('Unable to process log entry: %s' % (exc,)) except Exception as exc: warnings.warn('Unable to process log entry: %s' % (exc,)) # 代码位置raven.base.py def capture(self, event_type, data=None, date=None, time_spent=None, extra=None, stack=None, tags=None, sample_rate=None, **kwargs): if not self.is_enabled(): return exc_info = kwargs.get('exc_info') if exc_info is not None: if self.skip_error_for_logging(exc_info): return elif not self.should_capture(exc_info): self.logger.info( 'Not capturing exception due to filters: %s', exc_info[0], exc_info=sys.exc_info()) return self.record_exception_seen(exc_info) # \b\b组装拼接\b异常信息,构造成一个dict data = self.build_msg( event_type, data, date, time_spent, extra, stack, tags=tags, **kwargs) # should this event be sampled? if sample_rate is None: sample_rate = self.sample_rate # 序列化，编码,压缩信息并发送到服务器器 if self._random.random() \u003c sample_rate: self.send(**data) self._local_state.last_event_id = data['event_id'] return data['event_id'] 上面就是raven处理\b未知异常的主流程。\b一句话总结: Exception–\u003e (通过Signal) –\u003e exception_handler –\u003e capture –\u003e build\u0026send ","date":"2018-04-13","objectID":"/analyse-sentry-capture-md/:2:0","tags":["Django","后端","sentry"],"title":"Django中sentry怎么抓取异常","uri":"/analyse-sentry-capture-md/"},{"categories":null,"content":"需求 说起来都是泪。\b平时工作生活中习惯不好，脏话有点多了，比如 卧槽，沙比这种。虽然我认为这样的词在说的时候完全只是当语气词来用，但毕竟是脏话。所以，\b女朋友受不了，必须要纠正这样的恶习（她也会说脏话）。她说得记录我们说的次数，然后统计出来，我说好啊，每个月还要按照统计数发红包。于是，在这样的背景下我就想那就写个\b页面记录和展示吧。转念一下，这样\b需要女朋友这样的小白用户输入网址，\b\b提交表单不是太友好，于是想到了她经常用微信那不如搞一个微信公众号，除了这个功能还可以添加其他功能。\bSo\b, 需求定下来了~~ ","date":"2018-03-12","objectID":"/build-wechat-service/:1:0","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"实现 那么按照需求（文档）拆解了一下工作： 1. 本地开发，测试 2. 申请公众号 3. \b\b部署服务 4. 调试运行 5. 优化流\b程 ","date":"2018-03-12","objectID":"/build-wechat-service/:2:0","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"本地开发 web框架选用Tornado，因为Tornado本身就是 Web server + Web application 框架，并且自带异步库和协程库。而\b我选用\bDjango或Flask这样的web 框架，还需要搭建\bweb server比如uWSGI, gunicorn。 \b 存储用\bMongoDB和Redis 用Docker部署(轻松的迁移，维护和部署) Dockerfile示例： FROM python:3.6 EXPOSE 8888 ENV TZ Asia/Shanghai RUN echo $TZ \u003e /etc/timezone \u0026\u0026 \\ apt-get update \u0026\u0026 apt-get install -y tzdata \u0026\u0026 \\ rm /etc/localtime \u0026\u0026 \\ ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026\u0026 \\ dpkg-reconfigure -f noninteractive tzdata \u0026\u0026 \\ apt-get clean # ENV PIP_INDEX_URL http://mirrors.aliyun.com/pypi/simple # ENV PIP_TRUSTED_HOST mirrors.aliyun.com RUN mkdir -p /var/www/lxkaka WORKDIR /var/www/lxkaka COPY requirements.txt /var/www/lxkaka RUN pip install -r requirements.txt COPY ./lxkaka/ /var/www/lxkaka/ CMD python server.py 服务器相关配置统一写到server.conf，示例： port=8888 APPID = \"wxxxxxxxxxxxxx\" TOKEN = \"lxkaka\" AES_KEY = \"121234nmngvlkghkhlghghh\" Tornado服务启动示例: # app启动和入口 class Application(tornado.web.Application): def __init__(self): handlers = [ (r'/debug/', DebugHandler), ] settings = dict( blog_title='lxkaka', xsrf_cookies=False, cookie_secret='', login_url='/auth/login', debug=True, ) super(Application, self).__init__(handlers, **settings) def main(): tornado.options.parse_config_file('server.conf') tornado.options.parse_command_line() http_server = tornado.httpserver.HTTPServer(Application()) http_server.listen(options.port) tornado.ioloop.IOLoop.current().start() if __name__ == '__main__': main() # 接口处理 class DebugHandler(tornado.web.RequestHandler): async def get(self, *args, **kwargs): data = { 'APPID': options.APPID, 'TOKEN': options.TOKEN, 'author': 'lxkaka', } self.finish(json.dumps(data, ensure_ascii=False)) ","date":"2018-03-12","objectID":"/build-wechat-service/:2:1","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"申请公众号 https://mp.weixin.qq.com/ 公众号关键配置 ","date":"2018-03-12","objectID":"/build-wechat-service/:2:2","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"服务部署 方案1 \b利用FTP推送代码 配置简单，直接可利用IDE的deploy功能一键发布代\b码 安装vsftpd sudo apt-get install vsftpd 创建FTP用户 为FTP创建特定用户，一是为了避免匿名用户登录，二是禁止用户越权访问其它文件目录内容。 # 新建用户以及对应的用户目录，禁止登录系统 sudo useradd -d /home/[username] -s /sbin/nologin -m [username] # 更改密码 sudo passwd [username] # 调整权限 sudo chmod a-w /home/[username] # 创建data目录，避免500错误。 # data目录为ftp上传目录 sudo mkdir /home/[username]/data sudo chown -R [username]:[username] /home/[username]/data ftp配置文件 vsftpd的配置文件在/etc/vsftpd.conf 关键配置修改如下： # Allow anonymous FTP? (Disabled by default). anonymous_enable=NO # \b配置文件末尾添加 userlist_deny=NO userlist_enable=YES userlist_file=/etc/vsftpd.allowed_users 创建文件 /etc/vsftpd.allowed_users，写入用户名 # 创建并写入用户名 vim /etc/vsftpd.allowed_users # 重启ftp服务 sudo service vsftpd restart 客户端配置 方案2 用git+git-hook发布和自动部署 搭建git服务器 # 新建远程仓库文件夹 mkdir /home/lxkaka/server-repo # 初始化远程仓库 git init --bare # 在本地repo添加origin 地址 git remote add origin git@ip(或者host):/home/lxkaka/server-repo 至此本地开发测试完成，可直接push代码到远程仓库 配置git hook 在/home/lxkaka/servr-repo/hooks下面存在许多hook的\b例子，\b配置post-update \bhook就能满足需求 在此目录下 vim post-update \b# \b写入以下脚本 #!/bin/sh # # An example hook script to prepare a packed repository for use over # dumb transports. # # To enable this hook, rename this file to \"post-update\". unset GIT_DIR # \b\b代码库 DIR=/home/lxkaka/server/ cd $DIR # \b更新代码 git pull origin master docker-compose -f docker-compose.yml up --build -d 查看 post-update\b是否有x权限, 没有添加 chmod +x post-update ","date":"2018-03-12","objectID":"/build-wechat-service/:2:3","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"\b\b用docker-compose管理多个容器应用 这里有web服务容器，redis容器，mongo容器。通过\b编写docker-compose.yml来关联不同的容器为一个项目(project)。 Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 docker-compose.yml示例如下： mongo: restart: always image: mongo:3.4.9 volumes: - ./data/db:/data/db redis: restart: always image: redis:4.0.2 lxkaka: restart: always build: . links: - mongo - redis environment: - PIP_INDEX_URL=http://mirrors.aliyun.com/pypi/simple - PIP_TRUSTED_HOST=mirrors.aliyun.com ports: - \"8888:8888\" ","date":"2018-03-12","objectID":"/build-wechat-service/:2:4","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"Nginx配置 最重要的是虚拟主机部分的配置，\b这里有三种服务，通过不同的location\b匹配到不同的服服务。 server { listen 80; server_name www.lxkaka.wang lxkaka.wang; charset utf-8; root /home/lxkaka/blog; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; location = / { root /home/lxkaka/blog; index index.html index.htm; autoindex on; } # pass the gallery request to node server listening on 127.0.0.1:3000 location /gallery { proxy_pass http://127.0.0.1:3000/; } listen 443 ssl http2; # managed by Certbot ssl_certificate /etc/letsencrypt/live/lxkaka.wang/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/lxkaka.wang/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot # Allow file uploads client_max_body_size 50M; location ^~ /api/ { proxy_pass_header Server; proxy_set_header Host $http_host; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://127.0.0.1:8888/; } } ","date":"2018-03-12","objectID":"/build-wechat-service/:2:5","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"需求 说起来都是泪。\b平时工作生活中习惯不好，脏话有点多了，比如 卧槽，沙比这种。虽然我认为这样的词在说的时候完全只是当语气词来用，但毕竟是脏话。所以，\b女朋友受不了，必须要纠正这样的恶习（她也会说脏话）。她说得记录我们说的次数，然后统计出来，我说好啊，每个月还要按照统计数发红包。于是，在这样的背景下我就想那就写个\b页面记录和展示吧。转念一下，这样\b需要女朋友这样的小白用户输入网址，\b\b提交表单不是太友好，于是想到了她经常用微信那不如搞一个微信公众号，除了这个功能还可以添加其他功能。\bSo\b, 需求定下来了~~ ","date":"2018-03-12","objectID":"/build-wechat-service/:1:0","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"实现 那么按照需求（文档）拆解了一下工作： 1. 本地开发，测试 2. 申请公众号 3. \b\b部署服务 4. 调试运行 5. 优化流\b程 ","date":"2018-03-12","objectID":"/build-wechat-service/:2:0","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"本地开发 web框架选用Tornado，因为Tornado本身就是 Web server + Web application 框架，并且自带异步库和协程库。而\b我选用\bDjango或Flask这样的web 框架，还需要搭建\bweb server比如uWSGI, gunicorn。 \b 存储用\bMongoDB和Redis 用Docker部署(轻松的迁移，维护和部署) Dockerfile示例： FROM python:3.6 EXPOSE 8888 ENV TZ Asia/Shanghai RUN echo $TZ \u003e /etc/timezone \u0026\u0026 \\ apt-get update \u0026\u0026 apt-get install -y tzdata \u0026\u0026 \\ rm /etc/localtime \u0026\u0026 \\ ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026\u0026 \\ dpkg-reconfigure -f noninteractive tzdata \u0026\u0026 \\ apt-get clean # ENV PIP_INDEX_URL http://mirrors.aliyun.com/pypi/simple # ENV PIP_TRUSTED_HOST mirrors.aliyun.com RUN mkdir -p /var/www/lxkaka WORKDIR /var/www/lxkaka COPY requirements.txt /var/www/lxkaka RUN pip install -r requirements.txt COPY ./lxkaka/ /var/www/lxkaka/ CMD python server.py 服务器相关配置统一写到server.conf，示例： port=8888 APPID = \"wxxxxxxxxxxxxx\" TOKEN = \"lxkaka\" AES_KEY = \"121234nmngvlkghkhlghghh\" Tornado服务启动示例: # app启动和入口 class Application(tornado.web.Application): def __init__(self): handlers = [ (r'/debug/', DebugHandler), ] settings = dict( blog_title='lxkaka', xsrf_cookies=False, cookie_secret='', login_url='/auth/login', debug=True, ) super(Application, self).__init__(handlers, **settings) def main(): tornado.options.parse_config_file('server.conf') tornado.options.parse_command_line() http_server = tornado.httpserver.HTTPServer(Application()) http_server.listen(options.port) tornado.ioloop.IOLoop.current().start() if __name__ == '__main__': main() # 接口处理 class DebugHandler(tornado.web.RequestHandler): async def get(self, *args, **kwargs): data = { 'APPID': options.APPID, 'TOKEN': options.TOKEN, 'author': 'lxkaka', } self.finish(json.dumps(data, ensure_ascii=False)) ","date":"2018-03-12","objectID":"/build-wechat-service/:2:1","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"申请公众号 https://mp.weixin.qq.com/ 公众号关键配置 ","date":"2018-03-12","objectID":"/build-wechat-service/:2:2","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"服务部署 方案1 \b利用FTP推送代码 配置简单，直接可利用IDE的deploy功能一键发布代\b码 安装vsftpd sudo apt-get install vsftpd 创建FTP用户 为FTP创建特定用户，一是为了避免匿名用户登录，二是禁止用户越权访问其它文件目录内容。 # 新建用户以及对应的用户目录，禁止登录系统 sudo useradd -d /home/[username] -s /sbin/nologin -m [username] # 更改密码 sudo passwd [username] # 调整权限 sudo chmod a-w /home/[username] # 创建data目录，避免500错误。 # data目录为ftp上传目录 sudo mkdir /home/[username]/data sudo chown -R [username]:[username] /home/[username]/data ftp配置文件 vsftpd的配置文件在/etc/vsftpd.conf 关键配置修改如下： # Allow anonymous FTP? (Disabled by default). anonymous_enable=NO # \b配置文件末尾添加 userlist_deny=NO userlist_enable=YES userlist_file=/etc/vsftpd.allowed_users 创建文件 /etc/vsftpd.allowed_users，写入用户名 # 创建并写入用户名 vim /etc/vsftpd.allowed_users # 重启ftp服务 sudo service vsftpd restart 客户端配置 方案2 用git+git-hook发布和自动部署 搭建git服务器 # 新建远程仓库文件夹 mkdir /home/lxkaka/server-repo # 初始化远程仓库 git init --bare # 在本地repo添加origin 地址 git remote add origin git@ip(或者host):/home/lxkaka/server-repo 至此本地开发测试完成，可直接push代码到远程仓库 配置git hook 在/home/lxkaka/servr-repo/hooks下面存在许多hook的\b例子，\b配置post-update \bhook就能满足需求 在此目录下 vim post-update \b# \b写入以下脚本 #!/bin/sh # # An example hook script to prepare a packed repository for use over # dumb transports. # # To enable this hook, rename this file to \"post-update\". unset GIT_DIR # \b\b代码库 DIR=/home/lxkaka/server/ cd $DIR # \b更新代码 git pull origin master docker-compose -f docker-compose.yml up --build -d 查看 post-update\b是否有x权限, 没有添加 chmod +x post-update ","date":"2018-03-12","objectID":"/build-wechat-service/:2:3","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"\b\b用docker-compose管理多个容器应用 这里有web服务容器，redis容器，mongo容器。通过\b编写docker-compose.yml来关联不同的容器为一个项目(project)。 Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 docker-compose.yml示例如下： mongo: restart: always image: mongo:3.4.9 volumes: - ./data/db:/data/db redis: restart: always image: redis:4.0.2 lxkaka: restart: always build: . links: - mongo - redis environment: - PIP_INDEX_URL=http://mirrors.aliyun.com/pypi/simple - PIP_TRUSTED_HOST=mirrors.aliyun.com ports: - \"8888:8888\" ","date":"2018-03-12","objectID":"/build-wechat-service/:2:4","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"Nginx配置 最重要的是虚拟主机部分的配置，\b这里有三种服务，通过不同的location\b匹配到不同的服服务。 server { listen 80; server_name www.lxkaka.wang lxkaka.wang; charset utf-8; root /home/lxkaka/blog; # Add index.php to the list if you are using PHP index index.html index.htm index.nginx-debian.html; location = / { root /home/lxkaka/blog; index index.html index.htm; autoindex on; } # pass the gallery request to node server listening on 127.0.0.1:3000 location /gallery { proxy_pass http://127.0.0.1:3000/; } listen 443 ssl http2; # managed by Certbot ssl_certificate /etc/letsencrypt/live/lxkaka.wang/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/lxkaka.wang/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot # Allow file uploads client_max_body_size 50M; location ^~ /api/ { proxy_pass_header Server; proxy_set_header Host $http_host; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://127.0.0.1:8888/; } } ","date":"2018-03-12","objectID":"/build-wechat-service/:2:5","tags":["backend","wechat","deploy"],"title":"搭建微信公众号服务小记","uri":"/build-wechat-service/"},{"categories":null,"content":"Redis因为其高性能和易用性在我们后端的服务中发挥了巨大的作用，并且很多重要功能的实现都会依赖redis。除了常用的缓存，还有队列，发布订阅等重要用处。所以redis的服务高可用就显得尤为关键。这里从redis的高可用入手，梳理了一下目前主流的方案。 ","date":"2018-02-09","objectID":"/redis-ha/:0:0","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Redis-Sentinel Redis-Sentinel是Redis官方推荐的高可用性(HA)解决方案，当用Redis做Master-slave的高可用方案时，假如master宕机了，Redis本身(包括它的很多客户端)都没有实现自动进行主备切换，而Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换。 它的主要功能有以下几点 不时地监控redis是否按照预期良好地运行; 如果发现某个redis节点运行出现状况，能够通知另外一个进程(例如它的客户端); 能够进行自动切换。当一个master节点不可用时，能够选举出master的多个slave(如果有超过一个slave的话)中的一个来作为新的master,其它的slave节点会将它所追随的master的地址改为被提升为master的slave的新地址。 ","date":"2018-02-09","objectID":"/redis-ha/:1:0","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Sentinel支持集群 很显然，只使用单个sentinel进程来监控redis集群是不可靠的，当sentinel进程宕掉后(sentinel本身也有单点问题，single-point-of-failure)整个集群系统将无法按照预期的方式运行。所以有必要将sentinel集群，这样有几个好处： 即使有一些sentinel进程宕掉了，依然可以进行redis集群的主备切换； 如果只有一个sentinel进程，如果这个进程运行出错，或者是网络堵塞，那么将无法实现redis集群的主备切换（单点问题）; 如果有多个sentinel，redis的客户端可以随意地连接任意一个sentinel来获得关于redis集群中的信息。 redis-sentinel方案提供了单点的高可用解决方案，但是当数据量和业务量极速增长时，单点的reids不可能无限的纵向扩容（增大内存), 这个时候就需要redis有集群的能力来扛。 redis集群的几种实现方式如下： 客户端分片 优点简单客户端sharding不支持动态增删节点。劣势很大服务端Redis实例群拓扑结构有变化时，每个客户端都需要更新调整。连接不能共享，当应用规模增大时，资源浪费制约优化。一般不采用。 基于代理的分片，如codis和Twemproxy 路由查询， redis-cluster ","date":"2018-02-09","objectID":"/redis-ha/:1:1","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Twemproxy Twemproxy也叫nutcraker，是twtter开源的一个redis和memcache代理服务器程序。redis作为一个高效的缓存服务器，非常具有应用价值。但在用户数据量增大时，需要运行多个redis实例，此时将迫切需要一种工具统一管理多个redis实例，避免在每个客户端管理所有连接带来的不方便和不易维护，Twemproxy即为此目标而生。 ","date":"2018-02-09","objectID":"/redis-ha/:2:0","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"主要工作方式 分布式逻辑和存储引擎分开，逻辑层proxy代理，存储用的是原子redis，当每个层面需要添加删除节点必须重启服务生效（要重新利用散列函数生成KEY分片更新） Proxy无状态，redis数据层有状态的，客户端可以请求任一proxy代理上面，再由其转发至正确的redis节点，该KEY分片算法至某个节点都是预先已经算好的，在proxy配置文件保存着，但是如果更新或者删除节点，又要根据一致性hash重新计算分片，并且重启服务。 一致性hash算法，增减节点需要配置proxy通知新的算法，重启服务 优点： 比较轻，开发简单，对应用几乎透明 历史悠久，方案成熟 缺点： 代理影响性能 无法平滑地扩容/缩容 运维比较困难 proxy单点本身会有性能瓶颈 ","date":"2018-02-09","objectID":"/redis-ha/:2:1","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Codis codis由3大组件构成： codis-server : 修改过源码的redis, 支持slot，扩容迁移等 codis-proxy : 支持多线程，go语言实现的内核 codis Dashboard : 集群管理工具 提供web图形界面管理集群。 集群元数据存在在zookeeper或etcd。(Zookeeper/etcd存放数据路由表和codis-proxy节点的元信息，codis-config发起的命令通过其同步到各个存活的codis-proxy) 提供独立的组件codis-ha负责redis节点主备切换。 基于proxy的codis，客户端对路由表变化无感知。客户端需要从codis dashhoard调用list proxy命令获取所有proxy列表，并根据自身的轮询策略决定访问哪个proxy节点以实现负载均衡。 整个集群分为1024个哈希槽，分片算法位SlotId = crc32(key) % 1024，增减节点不需要重启服务 ","date":"2018-02-09","objectID":"/redis-ha/:3:0","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"主要工作方式 分布式逻辑和存储引擎分开，逻辑层codis-proxy，存储用的是修改过的codis-server,这种好处是proxy层可以自动扩展和收缩，存储层也同样可以，每个层面都可以热插拨 proxy无状态，codis-server分为组间，每个组存在一个主节点（必须有并且只能有一个）和多个从节点。客户端请求都是和proxy链接，链接哪个proxy都一样，然后由它根据zookeeper路由信息转发至正确节点，直接可以定位到正确节点上 优点： 对应用几乎透明 性能比 Twemproxy 好 有图形化界面，扩容容易，运维方便 缺点： 代理影响性能 组件过多，需要很多机器资源，部署比较难 修改了 Redis 代码，导致和官方无法同步，新特性跟进缓慢 ","date":"2018-02-09","objectID":"/redis-ha/:3:1","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Redis Cluster ","date":"2018-02-09","objectID":"/redis-ha/:4:0","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"主要工作方式 分布式的逻辑和存储引擎不分开，即又负责读写操作，又负责集群交互，升级困难，如果代码有bug，集群无法工作 这个结构为无中心的组织，不好把控集群当前的存活状态，客户端可以向任一节点发送请求，再有其重定向正确的节点上。如果在第一次请求和重定向期间cluster拓扑结构改变，则需要再一次或者多次重定向至正确的节点，但是这方面性能可以忽悠不计 整个集群分为16384个哈希槽，分片算法位SlotId = crc16(key) % 16384，增减节点不需要重启服务。Redis 集群通过 Gossip 协议同步节点信息，基本思想是节点之间互相交换信息最终所有节点达到一致。BTW. redis sentinel集群判定redis主节点down掉也是采用gossip协议。关于Gossip参考wiki。 优点： 组件 all-in-box，部署简单 性能最快（没有proxy） 自动故障转移、Slot 迁移中数据可用 官方原生集群方案，更新与支持有保障 缺点： 实践不多 客户端开放成本高（客户端不够成熟） 多键操作支持有限 reshard 操作不够自动化（redis-trib.rb ） ","date":"2018-02-09","objectID":"/redis-ha/:4:1","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Redis因为其高性能和易用性在我们后端的服务中发挥了巨大的作用，并且很多重要功能的实现都会依赖redis。除了常用的缓存，还有队列，发布订阅等重要用处。所以redis的服务高可用就显得尤为关键。这里从redis的高可用入手，梳理了一下目前主流的方案。 ","date":"2018-02-09","objectID":"/redis-ha/:0:0","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Redis-Sentinel Redis-Sentinel是Redis官方推荐的高可用性(HA)解决方案，当用Redis做Master-slave的高可用方案时，假如master宕机了，Redis本身(包括它的很多客户端)都没有实现自动进行主备切换，而Redis-sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换。 它的主要功能有以下几点 不时地监控redis是否按照预期良好地运行; 如果发现某个redis节点运行出现状况，能够通知另外一个进程(例如它的客户端); 能够进行自动切换。当一个master节点不可用时，能够选举出master的多个slave(如果有超过一个slave的话)中的一个来作为新的master,其它的slave节点会将它所追随的master的地址改为被提升为master的slave的新地址。 ","date":"2018-02-09","objectID":"/redis-ha/:1:0","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Sentinel支持集群 很显然，只使用单个sentinel进程来监控redis集群是不可靠的，当sentinel进程宕掉后(sentinel本身也有单点问题，single-point-of-failure)整个集群系统将无法按照预期的方式运行。所以有必要将sentinel集群，这样有几个好处： 即使有一些sentinel进程宕掉了，依然可以进行redis集群的主备切换； 如果只有一个sentinel进程，如果这个进程运行出错，或者是网络堵塞，那么将无法实现redis集群的主备切换（单点问题）; 如果有多个sentinel，redis的客户端可以随意地连接任意一个sentinel来获得关于redis集群中的信息。 redis-sentinel方案提供了单点的高可用解决方案，但是当数据量和业务量极速增长时，单点的reids不可能无限的纵向扩容（增大内存), 这个时候就需要redis有集群的能力来扛。 redis集群的几种实现方式如下： 客户端分片 优点简单客户端sharding不支持动态增删节点。劣势很大服务端Redis实例群拓扑结构有变化时，每个客户端都需要更新调整。连接不能共享，当应用规模增大时，资源浪费制约优化。一般不采用。 基于代理的分片，如codis和Twemproxy 路由查询， redis-cluster ","date":"2018-02-09","objectID":"/redis-ha/:1:1","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Twemproxy Twemproxy也叫nutcraker，是twtter开源的一个redis和memcache代理服务器程序。redis作为一个高效的缓存服务器，非常具有应用价值。但在用户数据量增大时，需要运行多个redis实例，此时将迫切需要一种工具统一管理多个redis实例，避免在每个客户端管理所有连接带来的不方便和不易维护，Twemproxy即为此目标而生。 ","date":"2018-02-09","objectID":"/redis-ha/:2:0","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"主要工作方式 分布式逻辑和存储引擎分开，逻辑层proxy代理，存储用的是原子redis，当每个层面需要添加删除节点必须重启服务生效（要重新利用散列函数生成KEY分片更新） Proxy无状态，redis数据层有状态的，客户端可以请求任一proxy代理上面，再由其转发至正确的redis节点，该KEY分片算法至某个节点都是预先已经算好的，在proxy配置文件保存着，但是如果更新或者删除节点，又要根据一致性hash重新计算分片，并且重启服务。 一致性hash算法，增减节点需要配置proxy通知新的算法，重启服务 优点： 比较轻，开发简单，对应用几乎透明 历史悠久，方案成熟 缺点： 代理影响性能 无法平滑地扩容/缩容 运维比较困难 proxy单点本身会有性能瓶颈 ","date":"2018-02-09","objectID":"/redis-ha/:2:1","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Codis codis由3大组件构成： codis-server : 修改过源码的redis, 支持slot，扩容迁移等 codis-proxy : 支持多线程，go语言实现的内核 codis Dashboard : 集群管理工具 提供web图形界面管理集群。 集群元数据存在在zookeeper或etcd。(Zookeeper/etcd存放数据路由表和codis-proxy节点的元信息，codis-config发起的命令通过其同步到各个存活的codis-proxy) 提供独立的组件codis-ha负责redis节点主备切换。 基于proxy的codis，客户端对路由表变化无感知。客户端需要从codis dashhoard调用list proxy命令获取所有proxy列表，并根据自身的轮询策略决定访问哪个proxy节点以实现负载均衡。 整个集群分为1024个哈希槽，分片算法位SlotId = crc32(key) % 1024，增减节点不需要重启服务 ","date":"2018-02-09","objectID":"/redis-ha/:3:0","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"主要工作方式 分布式逻辑和存储引擎分开，逻辑层codis-proxy，存储用的是修改过的codis-server,这种好处是proxy层可以自动扩展和收缩，存储层也同样可以，每个层面都可以热插拨 proxy无状态，codis-server分为组间，每个组存在一个主节点（必须有并且只能有一个）和多个从节点。客户端请求都是和proxy链接，链接哪个proxy都一样，然后由它根据zookeeper路由信息转发至正确节点，直接可以定位到正确节点上 优点： 对应用几乎透明 性能比 Twemproxy 好 有图形化界面，扩容容易，运维方便 缺点： 代理影响性能 组件过多，需要很多机器资源，部署比较难 修改了 Redis 代码，导致和官方无法同步，新特性跟进缓慢 ","date":"2018-02-09","objectID":"/redis-ha/:3:1","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"Redis Cluster ","date":"2018-02-09","objectID":"/redis-ha/:4:0","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"主要工作方式 分布式的逻辑和存储引擎不分开，即又负责读写操作，又负责集群交互，升级困难，如果代码有bug，集群无法工作 这个结构为无中心的组织，不好把控集群当前的存活状态，客户端可以向任一节点发送请求，再有其重定向正确的节点上。如果在第一次请求和重定向期间cluster拓扑结构改变，则需要再一次或者多次重定向至正确的节点，但是这方面性能可以忽悠不计 整个集群分为16384个哈希槽，分片算法位SlotId = crc16(key) % 16384，增减节点不需要重启服务。Redis 集群通过 Gossip 协议同步节点信息，基本思想是节点之间互相交换信息最终所有节点达到一致。BTW. redis sentinel集群判定redis主节点down掉也是采用gossip协议。关于Gossip参考wiki。 优点： 组件 all-in-box，部署简单 性能最快（没有proxy） 自动故障转移、Slot 迁移中数据可用 官方原生集群方案，更新与支持有保障 缺点： 实践不多 客户端开放成本高（客户端不够成熟） 多键操作支持有限 reshard 操作不够自动化（redis-trib.rb ） ","date":"2018-02-09","objectID":"/redis-ha/:4:1","tags":["redis","backend","high availability"],"title":"Redis高可用简要梳理","uri":"/redis-ha/"},{"categories":null,"content":"1.关键概念理解 同步：发起一个调用，得到结果才返回。 异步：调用发起后，调用直接返回；调用方主动询问被调用方获取结果，或被调用方通过回调函数。 阻塞：调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞：调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 同步才有阻塞和非阻塞之分； 阻塞与非阻塞关乎如何对待事情产生的结果（阻塞：不等到想要的结果我就不走了） ","date":"2018-01-25","objectID":"/io-models/:1:0","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"2.明确进程状态 理解进程的状态转换 就绪状态 -\u003e 运行状态：处于就绪状态的进程被调度后，获得CPU资源（分派CPU时间片），于是进程由就绪状态转换为运行状态。 运行状态 -\u003e 就绪状态：处于运行状态的进程在时间片用完后，不得不让出CPU，从而进程由运行状态转换为就绪状态。此外，在可剥夺的操作系统中，当有更高优先级的进程就 、 绪时，调度程度将正执行的进程转换为就绪状态，让更高优先级的进程执行。 运行状态 -\u003e 阻塞状态：当进程请求某一资源（如外设）的使用和分配或等待某一事件的发生（如I/O操作的完成）时，它就从运行状态转换为阻塞状态。进程以系统调用的形式请求操作系统提供服务，这是一种特殊的、由运行用户态程序调用操作系统内核过程的形式。 阻塞状态 -\u003e 就绪状态：当进程等待的事件到来时，如I/O操作结束或中断结束时，中断处理程序必须把相应进程的状态由阻塞状态转换为就绪状态。 ","date":"2018-01-25","objectID":"/io-models/:2:0","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"3.从操作系统层面执行应用程序理解 IO 模型 ","date":"2018-01-25","objectID":"/io-models/:3:0","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"阻塞IO模型： 简介：进程会一直阻塞，直到数据拷贝完成应用程序调用一个IO函数，导致应用程序阻塞，等待数据准备好。 如果数据没有准备好，一直等待….数据准备好了，从内核拷贝到用户空间，IO函数返回成功指示。 我们 第一次接触到的网络编程都是从 listen()、send()、recv()等接口开始的。使用这些接口可以很方便的构建服务器 /客户机的模型。 阻塞I/O模型图：在调用recv()/recvfrom（）函数时，发生在内核中等待数据和复制数据的过程。 当调用recv()函数时，系统首先查是否有准备好的数据。如果数据没有准备好，那么系统就处于等待状态。当数据准备好后，将数据从系统缓冲区复制到用户空间，然后该函数返回。在套接应用程序中，当调用recv()函数时，未必用户空间就已经存在数据，那么此时recv()函数就会处于等待状态。 阻塞模式给网络编程带来了一个很大的问题，如在调用 send()的同时，线程将被阻塞，在此期间，线程将无法执行任何运算或响应任何的网络请求。这给多客户机、多业务逻辑的网络编程带来了挑战。这时，我们可能会选择多线程的方式来解决这个问题。 应对多客户机的网络应用，最简单的解决方式是在服务器端使用多线程（或多进程）。多线程（或多进程）的目的是让每个连接都拥有独立的线程（或进程），这样任何一个连接的阻塞都不会影响其他的连接。 具体使用多进程还是多线程，并没有一个特定的模式。传统意义上，进程的开销要远远大于线程，所以，如果需要同时为较多的客户机提供服务，则不推荐使用多进程；如果单个服务执行体需要消耗较多的 CPU 资源，譬如需要进行大规模或长时间的数据运算或文件访问，则进程较为安全。 ","date":"2018-01-25","objectID":"/io-models/:3:1","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"非阻塞IO模型 简介：非阻塞IO通过进程反复调用IO函数（多次系统调用，并马上返回）；在数据拷贝的过程中，进程是阻塞的； 我们把一个SOCKET接口设置为非阻塞就是告诉内核，当所请求的I/O操作无法完成时，不要将进程睡眠，而是返回一个错误。这样我们的I/O操作函数将不断的测试数据是否已经准备好，如果没有准备好，继续测试，直到数据准备好为止。在这个不断测试的过程中，会大量的占用CPU的时间。 ","date":"2018-01-25","objectID":"/io-models/:3:2","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"IO复用模型： 简介：IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的 IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 ","date":"2018-01-25","objectID":"/io-models/:3:3","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"异步IO模型 简介：用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 ","date":"2018-01-25","objectID":"/io-models/:3:4","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"4.区别IO多路复用中的select poll epoll ","date":"2018-01-25","objectID":"/io-models/:4:0","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"select int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述符就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符 ","date":"2018-01-25","objectID":"/io-models/:4:1","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"poll int poll (struct pollfd *fds, unsigned int nfds, int timeout); 不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。 ","date":"2018-01-25","objectID":"/io-models/:4:2","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"epoll epoll是通过事件的就绪通知方式，调用epoll_create创建实例，调用epoll_ctl添加或删除监控的文件描述符，调用epoll_wait阻塞住，直到有就绪的文件描述符，通过epoll_event参数返回就绪状态的文件描述符和事件。 epoll操作过程需要三个接口，分别如下： int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大 生成一个 epoll 专用的文件描述符，其实是申请一个内核空间，用来存放想关注的 socket fd 上是否发生以及发生了什么事件。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)； 控制某个 epoll 文件描述符上的事件：注册、修改、删除。其中参数 epfd 是 epoll_create() 创建 epoll 专用的文件描述符。 int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); 等待 I/O 事件的发生；返回发生事件数。参数说明： epfd: 由 epoll_create() 生成的 Epoll 专用的文件描述符； epoll_event: 用于回传代处理事件的数组； maxevents: 每次能处理的事件数； timeout: 等待 I/O 事件发生的超时值； ","date":"2018-01-25","objectID":"/io-models/:4:3","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"区别总结 （1）select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。 （2）select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，epoll 通过 mmap 把内核空间和用户空间映射到同一块内存，省去了拷贝的操作。 ","date":"2018-01-25","objectID":"/io-models/:4:4","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"应用举例 Tornado： 使用单线程的方式，避免线程切换的性能开销，同时避免在使用一些函数接口时出现线程不安全的情况 支持异步非阻塞网络IO模型，避免主进程阻塞等待。 tornado 的 IOLoop 模块 是异步机制的核心，它包含了一系列已经打开的文件描述符和每个描述符的处理器 （handlers）。这些 handlers 就是对 select， poll , epoll等的封装。（所以本质上说是 IO 复用） Django 没有用异步，通过使用多进程的WSGI server（比如uWSGI）来实现并发，这也是WSGI普遍的做法。 ","date":"2018-01-25","objectID":"/io-models/:4:5","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"1.关键概念理解 同步：发起一个调用，得到结果才返回。 异步：调用发起后，调用直接返回；调用方主动询问被调用方获取结果，或被调用方通过回调函数。 阻塞：调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞：调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 同步才有阻塞和非阻塞之分； 阻塞与非阻塞关乎如何对待事情产生的结果（阻塞：不等到想要的结果我就不走了） ","date":"2018-01-25","objectID":"/io-models/:1:0","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"2.明确进程状态 理解进程的状态转换 就绪状态 -\u003e 运行状态：处于就绪状态的进程被调度后，获得CPU资源（分派CPU时间片），于是进程由就绪状态转换为运行状态。 运行状态 -\u003e 就绪状态：处于运行状态的进程在时间片用完后，不得不让出CPU，从而进程由运行状态转换为就绪状态。此外，在可剥夺的操作系统中，当有更高优先级的进程就 、 绪时，调度程度将正执行的进程转换为就绪状态，让更高优先级的进程执行。 运行状态 -\u003e 阻塞状态：当进程请求某一资源（如外设）的使用和分配或等待某一事件的发生（如I/O操作的完成）时，它就从运行状态转换为阻塞状态。进程以系统调用的形式请求操作系统提供服务，这是一种特殊的、由运行用户态程序调用操作系统内核过程的形式。 阻塞状态 -\u003e 就绪状态：当进程等待的事件到来时，如I/O操作结束或中断结束时，中断处理程序必须把相应进程的状态由阻塞状态转换为就绪状态。 ","date":"2018-01-25","objectID":"/io-models/:2:0","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"3.从操作系统层面执行应用程序理解 IO 模型 ","date":"2018-01-25","objectID":"/io-models/:3:0","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"阻塞IO模型： 简介：进程会一直阻塞，直到数据拷贝完成应用程序调用一个IO函数，导致应用程序阻塞，等待数据准备好。 如果数据没有准备好，一直等待….数据准备好了，从内核拷贝到用户空间，IO函数返回成功指示。 我们 第一次接触到的网络编程都是从 listen()、send()、recv()等接口开始的。使用这些接口可以很方便的构建服务器 /客户机的模型。 阻塞I/O模型图：在调用recv()/recvfrom（）函数时，发生在内核中等待数据和复制数据的过程。 当调用recv()函数时，系统首先查是否有准备好的数据。如果数据没有准备好，那么系统就处于等待状态。当数据准备好后，将数据从系统缓冲区复制到用户空间，然后该函数返回。在套接应用程序中，当调用recv()函数时，未必用户空间就已经存在数据，那么此时recv()函数就会处于等待状态。 阻塞模式给网络编程带来了一个很大的问题，如在调用 send()的同时，线程将被阻塞，在此期间，线程将无法执行任何运算或响应任何的网络请求。这给多客户机、多业务逻辑的网络编程带来了挑战。这时，我们可能会选择多线程的方式来解决这个问题。 应对多客户机的网络应用，最简单的解决方式是在服务器端使用多线程（或多进程）。多线程（或多进程）的目的是让每个连接都拥有独立的线程（或进程），这样任何一个连接的阻塞都不会影响其他的连接。 具体使用多进程还是多线程，并没有一个特定的模式。传统意义上，进程的开销要远远大于线程，所以，如果需要同时为较多的客户机提供服务，则不推荐使用多进程；如果单个服务执行体需要消耗较多的 CPU 资源，譬如需要进行大规模或长时间的数据运算或文件访问，则进程较为安全。 ","date":"2018-01-25","objectID":"/io-models/:3:1","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"非阻塞IO模型 简介：非阻塞IO通过进程反复调用IO函数（多次系统调用，并马上返回）；在数据拷贝的过程中，进程是阻塞的； 我们把一个SOCKET接口设置为非阻塞就是告诉内核，当所请求的I/O操作无法完成时，不要将进程睡眠，而是返回一个错误。这样我们的I/O操作函数将不断的测试数据是否已经准备好，如果没有准备好，继续测试，直到数据准备好为止。在这个不断测试的过程中，会大量的占用CPU的时间。 ","date":"2018-01-25","objectID":"/io-models/:3:2","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"IO复用模型： 简介：IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的 IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 ","date":"2018-01-25","objectID":"/io-models/:3:3","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"异步IO模型 简介：用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 ","date":"2018-01-25","objectID":"/io-models/:3:4","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"4.区别IO多路复用中的select poll epoll ","date":"2018-01-25","objectID":"/io-models/:4:0","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"select int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述符就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符 ","date":"2018-01-25","objectID":"/io-models/:4:1","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"poll int poll (struct pollfd *fds, unsigned int nfds, int timeout); 不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。 ","date":"2018-01-25","objectID":"/io-models/:4:2","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"epoll epoll是通过事件的就绪通知方式，调用epoll_create创建实例，调用epoll_ctl添加或删除监控的文件描述符，调用epoll_wait阻塞住，直到有就绪的文件描述符，通过epoll_event参数返回就绪状态的文件描述符和事件。 epoll操作过程需要三个接口，分别如下： int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大 生成一个 epoll 专用的文件描述符，其实是申请一个内核空间，用来存放想关注的 socket fd 上是否发生以及发生了什么事件。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)； 控制某个 epoll 文件描述符上的事件：注册、修改、删除。其中参数 epfd 是 epoll_create() 创建 epoll 专用的文件描述符。 int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); 等待 I/O 事件的发生；返回发生事件数。参数说明： epfd: 由 epoll_create() 生成的 Epoll 专用的文件描述符； epoll_event: 用于回传代处理事件的数组； maxevents: 每次能处理的事件数； timeout: 等待 I/O 事件发生的超时值； ","date":"2018-01-25","objectID":"/io-models/:4:3","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"区别总结 （1）select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。 （2）select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，epoll 通过 mmap 把内核空间和用户空间映射到同一块内存，省去了拷贝的操作。 ","date":"2018-01-25","objectID":"/io-models/:4:4","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"应用举例 Tornado： 使用单线程的方式，避免线程切换的性能开销，同时避免在使用一些函数接口时出现线程不安全的情况 支持异步非阻塞网络IO模型，避免主进程阻塞等待。 tornado 的 IOLoop 模块 是异步机制的核心，它包含了一系列已经打开的文件描述符和每个描述符的处理器 （handlers）。这些 handlers 就是对 select， poll , epoll等的封装。（所以本质上说是 IO 复用） Django 没有用异步，通过使用多进程的WSGI server（比如uWSGI）来实现并发，这也是WSGI普遍的做法。 ","date":"2018-01-25","objectID":"/io-models/:4:5","tags":["linux","IO"],"title":"操作系统IO模式(重整)","uri":"/io-models/"},{"categories":null,"content":"整体把握 (文中ZK与ZooKeeper混用请忽略这些细节) ZK 是针对分布式系统的协调服务（本身就是分布式应用程序），优点是可靠，可扩展，高性能。 遵循 C/S 模型。(这里C就是我们使用zk服务的机器，S自然就是提供ZK服务)。ZK可以提供单机服务，也可组成集群提供服务，还支持伪集群方式(一台物理机运行多个zookeeper实例）。客户端连接到一个单独的服务。客户端保持了一个TCP连接，通过这个TCP连接发送请求、获取响应、获取watch事件、和发送心跳。如果这个连接断了，会自动连接到其他不同的服务器。 下图我简单画了一下zk的工作模型 ZK的数据模型类似文件系统，由 znode 组成目录树的形式，每个节点下可以有子节点。 节点可以是以下四种类型： PERSISTENT：持久化目录节点，这个目录节点存储的数据不会丢失； PERSISTENT_SEQUENTIAL：顺序自动编号的目录节点，这种目录节点会根据当前已近存在的节点数自动加 1，然后返回给客户端已经成功创建的目录节点名； EPHEMERAL：临时目录节点，一旦创建这个节点的客户端与服务器端口也就是 session 超时，这种节点会被自动删除； EPHEMERAL_SEQUENTIAL：临时自动编号节点。 监控节点变化时，可以监控一个节点的变化，也可以监控一个节点所有子节点的变化。ZK一些很重要的应用都是依赖这些节点的特性。 下图我简单画了一下zk的node结构 ","date":"2017-12-21","objectID":"/zookeeper/:0:1","tags":["Tech Notes","ZooKeeper"],"title":"ZooKeeper初识整理(老酒装新瓶)","uri":"/zookeeper/"},{"categories":null,"content":"ZK的主要应用场景 配置管理（Configuration Management） 配置的管理在分布式应用环境中很常见，例如同一个应用系统需要多台 Server 运行，但是它们运行的应用系统的某些配置项是相同的，如果要修改这些相同的配置项，那么就必须同时修改每台运行这个应用系统的 Server，这样非常麻烦而且容易出错。 像这样的配置信息完全可以交给 Zookeeper 来管理，将配置信息保存在 Zookeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中。 下图是配置管理的结构图（引用） 集群管理（Group Membership) Zookeeper 能够很容易的实现集群管理的功能，如有多台 Server 组成一个服务集群，那么必须要一个“总管”知道当前集群中每台机器的服务状态，一旦有机器不能提供服务，集群中其它集群必须知道，从而做出调整重新分配服务策略。同样当增加集群的服务能力时，就会增加一台或多台 Server，同样也必须让“总管”知道。 Zookeeper 不仅能够帮你维护当前的集群中机器的服务状态，而且能够帮你选出一个“总管”，让这个总管来管理集群，这就是 Zookeeper 的另一个功能 Leader Election。 它们的实现方式都是在 Zookeeper 上创建一个 EPHEMERAL 类型的目录节点，然后每个 Server 在它们创建目录节点的父目录节点上调用getChildren(String path, boolean watch) 方法并设置 watch 为 true，由于是 EPHEMERAL 目录节点，当创建它的 Server 死去，这个目录节点也随之被删除，所以 Children 将会变化，这时 getChildren上的 Watch 将会被调用，所以其它 Server 就知道已经有某台 Server 死去了。新增 Server 也是同样的原理。 Zookeeper 如何实现 Leader Election，也就是选出一个 Master Server。和前面的一样每台 Server 创建一个 EPHEMERAL 目录节点，不同的是它还是一个 SEQUENTIAL 目录节点，所以它是个 EPHEMERAL_SEQUENTIAL 目录节点。之所以它是 EPHEMERAL_SEQUENTIAL 目录节点，是因为我们可以给每台 Server 编号，我们可以选择当前是最小编号的 Server 为 Master，假如这个最小编号的 Server 死去，由于是 EPHEMERAL 节点，死去的 Server 对应的节点也被删除，所以当前的节点列表中又出现一个最小编号的节点，我们就选择这个节点为当前 Master。这样就实现了动态选择 Master，避免了传统意义上单 Master 容易出现单点故障的问题。 下图是集群管理的原理示意 共享锁（Locks） 共享锁在同一个进程中很容易实现，但是在跨进程或者在不同 Server 之间就不好实现了。Zookeeper 却很容易实现这个功能，实现方式也是需要获得锁的 Server 创建一个 EPHEMERAL_SEQUENTIAL 目录节点，然后调用 getChildren方法获取当前的目录节点列表中最小的目录节点是不是就是自己创建的目录节点，如果正是自己创建的，那么它就获得了这个锁，如果不是那么它就调用 exists(String path, boolean watch) 方法并监控 Zookeeper 上目录节点列表的变化，一直到自己创建的节点是列表中最小编号的目录节点，从而获得锁，释放锁很简单，只要删除前面它自己所创建的目录节点就行了。 队列管理 Zookeeper 可以处理两种类型的队列： 1. 当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，这种是同步队列。 2. 队列按照 FIFO 方式进行入队和出队操作，例如实现生产者和消费者模型。 创建一个父目录 /synchronizing，每个成员都监控标志（Set Watch）位目录 /synchronizing/start 是否存在，然后每个成员都加入这个队列，加入队列的方式就是创建 /synchronizing/member_i 的临时目录节点，然后每个成员获取 /synchronizing 目录的所有目录节点，也就是 member_i。判断 i 的值是否已经是成员的个数，如果小于成员个数等待 /synchronizing/start 的出现，如果已经相等就创建 /synchronizing/start。 FIFO 队列用 Zookeeper 实现思路如下： 在特定的目录下创建 SEQUENTIAL 类型的子目录 /queue_i，这样就能保证所有成员加入队列时都是有编号的，出队列时通过 getChildren( ) 方法可以返回当前所有的队列中的元素，然后消费其中最小的一个，这样就能保证 FIFO。 ","date":"2017-12-21","objectID":"/zookeeper/:0:2","tags":["Tech Notes","ZooKeeper"],"title":"ZooKeeper初识整理(老酒装新瓶)","uri":"/zookeeper/"},{"categories":null,"content":"ZK简单实践 mac下 brew install zookeeper ZK服务可以包含一个节点，但它不是一个高可用和可靠的系统。如果ZK服务中有两个节点，那么这两个节点都必须已经启动并让服务正常运行，因为两个节点中的一个并不是严格意义上的多数。如果有三个节点(这可以称之为cluster了)，即使其中一个停机了，仍然可以获得正常运行的服务（三个中的两个是严格意义上的多数）。出于这个原因，ZooKeeper 的cluster中通常包含奇数数量的节点，因为就容错而言，与三个节点相比，四个节点并不占优势，因为只要有两个节点停机，ZooKeeper 服务就会停止。在有五个节点的集群上，需要三个节点停机才会导致 ZooKeeper 服务停止运作。 搭建3个ZK实例（伪集群配置） 在 usr/l/ocal/var/run/zookeeper/ 下新建文件夹 zk1, zk2, zk3 在 /usr/local/etc/zookeeper 下 新建 zk1.cfg, zk2.cfg, zk3.cfg 修改项 dataDir=/usr/local/var/run/zookeeper/zk1 # the port at which the clients will connect clientPort=2181 server.1=localhost:2888:3888 server.2=localhost:2889:3889 server.3=localhost:2890:3890 在文件夹 /usr/local/var/run/zookeeper/zk1 下 echo \"1\" \u003e /usr/local/var/run/zookeeper/zk1/myid zk2 下 echo \"2\" \u003e myid zk3 下 echo \"3\" \u003e myid; myid里面的值与 配置 文件里的 server. 编号一致 依次启动： zkServer start /usr/local/etc/zk1.cfg zkServer start /usr/local/etc/zk2.cfg zkServer start /usr/local/etc/zk3.cfg 检查状态： zkServer status /usr/local/etc/zk1.cfg 输出 ： ZooKeeper JMX enabled by default Using config: /usr/local/etc/zookeeper/zk1.cfg Mode: follower 创建一个znode 创建一个新znode和相关联的数据: create /test_data test1234 用 get 获取数据： get /test_data； 用 set 修改： set/test_data abcd1234 连接到其他 ZK 服务器，可获取到同样的数据 : get /test_data 1 这里在最后提供了一个可选参数 1。此参数为 /tes_data 上的数据设置了一个一次性的触发 器（名称为 watch）。如果另一个客户端在 /test_data 上修改数据，该客户端将会获得一 个异步通知。请注意，该通知只发送一次，除非 watch 被重新设置，否则不会因数据发生改变 而再次发送通知。 如图所示： 利用 python 实现的 zookeeper 客户端 kazoo 与 zk server 交互 创建临时自动编号节点 from kazoo.client import KazooClient hosts = ['127.0.0.1:12181', '127.0.0.1:12182', '127.0.0.1:12183'] zk = KazooClient(host=hosts[0]) zk.start() print(zk.state) # 创建一个目录节点 zk.create('/seq_test') # 创建一个临时自动编号子目录节点 zk.create('/seq_test/test_i', ephemeral=True, sequence=True) print(zk.get('/seq_test')) # 获取子节点信息 child = zk.get_children('/seq_test')s 定义一个 watch event, watch 一个节点的变化 def watch_event（child): print('action triggered：', child) zk.create('/election') child = zk.get_children('/election', watch=watch_event) zk.create('/election/child') zk.delete('/election/child') 结果示意图 ","date":"2017-12-21","objectID":"/zookeeper/:0:3","tags":["Tech Notes","ZooKeeper"],"title":"ZooKeeper初识整理(老酒装新瓶)","uri":"/zookeeper/"},{"categories":null,"content":"整体把握 (文中ZK与ZooKeeper混用请忽略这些细节) ZK 是针对分布式系统的协调服务（本身就是分布式应用程序），优点是可靠，可扩展，高性能。 遵循 C/S 模型。(这里C就是我们使用zk服务的机器，S自然就是提供ZK服务)。ZK可以提供单机服务，也可组成集群提供服务，还支持伪集群方式(一台物理机运行多个zookeeper实例）。客户端连接到一个单独的服务。客户端保持了一个TCP连接，通过这个TCP连接发送请求、获取响应、获取watch事件、和发送心跳。如果这个连接断了，会自动连接到其他不同的服务器。 下图我简单画了一下zk的工作模型 ZK的数据模型类似文件系统，由 znode 组成目录树的形式，每个节点下可以有子节点。 节点可以是以下四种类型： PERSISTENT：持久化目录节点，这个目录节点存储的数据不会丢失； PERSISTENT_SEQUENTIAL：顺序自动编号的目录节点，这种目录节点会根据当前已近存在的节点数自动加 1，然后返回给客户端已经成功创建的目录节点名； EPHEMERAL：临时目录节点，一旦创建这个节点的客户端与服务器端口也就是 session 超时，这种节点会被自动删除； EPHEMERAL_SEQUENTIAL：临时自动编号节点。 监控节点变化时，可以监控一个节点的变化，也可以监控一个节点所有子节点的变化。ZK一些很重要的应用都是依赖这些节点的特性。 下图我简单画了一下zk的node结构 ","date":"2017-12-21","objectID":"/zookeeper/:0:1","tags":["Tech Notes","ZooKeeper"],"title":"ZooKeeper初识整理(老酒装新瓶)","uri":"/zookeeper/"},{"categories":null,"content":"ZK的主要应用场景 配置管理（Configuration Management） 配置的管理在分布式应用环境中很常见，例如同一个应用系统需要多台 Server 运行，但是它们运行的应用系统的某些配置项是相同的，如果要修改这些相同的配置项，那么就必须同时修改每台运行这个应用系统的 Server，这样非常麻烦而且容易出错。 像这样的配置信息完全可以交给 Zookeeper 来管理，将配置信息保存在 Zookeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中。 下图是配置管理的结构图（引用） 集群管理（Group Membership) Zookeeper 能够很容易的实现集群管理的功能，如有多台 Server 组成一个服务集群，那么必须要一个“总管”知道当前集群中每台机器的服务状态，一旦有机器不能提供服务，集群中其它集群必须知道，从而做出调整重新分配服务策略。同样当增加集群的服务能力时，就会增加一台或多台 Server，同样也必须让“总管”知道。 Zookeeper 不仅能够帮你维护当前的集群中机器的服务状态，而且能够帮你选出一个“总管”，让这个总管来管理集群，这就是 Zookeeper 的另一个功能 Leader Election。 它们的实现方式都是在 Zookeeper 上创建一个 EPHEMERAL 类型的目录节点，然后每个 Server 在它们创建目录节点的父目录节点上调用getChildren(String path, boolean watch) 方法并设置 watch 为 true，由于是 EPHEMERAL 目录节点，当创建它的 Server 死去，这个目录节点也随之被删除，所以 Children 将会变化，这时 getChildren上的 Watch 将会被调用，所以其它 Server 就知道已经有某台 Server 死去了。新增 Server 也是同样的原理。 Zookeeper 如何实现 Leader Election，也就是选出一个 Master Server。和前面的一样每台 Server 创建一个 EPHEMERAL 目录节点，不同的是它还是一个 SEQUENTIAL 目录节点，所以它是个 EPHEMERAL_SEQUENTIAL 目录节点。之所以它是 EPHEMERAL_SEQUENTIAL 目录节点，是因为我们可以给每台 Server 编号，我们可以选择当前是最小编号的 Server 为 Master，假如这个最小编号的 Server 死去，由于是 EPHEMERAL 节点，死去的 Server 对应的节点也被删除，所以当前的节点列表中又出现一个最小编号的节点，我们就选择这个节点为当前 Master。这样就实现了动态选择 Master，避免了传统意义上单 Master 容易出现单点故障的问题。 下图是集群管理的原理示意 共享锁（Locks） 共享锁在同一个进程中很容易实现，但是在跨进程或者在不同 Server 之间就不好实现了。Zookeeper 却很容易实现这个功能，实现方式也是需要获得锁的 Server 创建一个 EPHEMERAL_SEQUENTIAL 目录节点，然后调用 getChildren方法获取当前的目录节点列表中最小的目录节点是不是就是自己创建的目录节点，如果正是自己创建的，那么它就获得了这个锁，如果不是那么它就调用 exists(String path, boolean watch) 方法并监控 Zookeeper 上目录节点列表的变化，一直到自己创建的节点是列表中最小编号的目录节点，从而获得锁，释放锁很简单，只要删除前面它自己所创建的目录节点就行了。 队列管理 Zookeeper 可以处理两种类型的队列： 1. 当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达，这种是同步队列。 2. 队列按照 FIFO 方式进行入队和出队操作，例如实现生产者和消费者模型。 创建一个父目录 /synchronizing，每个成员都监控标志（Set Watch）位目录 /synchronizing/start 是否存在，然后每个成员都加入这个队列，加入队列的方式就是创建 /synchronizing/member_i 的临时目录节点，然后每个成员获取 /synchronizing 目录的所有目录节点，也就是 member_i。判断 i 的值是否已经是成员的个数，如果小于成员个数等待 /synchronizing/start 的出现，如果已经相等就创建 /synchronizing/start。 FIFO 队列用 Zookeeper 实现思路如下： 在特定的目录下创建 SEQUENTIAL 类型的子目录 /queue_i，这样就能保证所有成员加入队列时都是有编号的，出队列时通过 getChildren( ) 方法可以返回当前所有的队列中的元素，然后消费其中最小的一个，这样就能保证 FIFO。 ","date":"2017-12-21","objectID":"/zookeeper/:0:2","tags":["Tech Notes","ZooKeeper"],"title":"ZooKeeper初识整理(老酒装新瓶)","uri":"/zookeeper/"},{"categories":null,"content":"ZK简单实践 mac下 brew install zookeeper ZK服务可以包含一个节点，但它不是一个高可用和可靠的系统。如果ZK服务中有两个节点，那么这两个节点都必须已经启动并让服务正常运行，因为两个节点中的一个并不是严格意义上的多数。如果有三个节点(这可以称之为cluster了)，即使其中一个停机了，仍然可以获得正常运行的服务（三个中的两个是严格意义上的多数）。出于这个原因，ZooKeeper 的cluster中通常包含奇数数量的节点，因为就容错而言，与三个节点相比，四个节点并不占优势，因为只要有两个节点停机，ZooKeeper 服务就会停止。在有五个节点的集群上，需要三个节点停机才会导致 ZooKeeper 服务停止运作。 搭建3个ZK实例（伪集群配置） 在 usr/l/ocal/var/run/zookeeper/ 下新建文件夹 zk1, zk2, zk3 在 /usr/local/etc/zookeeper 下 新建 zk1.cfg, zk2.cfg, zk3.cfg 修改项 dataDir=/usr/local/var/run/zookeeper/zk1 # the port at which the clients will connect clientPort=2181 server.1=localhost:2888:3888 server.2=localhost:2889:3889 server.3=localhost:2890:3890 在文件夹 /usr/local/var/run/zookeeper/zk1 下 echo \"1\" \u003e /usr/local/var/run/zookeeper/zk1/myid zk2 下 echo \"2\" \u003e myid zk3 下 echo \"3\" \u003e myid; myid里面的值与 配置 文件里的 server. 编号一致 依次启动： zkServer start /usr/local/etc/zk1.cfg zkServer start /usr/local/etc/zk2.cfg zkServer start /usr/local/etc/zk3.cfg 检查状态： zkServer status /usr/local/etc/zk1.cfg 输出 ： ZooKeeper JMX enabled by default Using config: /usr/local/etc/zookeeper/zk1.cfg Mode: follower 创建一个znode 创建一个新znode和相关联的数据: create /test_data test1234 用 get 获取数据： get /test_data； 用 set 修改： set/test_data abcd1234 连接到其他 ZK 服务器，可获取到同样的数据 : get /test_data 1 这里在最后提供了一个可选参数 1。此参数为 /tes_data 上的数据设置了一个一次性的触发 器（名称为 watch）。如果另一个客户端在 /test_data 上修改数据，该客户端将会获得一 个异步通知。请注意，该通知只发送一次，除非 watch 被重新设置，否则不会因数据发生改变 而再次发送通知。 如图所示： 利用 python 实现的 zookeeper 客户端 kazoo 与 zk server 交互 创建临时自动编号节点 from kazoo.client import KazooClient hosts = ['127.0.0.1:12181', '127.0.0.1:12182', '127.0.0.1:12183'] zk = KazooClient(host=hosts[0]) zk.start() print(zk.state) # 创建一个目录节点 zk.create('/seq_test') # 创建一个临时自动编号子目录节点 zk.create('/seq_test/test_i', ephemeral=True, sequence=True) print(zk.get('/seq_test')) # 获取子节点信息 child = zk.get_children('/seq_test')s 定义一个 watch event, watch 一个节点的变化 def watch_event（child): print('action triggered：', child) zk.create('/election') child = zk.get_children('/election', watch=watch_event) zk.create('/election/child') zk.delete('/election/child') 结果示意图 ","date":"2017-12-21","objectID":"/zookeeper/:0:3","tags":["Tech Notes","ZooKeeper"],"title":"ZooKeeper初识整理(老酒装新瓶)","uri":"/zookeeper/"},{"categories":null,"content":"背景 有这样的的批量查询需求，从mongo里要查每一个月内符合条件的交易流水总和。单条查询如下，可以看到查询条件其实很简单。但是测试的时候，发现每次执行的时候mongo的机器的cpu都跑满了，然后结果就是所有查询都拖拖慢了，甚至卡住。 query = { 'transaction_date': {'$gte': int_start, '$lte': int_end}, 'business_uid': {'$in': [business.uid.hex for business in businesses]}, } result = db.aggregate([ {'$match': query}, {'$group': { '_id': '', 'total_transaction_amount': {'$sum': '$transaction_amount'}, }} ]) 针对这种情况，从两方面来排查。 1. 看单条查询本身是否慢查询 2. 看mongo server是否cpu确实性能有限。 ","date":"2017-10-21","objectID":"/mongo-optimize-md/:0:1","tags":["Tech Notes","Mongodb"],"title":"Mongo查询优化-借助explain分析查询计划和执行信息","uri":"/mongo-optimize-md/"},{"categories":null,"content":"解决方案 从CPU跑满的问题出发，首先cpu杀手会有下面几种case: 全表扫描 索引不合理 大量的数据排序 回到我们之前的查询，我们只是做了aggregate的操作，首先排除存在大量的数据排序的问题。 这时候需要分析一波查询计划，需要用到 explain。 explain有三种模式： queryPlanner 这是默认模式，queryPlanner模式下并不会去真正进行query语句查询，而是针对query语句进行执行计划分析并选出winning plan。简单来说就是优化查询 executionStats 字面意思，返回优化查询（winning plan）执行的详细信息 allPlansExecution 这种模式除了winning plan 还包括其他方案的执行的详细信息 详细信息可查看官方文档这里。 这里需要注意: aggregate只支持queryPlanner模式，查看pipline是怎么执行的。 回到之前的查询，用expalain分析查询如下: db.transaction.find({ transaction_date: { $gte: 20170901, $lte: 20170930 } , business_uid: { $in: [ \"911c98acc17a4b9c8fd7cf200f3325f8\", \"8e870721395b46b4be2f257ef45a8547\" ] } }).explain(\"executionStats\"); 结果如下(这里为了展示一下explain的返回结果就全部显示)： { \"queryPlanner\" : { \"plannerVersion\" : 1.0, \"namespace\" : \"reconciliation.transaction\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ { \"transaction_date\" : { \"$lte\" : 20170930.0 } }, { \"transaction_date\" : { \"$gte\" : 20170901.0 } }, { \"business_uid\" : { \"$in\" : [ \"8e870721395b46b4be2f257ef45a8547\", \"911c98acc17a4b9c8fd7cf200f3325f8\" ] } } ] }, \"winningPlan\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"$and\" : [ { \"transaction_date\" : { \"$lte\" : 20170930.0 } }, { \"transaction_date\" : { \"$gte\" : 20170901.0 } }, { \"business_uid\" : { \"$in\" : [ \"8e870721395b46b4be2f257ef45a8547\", \"911c98acc17a4b9c8fd7cf200f3325f8\" ] } } ] }, \"direction\" : \"forward\" }, \"rejectedPlans\" : [ ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 1123.0, \"executionTimeMillis\" : 1413.0, \"totalKeysExamined\" : 0.0, \"totalDocsExamined\" : 927973.0, \"executionStages\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"$and\" : [ { \"transaction_date\" : { \"$lte\" : 20170930.0 } }, { \"transaction_date\" : { \"$gte\" : 20170901.0 } }, { \"business_uid\" : { \"$in\" : [ \"8e870721395b46b4be2f257ef45a8547\", \"911c98acc17a4b9c8fd7cf200f3325f8\" ] } } ] }, \"nReturned\" : 1123.0, \"executionTimeMillisEstimate\" : 1388.0, \"works\" : 927975.0, \"advanced\" : 1123.0, \"needTime\" : 926851.0, \"needYield\" : 0.0, \"saveState\" : 7259.0, \"restoreState\" : 7259.0, \"isEOF\" : 1.0, \"invalidates\" : 0.0, \"direction\" : \"forward\", \"docsExamined\" : 927973.0 } }, \"serverInfo\" : { \"host\" : \"ip-172-31-7-170\", \"port\" : 27017.0, \"version\" : \"3.4.5\", \"gitVersion\" : \"520b8f3092c48d934f0cd78ab5f40fe594f96863\" }, \"ok\" : 1.0 } 通过分析返回结果，看到 winningPlan 和executionStats里stage(查询方式)都是 COLLSCAN 即全表扫描。所以，cpu跑满的原因就是查询的时候是全表扫描。 解决: 很自然，我们已有的索引没用。需要针对查询建立复合索引 db.transaction.createIndex({transaction_date: 1, business_uid: 1}) 在看查询分析结果(这里只展示关键部分) \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 1123.0, \"executionTimeMillis\" : 7.0, \"totalKeysExamined\" : 1185.0, \"totalDocsExamined\" : 1123.0, \"executionStages\" : { \"stage\" : \"FETCH\", \"nReturned\" : 1123.0, \"executionTimeMillisEstimate\" : 10.0, \"works\" : 1186.0, \"advanced\" : 1123.0, \"needTime\" : 62.0, \"needYield\" : 0.0, \"saveState\" : 9.0, \"restoreState\" : 9.0, \"isEOF\" : 1.0, \"invalidates\" : 0.0, \"docsExamined\" : 1123.0, \"alreadyHasObj\" : 0.0, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"nReturned\" : 1123.0, \"executionTimeMillisEstimate\" : 10.0, \"works\" : 1186.0, \"advanced\" : 1123.0, \"needTime\" : 62.0, \"needYield\" : 0.0, \"saveState\" : 9.0, \"restoreState\" : 9.0, \"isEOF\" : 1.0, \"invalidates\" : 0.0, \"keyPattern\" : { \"transaction_date\" : 1.0, \"business_uid\" : 1.0 }, \"indexName\" : \"transaction_date_1_business_uid_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"transaction_date\" : [ ], \"business_uid\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2.0, \"direction\" : \"forward\", \"indexBounds\" : { \"transaction_date\" : [ \"[20170901.0, 20170930.0]\" ], \"business_uid\" : [ \"[\\\"8e870721395b46b4be2f257ef45a8547\\\", \\\"8e870721395b46b4be2f257ef45a8547\\\"]\", \"[\\\"911c98acc17a4b9c8fd7cf200f3325f8\\\", \\\"911c98acc17a4b9c8fd7cf200f3325f8\\\"]\" ] }, \"keysExamined\" : 1185.0, \"seeks\" : 63.0, \"dupsTested\" : 0.0, \"dupsDropped\" : 0.0, \"seenInvalidated\" : 0.0 } } } stage已经变为 FETCH(根据索引去检索指定document, inputStage是 IXSCAN), 关键指标 executionTimeMillis为7（由1413将为7). 测试，查询速度飞起来了，观察CPU占用也是正常水平。 总结： 当查询mongo遇到性能或速度问题时，要善用","date":"2017-10-21","objectID":"/mongo-optimize-md/:0:2","tags":["Tech Notes","Mongodb"],"title":"Mongo查询优化-借助explain分析查询计划和执行信息","uri":"/mongo-optimize-md/"},{"categories":null,"content":"背景 有这样的的批量查询需求，从mongo里要查每一个月内符合条件的交易流水总和。单条查询如下，可以看到查询条件其实很简单。但是测试的时候，发现每次执行的时候mongo的机器的cpu都跑满了，然后结果就是所有查询都拖拖慢了，甚至卡住。 query = { 'transaction_date': {'$gte': int_start, '$lte': int_end}, 'business_uid': {'$in': [business.uid.hex for business in businesses]}, } result = db.aggregate([ {'$match': query}, {'$group': { '_id': '', 'total_transaction_amount': {'$sum': '$transaction_amount'}, }} ]) 针对这种情况，从两方面来排查。 1. 看单条查询本身是否慢查询 2. 看mongo server是否cpu确实性能有限。 ","date":"2017-10-21","objectID":"/mongo-optimize-md/:0:1","tags":["Tech Notes","Mongodb"],"title":"Mongo查询优化-借助explain分析查询计划和执行信息","uri":"/mongo-optimize-md/"},{"categories":null,"content":"解决方案 从CPU跑满的问题出发，首先cpu杀手会有下面几种case: 全表扫描 索引不合理 大量的数据排序 回到我们之前的查询，我们只是做了aggregate的操作，首先排除存在大量的数据排序的问题。 这时候需要分析一波查询计划，需要用到 explain。 explain有三种模式： queryPlanner 这是默认模式，queryPlanner模式下并不会去真正进行query语句查询，而是针对query语句进行执行计划分析并选出winning plan。简单来说就是优化查询 executionStats 字面意思，返回优化查询（winning plan）执行的详细信息 allPlansExecution 这种模式除了winning plan 还包括其他方案的执行的详细信息 详细信息可查看官方文档这里。 这里需要注意: aggregate只支持queryPlanner模式，查看pipline是怎么执行的。 回到之前的查询，用expalain分析查询如下: db.transaction.find({ transaction_date: { $gte: 20170901, $lte: 20170930 } , business_uid: { $in: [ \"911c98acc17a4b9c8fd7cf200f3325f8\", \"8e870721395b46b4be2f257ef45a8547\" ] } }).explain(\"executionStats\"); 结果如下(这里为了展示一下explain的返回结果就全部显示)： { \"queryPlanner\" : { \"plannerVersion\" : 1.0, \"namespace\" : \"reconciliation.transaction\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"$and\" : [ { \"transaction_date\" : { \"$lte\" : 20170930.0 } }, { \"transaction_date\" : { \"$gte\" : 20170901.0 } }, { \"business_uid\" : { \"$in\" : [ \"8e870721395b46b4be2f257ef45a8547\", \"911c98acc17a4b9c8fd7cf200f3325f8\" ] } } ] }, \"winningPlan\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"$and\" : [ { \"transaction_date\" : { \"$lte\" : 20170930.0 } }, { \"transaction_date\" : { \"$gte\" : 20170901.0 } }, { \"business_uid\" : { \"$in\" : [ \"8e870721395b46b4be2f257ef45a8547\", \"911c98acc17a4b9c8fd7cf200f3325f8\" ] } } ] }, \"direction\" : \"forward\" }, \"rejectedPlans\" : [ ] }, \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 1123.0, \"executionTimeMillis\" : 1413.0, \"totalKeysExamined\" : 0.0, \"totalDocsExamined\" : 927973.0, \"executionStages\" : { \"stage\" : \"COLLSCAN\", \"filter\" : { \"$and\" : [ { \"transaction_date\" : { \"$lte\" : 20170930.0 } }, { \"transaction_date\" : { \"$gte\" : 20170901.0 } }, { \"business_uid\" : { \"$in\" : [ \"8e870721395b46b4be2f257ef45a8547\", \"911c98acc17a4b9c8fd7cf200f3325f8\" ] } } ] }, \"nReturned\" : 1123.0, \"executionTimeMillisEstimate\" : 1388.0, \"works\" : 927975.0, \"advanced\" : 1123.0, \"needTime\" : 926851.0, \"needYield\" : 0.0, \"saveState\" : 7259.0, \"restoreState\" : 7259.0, \"isEOF\" : 1.0, \"invalidates\" : 0.0, \"direction\" : \"forward\", \"docsExamined\" : 927973.0 } }, \"serverInfo\" : { \"host\" : \"ip-172-31-7-170\", \"port\" : 27017.0, \"version\" : \"3.4.5\", \"gitVersion\" : \"520b8f3092c48d934f0cd78ab5f40fe594f96863\" }, \"ok\" : 1.0 } 通过分析返回结果，看到 winningPlan 和executionStats里stage(查询方式)都是 COLLSCAN 即全表扫描。所以，cpu跑满的原因就是查询的时候是全表扫描。 解决: 很自然，我们已有的索引没用。需要针对查询建立复合索引 db.transaction.createIndex({transaction_date: 1, business_uid: 1}) 在看查询分析结果(这里只展示关键部分) \"executionStats\" : { \"executionSuccess\" : true, \"nReturned\" : 1123.0, \"executionTimeMillis\" : 7.0, \"totalKeysExamined\" : 1185.0, \"totalDocsExamined\" : 1123.0, \"executionStages\" : { \"stage\" : \"FETCH\", \"nReturned\" : 1123.0, \"executionTimeMillisEstimate\" : 10.0, \"works\" : 1186.0, \"advanced\" : 1123.0, \"needTime\" : 62.0, \"needYield\" : 0.0, \"saveState\" : 9.0, \"restoreState\" : 9.0, \"isEOF\" : 1.0, \"invalidates\" : 0.0, \"docsExamined\" : 1123.0, \"alreadyHasObj\" : 0.0, \"inputStage\" : { \"stage\" : \"IXSCAN\", \"nReturned\" : 1123.0, \"executionTimeMillisEstimate\" : 10.0, \"works\" : 1186.0, \"advanced\" : 1123.0, \"needTime\" : 62.0, \"needYield\" : 0.0, \"saveState\" : 9.0, \"restoreState\" : 9.0, \"isEOF\" : 1.0, \"invalidates\" : 0.0, \"keyPattern\" : { \"transaction_date\" : 1.0, \"business_uid\" : 1.0 }, \"indexName\" : \"transaction_date_1_business_uid_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"transaction_date\" : [ ], \"business_uid\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2.0, \"direction\" : \"forward\", \"indexBounds\" : { \"transaction_date\" : [ \"[20170901.0, 20170930.0]\" ], \"business_uid\" : [ \"[\\\"8e870721395b46b4be2f257ef45a8547\\\", \\\"8e870721395b46b4be2f257ef45a8547\\\"]\", \"[\\\"911c98acc17a4b9c8fd7cf200f3325f8\\\", \\\"911c98acc17a4b9c8fd7cf200f3325f8\\\"]\" ] }, \"keysExamined\" : 1185.0, \"seeks\" : 63.0, \"dupsTested\" : 0.0, \"dupsDropped\" : 0.0, \"seenInvalidated\" : 0.0 } } } stage已经变为 FETCH(根据索引去检索指定document, inputStage是 IXSCAN), 关键指标 executionTimeMillis为7（由1413将为7). 测试，查询速度飞起来了，观察CPU占用也是正常水平。 总结： 当查询mongo遇到性能或速度问题时，要善用","date":"2017-10-21","objectID":"/mongo-optimize-md/:0:2","tags":["Tech Notes","Mongodb"],"title":"Mongo查询优化-借助explain分析查询计划和执行信息","uri":"/mongo-optimize-md/"},{"categories":null,"content":"在搭建好ELK之后，我们可能会有不同的类型的日志。当我们想把这些不同的日志收集起来并需要查看分析，这个时候就需要合理的配置logstash, 才能达到我们想要的结果。 ","date":"2017-10-12","objectID":"/elx-process/:0:0","tags":["Tech Notes","ELK"],"title":"ELK进阶之Logstash配置和Kibana应用","uri":"/elx-process/"},{"categories":null,"content":"配置Logstash 这里我们以三种日志为例，首先需要配置日志来源，即filebeat里的配置需要指明三种不同的日志，为了便于logstash的过配置，这里添加 tags来做区分。也可以添加field。 示例如下： - input_type: log paths: - /var/log/nginx/access.log tags: [\"nginx\"] - input_type: log paths: - /home/zaihui/server/logs/track.log tags: [\"track\"] - input_type: log paths: - /home/zaihui/server/logs/access.log tags: [\"access\"] 然后针对不同的日志格式，我们采用logstash插件grok来匹配过滤。logstash配置实际上可以分为三种 input, filter, output 这三部分配置可以在同一文件中配置，不过我选择将这三部分配置分别放在三个文件中。这样配置起来既方便又清晰。 input 示例如下: input { beats { port =\u003e 5443 type =\u003e log ssl =\u003e false } } filter 示例如下 filter { if \"nginx\" in [tags] { grok { match =\u003e { \"message\" =\u003e \"\\[%{HTTPDATE:time}\\] %{IPORHOST:client} \\\"%{WORD:method} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}\\\" %{NUMBER:status_code} %{NUMBER:bytes} \\\"(?\u003chttp_referer\u003e\\S+)\\\" \\\"%{GREEDYDATA:http_user_agent}\\\" \\\"%{GREEDYDATA:data}\\\"\" } } } else if \"access\" in [tags] { grok { match =\u003e { \"message\" =\u003e \"%{TIMESTAMP_ISO8601:datetime} %{IPORHOST:client} %{WORD:method} %{URIPATHPARAM:request} \\$%{GREEDYDATA:payload}\\$ %{NOTSPACE:protocol} %{NUMBER:status_code} %{NUMBER:duration} %{QUOTEDSTRING:agent}\" } } } else if \"track\" in [tags] { grok { match =\u003e { \"message\" =\u003e \"%{LOGLEVEL:log_level} %{TIMESTAMP_ISO8601:log_time} %{PATH:path} %{WORD:function} %{GREEDYDATA:content}\"} } } else if \"_grokparsefailure\" in [tags] { grok { remove_tag =\u003e [\"_grokparsefailure\"] } } mutate { remove_filed =\u003e [\"offset\", \"@version\", \"beat.name\", \"input_type\"] } } 其中关键部分是 grok的pattern要写正确。这里常用的patterns在这里查看. 校验自己写的pattern对不对，在这里验证. 比如下面的就是正确的姿势 如果需要解析nginx accec log 日志格式形如 [12/Oct/2017:22:00:15 +0800] 54.223.140.204 “GET /api/v1/auths/jssdk_signature/?category=Customer HTTP/1.1” 200 142 “http://www.wozaihui.com/web/customer#/illegal\" “Mozilla/5.0 (linux) AppleWebKit/537.36 (KHTML, like Gecko) jsdom/11.1.0” “-” 可参考如下pattern \\[%{HTTPDATE:time}\\] %{IPORHOST:client} \\\"%{WORD:method} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}\\\" %{NUMBER:status_code} %{NUMBER:bytes} \\\"(?\u003chttp_referer\u003e\\S+)\\\" \\\"%{GREEDYDATA:http_user_agent}\\\" \\\"%{GREEDYDATA:data}\\\" output示例 按照tags给日志添加 index output { if \"nginx\" in [tags] { elasticsearch { hosts =\u003e [\"localhost:9200\"] index =\u003e \"nginx-access-%{+YYYY.MM.dd}\" } } if \"track\" in [tags] { elasticsearch { hosts =\u003e [\"localhost:9200\"] index =\u003e \"track-%{+YYYY.MM.dd}\" } } if \"access\" in [tags] { elasticsearch { hosts =\u003e [\"localhost:9200\"] index =\u003e \"access-%{+YYYY.MM.dd}\" } } stdout {codec =\u003e rubydebug} ``` ","date":"2017-10-12","objectID":"/elx-process/:0:1","tags":["Tech Notes","ELK"],"title":"ELK进阶之Logstash配置和Kibana应用","uri":"/elx-process/"},{"categories":null,"content":"Kibana应用 按照logstash的output在 Kibana \u003e Management \u003e Index Patterns配置index Kibana \u003e Discover 提供一系列交互方式能非常直观的浏览并搜索数据。对数据定制过滤的过程是会被记录下来，可以理解成在查询过程中每一步操作都形成了报表，并以URL的方式可被调用，这样你就可以把URL发给其他人，在远端查看相同的过滤结果。 Kibana \u003e Visualize 可以根据自己的需求对不同的field定制图表，比如饼图，直方图，云图 Kibana \u003e Dashboard 添加Visualize中的图表创建Dashboard。 点“+”号按钮，选择已创建好的图表，然后在页面上拖拽布局。 2018.07.12更新 ","date":"2017-10-12","objectID":"/elx-process/:0:2","tags":["Tech Notes","ELK"],"title":"ELK进阶之Logstash配置和Kibana应用","uri":"/elx-process/"},{"categories":null,"content":"elasticsearch 删除历史数据 随着日志量的上升，机器硬盘总会达到上限，这就需要我们必须对历史数据做处理，释放硬盘空间。这里可以用两种方法来达到我们的目的。 使用es api _delete_by_query 比如下面的示例，删除10天前的所有index curl -H'Content-Type:application/json' -d'{ \"query\": { \"range\": { \"@timestamp\": { \"lt\": \"now-10d\", \"format\": \"epoch_millis\" } } } } ' -XPOST \"http://127.0.0.1:9200/*/_delete_by_query?pretty\" 然后使用 crontab 来定时执行上面的命令 每5分钟执行一次 sudo crontal -e */5 * * * * /usr/bin/curl -u username:password -H'Content-Type:application/json' -d'{\"query\":{\"range\":{\"@timestamp\":{\"lt\":\"now-7d\",\"format\":\"epoch_millis\"}}}}' -XPOST \"http://127.0.0.1:9200/*-*/_delete_by_query?pretty\" \u003e /tmp/elk_clean.txt 这种方法简单，易用。但是效率很低不适用大量数据的删除 使用 curator \b pip isntall elasticsearch-curator 配置文件 --- client: hosts: - 127.0.0.1 port: 9200 logging: loglevel: INFO logfile: \"/home/curator/logs/actions.log\" logformat: default blacklist: ['elasticsearch', 'urllib3'] 定义删除操作 --- actions: 1: action: delete_indices description: \u003e- Delete indices older than 90 days (based on index name), for regex indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly. options: ignore_empty_list: True timeout_override: continue_if_exception: False disable_action: False filters: - filtertype: pattern kind: regex value: ^.* exclude: - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 120 exclude: 上面的示例表示删除120天前所有的索引 \b执行curator --config config.yml action.yml 当然也可以加入crontab\b来定时执行 这种方法虽然复杂，配置的条件需要详细阅读文档，但是\b功能强大，执行效率非常高。 ","date":"2017-10-12","objectID":"/elx-process/:0:3","tags":["Tech Notes","ELK"],"title":"ELK进阶之Logstash配置和Kibana应用","uri":"/elx-process/"},{"categories":null,"content":"在搭建好ELK之后，我们可能会有不同的类型的日志。当我们想把这些不同的日志收集起来并需要查看分析，这个时候就需要合理的配置logstash, 才能达到我们想要的结果。 ","date":"2017-10-12","objectID":"/elx-process/:0:0","tags":["Tech Notes","ELK"],"title":"ELK进阶之Logstash配置和Kibana应用","uri":"/elx-process/"},{"categories":null,"content":"配置Logstash 这里我们以三种日志为例，首先需要配置日志来源，即filebeat里的配置需要指明三种不同的日志，为了便于logstash的过配置，这里添加 tags来做区分。也可以添加field。 示例如下： - input_type: log paths: - /var/log/nginx/access.log tags: [\"nginx\"] - input_type: log paths: - /home/zaihui/server/logs/track.log tags: [\"track\"] - input_type: log paths: - /home/zaihui/server/logs/access.log tags: [\"access\"] 然后针对不同的日志格式，我们采用logstash插件grok来匹配过滤。logstash配置实际上可以分为三种 input, filter, output 这三部分配置可以在同一文件中配置，不过我选择将这三部分配置分别放在三个文件中。这样配置起来既方便又清晰。 input 示例如下: input { beats { port =\u003e 5443 type =\u003e log ssl =\u003e false } } filter 示例如下 filter { if \"nginx\" in [tags] { grok { match =\u003e { \"message\" =\u003e \"\\[%{HTTPDATE:time}\\] %{IPORHOST:client} \\\"%{WORD:method} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}\\\" %{NUMBER:status_code} %{NUMBER:bytes} \\\"(?\u003chttp_referer\u003e\\S+)\\\" \\\"%{GREEDYDATA:http_user_agent}\\\" \\\"%{GREEDYDATA:data}\\\"\" } } } else if \"access\" in [tags] { grok { match =\u003e { \"message\" =\u003e \"%{TIMESTAMP_ISO8601:datetime} %{IPORHOST:client} %{WORD:method} %{URIPATHPARAM:request} \\$%{GREEDYDATA:payload}\\$ %{NOTSPACE:protocol} %{NUMBER:status_code} %{NUMBER:duration} %{QUOTEDSTRING:agent}\" } } } else if \"track\" in [tags] { grok { match =\u003e { \"message\" =\u003e \"%{LOGLEVEL:log_level} %{TIMESTAMP_ISO8601:log_time} %{PATH:path} %{WORD:function} %{GREEDYDATA:content}\"} } } else if \"_grokparsefailure\" in [tags] { grok { remove_tag =\u003e [\"_grokparsefailure\"] } } mutate { remove_filed =\u003e [\"offset\", \"@version\", \"beat.name\", \"input_type\"] } } 其中关键部分是 grok的pattern要写正确。这里常用的patterns在这里查看. 校验自己写的pattern对不对，在这里验证. 比如下面的就是正确的姿势 如果需要解析nginx accec log 日志格式形如 [12/Oct/2017:22:00:15 +0800] 54.223.140.204 “GET /api/v1/auths/jssdk_signature/?category=Customer HTTP/1.1” 200 142 “http://www.wozaihui.com/web/customer#/illegal\" “Mozilla/5.0 (linux) AppleWebKit/537.36 (KHTML, like Gecko) jsdom/11.1.0” “-” 可参考如下pattern \\[%{HTTPDATE:time}\\] %{IPORHOST:client} \\\"%{WORD:method} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}\\\" %{NUMBER:status_code} %{NUMBER:bytes} \\\"(?\u003chttp_referer\u003e\\S+)\\\" \\\"%{GREEDYDATA:http_user_agent}\\\" \\\"%{GREEDYDATA:data}\\\" output示例 按照tags给日志添加 index output { if \"nginx\" in [tags] { elasticsearch { hosts =\u003e [\"localhost:9200\"] index =\u003e \"nginx-access-%{+YYYY.MM.dd}\" } } if \"track\" in [tags] { elasticsearch { hosts =\u003e [\"localhost:9200\"] index =\u003e \"track-%{+YYYY.MM.dd}\" } } if \"access\" in [tags] { elasticsearch { hosts =\u003e [\"localhost:9200\"] index =\u003e \"access-%{+YYYY.MM.dd}\" } } stdout {codec =\u003e rubydebug} ``` ","date":"2017-10-12","objectID":"/elx-process/:0:1","tags":["Tech Notes","ELK"],"title":"ELK进阶之Logstash配置和Kibana应用","uri":"/elx-process/"},{"categories":null,"content":"Kibana应用 按照logstash的output在 Kibana \u003e Management \u003e Index Patterns配置index Kibana \u003e Discover 提供一系列交互方式能非常直观的浏览并搜索数据。对数据定制过滤的过程是会被记录下来，可以理解成在查询过程中每一步操作都形成了报表，并以URL的方式可被调用，这样你就可以把URL发给其他人，在远端查看相同的过滤结果。 Kibana \u003e Visualize 可以根据自己的需求对不同的field定制图表，比如饼图，直方图，云图 Kibana \u003e Dashboard 添加Visualize中的图表创建Dashboard。 点“+”号按钮，选择已创建好的图表，然后在页面上拖拽布局。 2018.07.12更新 ","date":"2017-10-12","objectID":"/elx-process/:0:2","tags":["Tech Notes","ELK"],"title":"ELK进阶之Logstash配置和Kibana应用","uri":"/elx-process/"},{"categories":null,"content":"elasticsearch 删除历史数据 随着日志量的上升，机器硬盘总会达到上限，这就需要我们必须对历史数据做处理，释放硬盘空间。这里可以用两种方法来达到我们的目的。 使用es api _delete_by_query 比如下面的示例，删除10天前的所有index curl -H'Content-Type:application/json' -d'{ \"query\": { \"range\": { \"@timestamp\": { \"lt\": \"now-10d\", \"format\": \"epoch_millis\" } } } } ' -XPOST \"http://127.0.0.1:9200/*/_delete_by_query?pretty\" 然后使用 crontab 来定时执行上面的命令 每5分钟执行一次 sudo crontal -e */5 * * * * /usr/bin/curl -u username:password -H'Content-Type:application/json' -d'{\"query\":{\"range\":{\"@timestamp\":{\"lt\":\"now-7d\",\"format\":\"epoch_millis\"}}}}' -XPOST \"http://127.0.0.1:9200/*-*/_delete_by_query?pretty\" \u003e /tmp/elk_clean.txt 这种方法简单，易用。但是效率很低不适用大量数据的删除 使用 curator \b pip isntall elasticsearch-curator 配置文件 --- client: hosts: - 127.0.0.1 port: 9200 logging: loglevel: INFO logfile: \"/home/curator/logs/actions.log\" logformat: default blacklist: ['elasticsearch', 'urllib3'] 定义删除操作 --- actions: 1: action: delete_indices description: \u003e- Delete indices older than 90 days (based on index name), for regex indices. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly. options: ignore_empty_list: True timeout_override: continue_if_exception: False disable_action: False filters: - filtertype: pattern kind: regex value: ^.* exclude: - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 120 exclude: 上面的示例表示删除120天前所有的索引 \b执行curator --config config.yml action.yml 当然也可以加入crontab\b来定时执行 这种方法虽然复杂，配置的条件需要详细阅读文档，但是\b功能强大，执行效率非常高。 ","date":"2017-10-12","objectID":"/elx-process/:0:3","tags":["Tech Notes","ELK"],"title":"ELK进阶之Logstash配置和Kibana应用","uri":"/elx-process/"},{"categories":null,"content":"》query的时候 QuerySet.first() 和 QuerySet[0]的区别: 指明了排序，即 order_by('field')的时候， **first()和[0]**是等价的 都是取出按照指定字段排序的第一个结果 如： reh = HuiPayTransaction.objects.using(‘read’).values(‘account_id’).annotate(count=Count(‘account_id’)).order_by(’-account_id’) reh.first() Out[139]: {‘account_id’: 291589, ‘count’: 3} reh[0] Out[140]: {‘account_id’: 291589, ‘count’: 3} ``` 如果没有指明排序，这个时候 first() 会默认用 order_by('pk')排序。返回的结果就是数据库里id最小的那一条记录，与**[0]**返回的不一致 如： reh = HuiPayTransaction.objects.using(‘read’).filter(account_id__gte=0).values(‘account_id’).annotate(count=Count(‘account_id’)) reh.first() Out[154]: {‘account_id’: 911, ‘count’: 1} reh[0] Out[155]: {‘account_id’: 0, ‘count’: 27} ``` ","date":"2017-09-05","objectID":"/django-use/:0:1","tags":["Tech Notes","Django","BE"],"title":"Django特殊使用场景集合(updated at 2017-12-21）","uri":"/django-use/"},{"categories":null,"content":"》 联表查的时候，重复字段指定表名 如下查询 query_set = MerchandiseLog.objects.using('read').select_related('merchandise').filter(**query).extra( {'date_created': 'Date(created_at)'}).values_list('date_created') 对应的sql SELECT (Date(created_at)) AS \"date_created\" FROM \"zmall_merchandiselog\" INNER JOIN \"zmall_merchandise\" ON ( \"zmall_merchandiselog\".\"merchandise_id\" = \"zmall_merchandise\".\"id\" ) WHERE (\"zmall_merchandiselog\".\"business_group_id\" = 1 AND \"zmall_merchandise\".\"folder_id\" = 2 AND \"zmall_merchandiselog\".\"created_at\" \u003e= 2017-09-01 00:00:00 AND \"zmall_merchandiselog\".\"created_at\" \u003c= 2017-09-05 23:59:59.999999 AND \"zmall_merchandiselog\".\"log_type\" = points_buy) 报错：ambiguous column name: created_at 这个时候问题就很明显了，表 zmall_merchandiselog 和表 zmall_merchandise都有字段 created_at,数据库当然不知道 你要用哪个 created_at 正确姿势, 指明表名 query_set = MerchandiseLog.objects.using('read').select_related('merchandise').filter(**query).extra( {'date_created': 'Date(zmall_merchandiselog.created_at)'}).values_list('date_created') 对应的sql SELECT (Date(zmall_merchandiselog.created_at)) AS \"date_created\" FROM \"zmall_merchandiselog\" INNER JOIN \"zmall_merchandise\" ON ( \"zmall_merchandiselog\".\"merchandise_id\" = \"zmall_merchandise\".\"id\" ) WHERE (\"zmall_merchandiselog\".\"created_at\" \u003c= 2017-09-05 23:59:59.999999 AND \"zmall_merchandiselog\".\"log_type\" = points_buy AND \"zmall_merchandise\".\"folder_id\" = 2 AND \"zmall_merchandiselog\".\"created_at\" \u003e= 2017-09-01 00:00:00 AND \"zmall_merchandiselog\".\"business_group_id\" = 1) ","date":"2017-09-05","objectID":"/django-use/:0:2","tags":["Tech Notes","Django","BE"],"title":"Django特殊使用场景集合(updated at 2017-12-21）","uri":"/django-use/"},{"categories":null,"content":"》多次aggragate合并成一次查询 如下查询，我们想按条件count某些字段，甚至其中的字段需要先annotate # last_visited_date 百分位数 last_visited_date_rate = dw_membership.filter(last_visited_date__lte=member_data.get('last_visited_date')).\\ order_by('last_visited_date').count() / total_count # visit_count 百分位数 visit_count_rate = dw_membership.filter(visit_count__lte=member_data.get('visit_count')).\\ order_by('visit_count').count() / total_count # consumptions_amount 百分位数 consume_amount_rate = dw_membership.filter(consumptions_original_amount__lte=member_data.get('consume_amount')).\\ order_by('consumptions_original_amount').count() / total_count # average_consume 百分位数 average_consume_rate = dw_membership.annotate(average_consume=ExpressionWrapper( F('consumptions_original_amount') / F('consumptions_count'), output_field=DecimalField())).filter( average_consume__lte=member_data.get('average_consume')).count() / total_count 用 Case When合并以下操作为一个query query_res = dw_membership.aggregate( last_visited_date_counter=Count(Case( # 比当前会员到店间隔大的会员数 When(last_visited_date__lte=member_data.get('last_visited_date'), then=F('membership_uid')))), visit_count_counter=Count(Case( # 比当前会员到店次数少的会员数 When(visit_count__lte=member_data.get('visit_count'), then=F('membership_uid')))), consume_amount_counter=Count(Case( # 比当前会员消费金额低的会员数 When(consumptions_original_amount__lte=member_data.get('consume_amount'), then=F('membership_uid'))))) 合并操作时善用 `Case`, `When`再搭配上aggregate操作比如 `Count`, `Sum`, `Max`,`Min`等，会起到不错的效果。但最好充分测试，很可能因为Django的版本问题存在bug。 比如这样使用: ```python dw_membership.annotate(average_consume=ExpressionWrapper( F('consumptions_original_amount') / F('consumptions_count'), output_field=DecimalField())).aggregate(average_consume_amount_counter=Count(Case( # 比当前会员平均消费金额低的会员数 When(average_consume__lte=member_data.get('average_consume'), then=F('membership_uid'))))) ``` 会有这样的[bug](https://code.djangoproject.com/ticket/25307), 1.11以后应该已经修复了... ### 》 特殊场景执行raw sql可以大大提高性能 场景举例，读取数据库里的数据并上传。 我们看下面这样一个例子, 取到数据库的每行记录并写入文件。在测试过程中会发现4000多条数据竟然耗时要30多秒，性能是我们不能接受的。定位瓶颈不是一蹴而就的，从代码方面来说是需要逐步分析缩小范围，不确定的地方动用性能分析工具帮助定位。下面展示的代码和分析结果就是确定瓶颈范围后进行的最后一步的定位。 ```python def _fill_excel_data(sheet_name, columns, data): \"\"\" * 通用的填充 Excel 数据的函数，返回文件的 BytesIO 对象, xls格式上限是65536 rows * 这里data是QuerySet \"\"\" wb = xlwt.Workbook(encoding='utf-8') ws = wb.add_sheet(sheet_name) for i, title in enumerate(columns.keys()): ws.write(0, i, title) for i, record in enumerate(data): row_num = i + 1 for j, func in enumerate(columns.values()): tmp = func(record) ws.write(row_num, j, tmp) attached_file = BytesIO() wb.save(attached_file) return attached_file 我们用 line_profiler分析这段代码的主要耗时在什么地方，下面是分析结果 在分析结果我们清楚的看到，绝大部分耗时是在 func(record)这一行, 而record是QuerySet的一条记录，这行代码真正执行的操作时去数据库里拉取查询结果。所以我们找到了瓶颈。就是 每一行记录都去数据库查询一次，巨大的IO开销。 解决方案：减少数据库查询次数。实际上就是避开Django ORM的 lazy load特性，一次数据库查询就把所需记录load到内存中，然后在进行写文件操作。实现方案就是执行raw sql查询到所有记录。 步骤： 从 QuerySet.query解析raw sql 执行raw sql 处理结果 代码如下： sql, params = data.query.sql_with_params() def direct_fetch_with_sql(sql, params): \"\"\"一些特殊场景可以用此函数来performing raw sql\"\"\" cursor = connections['read'].cursor() cursor.execute(sql, params) return dictfetchall(cursor) def dictfetchall(cursor): \"\"\"把fetchall返回的tuple转换为dict\"\"\" columns = [col[0] for col in cursor.description] return [ dict(zip(columns, row)) for row in cursor.fetchall() ] 最后优化代码如下： def _fill_excel_data(sheet_name, columns, data): \"\"\" 通用的填充 Excel 数据的函数，返回文件的 BytesIO 对象, xls格式上限是65536 rows \"\"\" wb = xlwt.Workbook(encoding='utf-8') ws = wb.add_sheet(sheet_name) for i, title in enumerate(columns.keys()): ws.write(0, i, title) sql, params = data.query.sql_with_params() data = direct_fetch_with_sql(sql, params) for i, record in enumerate(data): row_num = i + 1 for j, func in enumerate(columns.values()): ws.write(row_num, j, func(record)) attached_file = BytesIO() wb.save(attached_file) return attached_file 经测试，性能得到几何级的提升。20万条记录写入文件并上传只需要30秒。 注意：对于大数据量的QuerySet进行适当的分隔，不要撑爆内存。 ###《 单元测试是否需要覆写tearDown 如果使用Python内建的unittest.TestCase来作为UT的基类, 注意需要覆写tearDown来实现case执行完后的清理工作 如： class ","date":"2017-09-05","objectID":"/django-use/:0:3","tags":["Tech Notes","Django","BE"],"title":"Django特殊使用场景集合(updated at 2017-12-21）","uri":"/django-use/"},{"categories":null,"content":"》query的时候 QuerySet.first() 和 QuerySet[0]的区别: 指明了排序，即 order_by('field')的时候， **first()和[0]**是等价的 都是取出按照指定字段排序的第一个结果 如： reh = HuiPayTransaction.objects.using(‘read’).values(‘account_id’).annotate(count=Count(‘account_id’)).order_by(’-account_id’) reh.first() Out[139]: {‘account_id’: 291589, ‘count’: 3} reh[0] Out[140]: {‘account_id’: 291589, ‘count’: 3} ``` 如果没有指明排序，这个时候 first() 会默认用 order_by('pk')排序。返回的结果就是数据库里id最小的那一条记录，与**[0]**返回的不一致 如： reh = HuiPayTransaction.objects.using(‘read’).filter(account_id__gte=0).values(‘account_id’).annotate(count=Count(‘account_id’)) reh.first() Out[154]: {‘account_id’: 911, ‘count’: 1} reh[0] Out[155]: {‘account_id’: 0, ‘count’: 27} ``` ","date":"2017-09-05","objectID":"/django-use/:0:1","tags":["Tech Notes","Django","BE"],"title":"Django特殊使用场景集合(updated at 2017-12-21）","uri":"/django-use/"},{"categories":null,"content":"》 联表查的时候，重复字段指定表名 如下查询 query_set = MerchandiseLog.objects.using('read').select_related('merchandise').filter(**query).extra( {'date_created': 'Date(created_at)'}).values_list('date_created') 对应的sql SELECT (Date(created_at)) AS \"date_created\" FROM \"zmall_merchandiselog\" INNER JOIN \"zmall_merchandise\" ON ( \"zmall_merchandiselog\".\"merchandise_id\" = \"zmall_merchandise\".\"id\" ) WHERE (\"zmall_merchandiselog\".\"business_group_id\" = 1 AND \"zmall_merchandise\".\"folder_id\" = 2 AND \"zmall_merchandiselog\".\"created_at\" \u003e= 2017-09-01 00:00:00 AND \"zmall_merchandiselog\".\"created_at\" \u003c= 2017-09-05 23:59:59.999999 AND \"zmall_merchandiselog\".\"log_type\" = points_buy) 报错：ambiguous column name: created_at 这个时候问题就很明显了，表 zmall_merchandiselog 和表 zmall_merchandise都有字段 created_at,数据库当然不知道 你要用哪个 created_at 正确姿势, 指明表名 query_set = MerchandiseLog.objects.using('read').select_related('merchandise').filter(**query).extra( {'date_created': 'Date(zmall_merchandiselog.created_at)'}).values_list('date_created') 对应的sql SELECT (Date(zmall_merchandiselog.created_at)) AS \"date_created\" FROM \"zmall_merchandiselog\" INNER JOIN \"zmall_merchandise\" ON ( \"zmall_merchandiselog\".\"merchandise_id\" = \"zmall_merchandise\".\"id\" ) WHERE (\"zmall_merchandiselog\".\"created_at\" \u003c= 2017-09-05 23:59:59.999999 AND \"zmall_merchandiselog\".\"log_type\" = points_buy AND \"zmall_merchandise\".\"folder_id\" = 2 AND \"zmall_merchandiselog\".\"created_at\" \u003e= 2017-09-01 00:00:00 AND \"zmall_merchandiselog\".\"business_group_id\" = 1) ","date":"2017-09-05","objectID":"/django-use/:0:2","tags":["Tech Notes","Django","BE"],"title":"Django特殊使用场景集合(updated at 2017-12-21）","uri":"/django-use/"},{"categories":null,"content":"》多次aggragate合并成一次查询 如下查询，我们想按条件count某些字段，甚至其中的字段需要先annotate # last_visited_date 百分位数 last_visited_date_rate = dw_membership.filter(last_visited_date__lte=member_data.get('last_visited_date')).\\ order_by('last_visited_date').count() / total_count # visit_count 百分位数 visit_count_rate = dw_membership.filter(visit_count__lte=member_data.get('visit_count')).\\ order_by('visit_count').count() / total_count # consumptions_amount 百分位数 consume_amount_rate = dw_membership.filter(consumptions_original_amount__lte=member_data.get('consume_amount')).\\ order_by('consumptions_original_amount').count() / total_count # average_consume 百分位数 average_consume_rate = dw_membership.annotate(average_consume=ExpressionWrapper( F('consumptions_original_amount') / F('consumptions_count'), output_field=DecimalField())).filter( average_consume__lte=member_data.get('average_consume')).count() / total_count 用 Case When合并以下操作为一个query query_res = dw_membership.aggregate( last_visited_date_counter=Count(Case( # 比当前会员到店间隔大的会员数 When(last_visited_date__lte=member_data.get('last_visited_date'), then=F('membership_uid')))), visit_count_counter=Count(Case( # 比当前会员到店次数少的会员数 When(visit_count__lte=member_data.get('visit_count'), then=F('membership_uid')))), consume_amount_counter=Count(Case( # 比当前会员消费金额低的会员数 When(consumptions_original_amount__lte=member_data.get('consume_amount'), then=F('membership_uid'))))) 合并操作时善用 `Case`, `When`再搭配上aggregate操作比如 `Count`, `Sum`, `Max`,`Min`等，会起到不错的效果。但最好充分测试，很可能因为Django的版本问题存在bug。 比如这样使用: ```python dw_membership.annotate(average_consume=ExpressionWrapper( F('consumptions_original_amount') / F('consumptions_count'), output_field=DecimalField())).aggregate(average_consume_amount_counter=Count(Case( # 比当前会员平均消费金额低的会员数 When(average_consume__lte=member_data.get('average_consume'), then=F('membership_uid'))))) ``` 会有这样的[bug](https://code.djangoproject.com/ticket/25307), 1.11以后应该已经修复了... ### 》 特殊场景执行raw sql可以大大提高性能 场景举例，读取数据库里的数据并上传。 我们看下面这样一个例子, 取到数据库的每行记录并写入文件。在测试过程中会发现4000多条数据竟然耗时要30多秒，性能是我们不能接受的。定位瓶颈不是一蹴而就的，从代码方面来说是需要逐步分析缩小范围，不确定的地方动用性能分析工具帮助定位。下面展示的代码和分析结果就是确定瓶颈范围后进行的最后一步的定位。 ```python def _fill_excel_data(sheet_name, columns, data): \"\"\" * 通用的填充 Excel 数据的函数，返回文件的 BytesIO 对象, xls格式上限是65536 rows * 这里data是QuerySet \"\"\" wb = xlwt.Workbook(encoding='utf-8') ws = wb.add_sheet(sheet_name) for i, title in enumerate(columns.keys()): ws.write(0, i, title) for i, record in enumerate(data): row_num = i + 1 for j, func in enumerate(columns.values()): tmp = func(record) ws.write(row_num, j, tmp) attached_file = BytesIO() wb.save(attached_file) return attached_file 我们用 line_profiler分析这段代码的主要耗时在什么地方，下面是分析结果 在分析结果我们清楚的看到，绝大部分耗时是在 func(record)这一行, 而record是QuerySet的一条记录，这行代码真正执行的操作时去数据库里拉取查询结果。所以我们找到了瓶颈。就是 每一行记录都去数据库查询一次，巨大的IO开销。 解决方案：减少数据库查询次数。实际上就是避开Django ORM的 lazy load特性，一次数据库查询就把所需记录load到内存中，然后在进行写文件操作。实现方案就是执行raw sql查询到所有记录。 步骤： 从 QuerySet.query解析raw sql 执行raw sql 处理结果 代码如下： sql, params = data.query.sql_with_params() def direct_fetch_with_sql(sql, params): \"\"\"一些特殊场景可以用此函数来performing raw sql\"\"\" cursor = connections['read'].cursor() cursor.execute(sql, params) return dictfetchall(cursor) def dictfetchall(cursor): \"\"\"把fetchall返回的tuple转换为dict\"\"\" columns = [col[0] for col in cursor.description] return [ dict(zip(columns, row)) for row in cursor.fetchall() ] 最后优化代码如下： def _fill_excel_data(sheet_name, columns, data): \"\"\" 通用的填充 Excel 数据的函数，返回文件的 BytesIO 对象, xls格式上限是65536 rows \"\"\" wb = xlwt.Workbook(encoding='utf-8') ws = wb.add_sheet(sheet_name) for i, title in enumerate(columns.keys()): ws.write(0, i, title) sql, params = data.query.sql_with_params() data = direct_fetch_with_sql(sql, params) for i, record in enumerate(data): row_num = i + 1 for j, func in enumerate(columns.values()): ws.write(row_num, j, func(record)) attached_file = BytesIO() wb.save(attached_file) return attached_file 经测试，性能得到几何级的提升。20万条记录写入文件并上传只需要30秒。 注意：对于大数据量的QuerySet进行适当的分隔，不要撑爆内存。 ###《 单元测试是否需要覆写tearDown 如果使用Python内建的unittest.TestCase来作为UT的基类, 注意需要覆写tearDown来实现case执行完后的清理工作 如： class ","date":"2017-09-05","objectID":"/django-use/:0:3","tags":["Tech Notes","Django","BE"],"title":"Django特殊使用场景集合(updated at 2017-12-21）","uri":"/django-use/"},{"categories":null,"content":"ELK简要说明 ELK 不是一款软件，而是 Elasticsearch、Logstash 和 Kibana 三种软件产品的首字母缩写。这三者都是开源软件，通常配合使用，而且又先后归于 Elastic.co 公司名下，所以被简称为 ELK Stack。根据 Google Trend 的信息显示，ELK Stack 已经成为目前最流行的集中式日志解决方案。 Elasticsearch：分布式搜索和分析引擎，具有高可伸缩、高可靠和易管理等特点。基于 Apache Lucene 构建，能对大容量的数据进行接近实时的存储、搜索和分析操作。通常被用作某些应用的基础搜索引擎，使其具有复杂的搜索功能； Logstash：数据收集引擎。它支持动态的从各种数据源搜集数据，并对数据进行过滤、分析、丰富、统一格式等操作，然后存储到用户指定的位置； Kibana：数据分析和可视化平台。通常与 Elasticsearch 配合使用，对其中数据进行搜索、分析和以统计图表的方式展示； Filebeat：ELK 协议栈的新成员，一个轻量级开源日志文件数据搜集器，基于 Logstash-Forwarder 源代码开发，是对它的替代。在需要采集日志数据的 server 上安装 Filebeat，并指定日志目录或日志文件后，Filebeat 就能读取数据，迅速发送到 Logstash 进行解析，亦或直接发送到 Elasticsearch 进行集中式存储和分析。 ","date":"2017-08-26","objectID":"/elk-build/:0:1","tags":["Tech Notes","ELK"],"title":"ELK stack日志系统搭建","uri":"/elk-build/"},{"categories":null,"content":"基于Filebeat的架构 Filebeat作为客户端，轻量及安全。我们采用Filebeat来作为日志的收集器，下图是整个ELK stack日志系统的架构。 因为免费的 ELK 没有任何安全机制，所以这里使用了 Nginx 作反向代理，避免用户直接访问 Kibana 服务器。加上配置 Nginx 实现简单的用户认证，一定程度上提高安全性。另外，Nginx 本身具有负载均衡的作用，能够提高系统访问性能。 ","date":"2017-08-26","objectID":"/elk-build/:0:2","tags":["Tech Notes","ELK"],"title":"ELK stack日志系统搭建","uri":"/elk-build/"},{"categories":null,"content":"搭建过程 环境为 Unbuntu 16.04 1. 安装 JAVA Elasticsearch 运行依赖java 8 ，推荐安装 Oracle JDK 1.8。我从ppa仓库安装java8 安装 python-software-properties，可以轻松添加repo sudo apt-get update sudo apt-get install -y python-software-properties soft-common-properties apt-transport-https 用 apt-add-repository添加 java8 PPA repo sudo apt-add-repository ppa:webupd8team/java -y sudo apt-get update 安装 java8 sudo apt-get install -y oracle-java8-installer java -version # 验证下 2. 安装配置Elasticsearch 从 elastic repository安装 首先添加 elastic repo key 到机器 wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - 添加 elastic 5.x repository 到 source.list.d 目录 echo \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-5.x.list 生效elastic安装源，并安装 sudo apt-get update sudp apt-get install -y elasticsearch Elasticsearch安装完毕，然后配置 elasticsearch.yml cd /etc/elasticsearch vim elasticsearch.yml memory 部分，关闭内存交换，避免系统负载过高 bootstrap.memorylock: true Network 部分： network.host: localhost http.port: 9200 保存并推出vim 修改 elasticsearch service 文件 vim /usr/lib/systemd/elasticsearch.service LimitMEMLOCK=infinity 修改默认配置 vim /etc/default/elasticsearch MAX_LOCKED_MEMORY=unlimited 配置完毕，启动服务 启动并设置成开机自动启动 sudo systemctl daemon-reload sudo systemctl enable elasticsearch sudo systemctl start elasticsearch 检查是否正常启动，是否在监听端口9200 netstat -plnt 显示如下，则正确启动 tcp6 0 0 :::9200 :::* LISTEN - tcp6 0 0 :::9300 :::* LISTEN - tcp6 0 0 :::22 :::* LISTEN - tcp6 0 0 127.0.0.1:9600 :::* LISTEN - 访问下9200端口 `curl -XGET 'localhost:9200'` 返回如下结果, 则elasticseach已经正常工作 ``` { \"name\" : \"node-1\", \"cluster_name\" : \"zaihui-elk\", \"cluster_uuid\" : \"L_BrQy4PRO2ed_8CXt3IuA\", \"version\" : { \"number\" : \"5.5.2\", \"build_hash\" : \"b2f0c09\", \"build_date\" : \"2017-08-14T12:33:14.154Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.6.0\" }, \"tagline\" : \"You Know, for Search\" } ``` #### 3. 安装和配置Kibana ELK stack的各个组件安装配置方法基本一致 sudo apt-get install -y kibana # 安装 vim /etc/kibana/kibana.yml # 配置 配置如下必选项 server.port: 5601 server.host: “localhost” elasticsearch.url: “http://localhost:9200” 启动并设置开机自动启动 sudo systemctl enable kibana sudo systemctl start kibana 检查是否正常启动（kibana是一个node服务） `netstat -plnt` 输出如下，则正确安装配置 Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - tcp 0 0 127.0.0.1:5601 0.0.0.0:* LISTEN - ### 4. 配置Nginx反向代理Kibana * 安装 Nginx 和 apache2-utils包 `sudo apt-get install -y nginx apache2-utils` Nginx可以用apache2-utils来添加一些辅助功能，比如用 htpasswd 来管理用户访问权限 * 配置kibana服务 ``` cd /etc/niginx/ vim /conf.d/kibana.conf ``` 配置如下 ``` server { listen 80; server_name localhost; auth_basic \"Restricted Access\"; auth_basic_user_file /etc/nginx/.kibana-user; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } ``` * 用 htpasswd创建一个可访问账号 `sudo htppasswd -c /etc/nginx/.kibana-user lxkaka` * 验证nginx配置正确，并启动nginx ``` nginx -t systemctl enable nginx systemctl restart nginx ``` #### 5. 安装和配置Logstash * 安装和生成SSL证书 `sudo apt-get install -y logstash` logstash 支持SSL协议，可以用OpenSSL生成SSL证书来增加安全性，client可以此来验证elastic server。 ``` cd /etc/logstash openssl req -subj /CN=elk-master -x509 -days 3650 -batch -nodes -newkey rsa:4096 -keyout logstash.key -out logstash.crt ``` * 配置输入，解析，输出三份配置 当然三份配置都可以写到一个文件中，不过为了维护和清晰我觉得分开配置比较合适。 编辑来自filebeat的输入配置 ``` cd /etc/logstash/ vim conf.d/filebeat-input.conf ``` 如下： ``` input { beats { port =\u003e 5443 type =\u003e log ssl =\u003e true ssl_certificate =\u003e \"/etc/logstash/logstash.crt\" ssl_key =\u003e \"/etc/logstash/logstash.key\" } } ``` 编辑过滤解析配置文件(这里以解析niginx accesslog为例） `vim conf.d/nginxlog-filter.conf` ``` filter { grok { match =\u003e { \"message\" =\u003e \"%{IPORHOST:clientip} \\[%{HTTPDATE:time}\\] \\\"%{WORD:verb} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}\\\" %{NUMBER:http_status_code} %{NUMBER:bytes} \\\"(?\u003chttp_referer\u003e","date":"2017-08-26","objectID":"/elk-build/:0:3","tags":["Tech Notes","ELK"],"title":"ELK stack日志系统搭建","uri":"/elk-build/"},{"categories":null,"content":"ELK简要说明 ELK 不是一款软件，而是 Elasticsearch、Logstash 和 Kibana 三种软件产品的首字母缩写。这三者都是开源软件，通常配合使用，而且又先后归于 Elastic.co 公司名下，所以被简称为 ELK Stack。根据 Google Trend 的信息显示，ELK Stack 已经成为目前最流行的集中式日志解决方案。 Elasticsearch：分布式搜索和分析引擎，具有高可伸缩、高可靠和易管理等特点。基于 Apache Lucene 构建，能对大容量的数据进行接近实时的存储、搜索和分析操作。通常被用作某些应用的基础搜索引擎，使其具有复杂的搜索功能； Logstash：数据收集引擎。它支持动态的从各种数据源搜集数据，并对数据进行过滤、分析、丰富、统一格式等操作，然后存储到用户指定的位置； Kibana：数据分析和可视化平台。通常与 Elasticsearch 配合使用，对其中数据进行搜索、分析和以统计图表的方式展示； Filebeat：ELK 协议栈的新成员，一个轻量级开源日志文件数据搜集器，基于 Logstash-Forwarder 源代码开发，是对它的替代。在需要采集日志数据的 server 上安装 Filebeat，并指定日志目录或日志文件后，Filebeat 就能读取数据，迅速发送到 Logstash 进行解析，亦或直接发送到 Elasticsearch 进行集中式存储和分析。 ","date":"2017-08-26","objectID":"/elk-build/:0:1","tags":["Tech Notes","ELK"],"title":"ELK stack日志系统搭建","uri":"/elk-build/"},{"categories":null,"content":"基于Filebeat的架构 Filebeat作为客户端，轻量及安全。我们采用Filebeat来作为日志的收集器，下图是整个ELK stack日志系统的架构。 因为免费的 ELK 没有任何安全机制，所以这里使用了 Nginx 作反向代理，避免用户直接访问 Kibana 服务器。加上配置 Nginx 实现简单的用户认证，一定程度上提高安全性。另外，Nginx 本身具有负载均衡的作用，能够提高系统访问性能。 ","date":"2017-08-26","objectID":"/elk-build/:0:2","tags":["Tech Notes","ELK"],"title":"ELK stack日志系统搭建","uri":"/elk-build/"},{"categories":null,"content":"搭建过程 环境为 Unbuntu 16.04 1. 安装 JAVA Elasticsearch 运行依赖java 8 ，推荐安装 Oracle JDK 1.8。我从ppa仓库安装java8 安装 python-software-properties，可以轻松添加repo sudo apt-get update sudo apt-get install -y python-software-properties soft-common-properties apt-transport-https 用 apt-add-repository添加 java8 PPA repo sudo apt-add-repository ppa:webupd8team/java -y sudo apt-get update 安装 java8 sudo apt-get install -y oracle-java8-installer java -version # 验证下 2. 安装配置Elasticsearch 从 elastic repository安装 首先添加 elastic repo key 到机器 wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - 添加 elastic 5.x repository 到 source.list.d 目录 echo \"deb https://artifacts.elastic.co/packages/5.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-5.x.list 生效elastic安装源，并安装 sudo apt-get update sudp apt-get install -y elasticsearch Elasticsearch安装完毕，然后配置 elasticsearch.yml cd /etc/elasticsearch vim elasticsearch.yml memory 部分，关闭内存交换，避免系统负载过高 bootstrap.memorylock: true Network 部分： network.host: localhost http.port: 9200 保存并推出vim 修改 elasticsearch service 文件 vim /usr/lib/systemd/elasticsearch.service LimitMEMLOCK=infinity 修改默认配置 vim /etc/default/elasticsearch MAX_LOCKED_MEMORY=unlimited 配置完毕，启动服务 启动并设置成开机自动启动 sudo systemctl daemon-reload sudo systemctl enable elasticsearch sudo systemctl start elasticsearch 检查是否正常启动，是否在监听端口9200 netstat -plnt 显示如下，则正确启动 tcp6 0 0 :::9200 :::* LISTEN - tcp6 0 0 :::9300 :::* LISTEN - tcp6 0 0 :::22 :::* LISTEN - tcp6 0 0 127.0.0.1:9600 :::* LISTEN - 访问下9200端口 `curl -XGET 'localhost:9200'` 返回如下结果, 则elasticseach已经正常工作 ``` { \"name\" : \"node-1\", \"cluster_name\" : \"zaihui-elk\", \"cluster_uuid\" : \"L_BrQy4PRO2ed_8CXt3IuA\", \"version\" : { \"number\" : \"5.5.2\", \"build_hash\" : \"b2f0c09\", \"build_date\" : \"2017-08-14T12:33:14.154Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.6.0\" }, \"tagline\" : \"You Know, for Search\" } ``` #### 3. 安装和配置Kibana ELK stack的各个组件安装配置方法基本一致 sudo apt-get install -y kibana # 安装 vim /etc/kibana/kibana.yml # 配置 配置如下必选项 server.port: 5601 server.host: “localhost” elasticsearch.url: “http://localhost:9200” 启动并设置开机自动启动 sudo systemctl enable kibana sudo systemctl start kibana 检查是否正常启动（kibana是一个node服务） `netstat -plnt` 输出如下，则正确安装配置 Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - tcp 0 0 127.0.0.1:5601 0.0.0.0:* LISTEN - ### 4. 配置Nginx反向代理Kibana * 安装 Nginx 和 apache2-utils包 `sudo apt-get install -y nginx apache2-utils` Nginx可以用apache2-utils来添加一些辅助功能，比如用 htpasswd 来管理用户访问权限 * 配置kibana服务 ``` cd /etc/niginx/ vim /conf.d/kibana.conf ``` 配置如下 ``` server { listen 80; server_name localhost; auth_basic \"Restricted Access\"; auth_basic_user_file /etc/nginx/.kibana-user; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } ``` * 用 htpasswd创建一个可访问账号 `sudo htppasswd -c /etc/nginx/.kibana-user lxkaka` * 验证nginx配置正确，并启动nginx ``` nginx -t systemctl enable nginx systemctl restart nginx ``` #### 5. 安装和配置Logstash * 安装和生成SSL证书 `sudo apt-get install -y logstash` logstash 支持SSL协议，可以用OpenSSL生成SSL证书来增加安全性，client可以此来验证elastic server。 ``` cd /etc/logstash openssl req -subj /CN=elk-master -x509 -days 3650 -batch -nodes -newkey rsa:4096 -keyout logstash.key -out logstash.crt ``` * 配置输入，解析，输出三份配置 当然三份配置都可以写到一个文件中，不过为了维护和清晰我觉得分开配置比较合适。 编辑来自filebeat的输入配置 ``` cd /etc/logstash/ vim conf.d/filebeat-input.conf ``` 如下： ``` input { beats { port =\u003e 5443 type =\u003e log ssl =\u003e true ssl_certificate =\u003e \"/etc/logstash/logstash.crt\" ssl_key =\u003e \"/etc/logstash/logstash.key\" } } ``` 编辑过滤解析配置文件(这里以解析niginx accesslog为例） `vim conf.d/nginxlog-filter.conf` ``` filter { grok { match =\u003e { \"message\" =\u003e \"%{IPORHOST:clientip} \\[%{HTTPDATE:time}\\] \\\"%{WORD:verb} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}\\\" %{NUMBER:http_status_code} %{NUMBER:bytes} \\\"(?\u003chttp_referer\u003e","date":"2017-08-26","objectID":"/elk-build/:0:3","tags":["Tech Notes","ELK"],"title":"ELK stack日志系统搭建","uri":"/elk-build/"},{"categories":null,"content":"抓包 当我们想分析线上请求的数据和响应是否符合预期，这个时候抓包就成了一种好的选择。抓包工具charles原理很简单用来做代理服务器，中途截获请求，即可展示request和response的内容。当想抓取 https 的请求，就需要做些额外的工作呢。首先得搞清楚抓取 https请求的原理吧，那么就得先挖一挖https。 ","date":"2017-08-19","objectID":"/https-charles/:0:1","tags":["Tech Notes"],"title":"从Charles抓包理解HTTPS","uri":"/https-charles/"},{"categories":null,"content":"HTTPS原理总结 引用一段常见的概述： HTTPS(Hyper Text Transfer Protocol Secure)，是一种基于SSL/TLS的HTTP，所有的HTTP数据都是在SSL/TLS协议封装之上进行传输的。HTTPS协议是在HTTP协议的基础上，添加了SSL/TLS握手以及数据加密传输，也属于应用层协议。所以，研究HTTPS协议原理，最终就是研究SSL/TLS协议。 简单说来，HTTPS就是增强HTTP的安全性，加密传输内容。为了保证这个加密的有效，有了SSL/TSL握手。所以，理解https就是理解这个握手过程。我画了下面这个图展示这个握手过程 针对此图，注明3点 核心是为了用对称加密(如AES）对传输内容进行加密 为了保证这个这个加密密钥的安全，采用非对称加密(如RSA）来传递密钥 为了防止中间人攻击，而必须验证服务器公钥的有效性，所以有了数字证书来做验证 其中数字证书验证是一个关键步骤，其实这个数字证书是一个证书信任链。因为网站的数字证书不是由root CA直接颁发的，而是由intermediate CA颁发的。 证书链是从网站证书开始，逐级向上，一直到根证书。每个证书的数字签名都是上级证书的私钥签署的。 浏览器和系统预置了受信任的根证书 验证到受信任的根证书，则验证通过(已经被浏览器信任，这是验证过程有效的前提） 一次验证过程如图展示 以X.509v3证书举例，server返回的证书其包含三部分：tbsCertificate、SignatureAlgorithm、SignatureValue； 浏览器读取证书中的tbsCertificate部分（明文），使用SignatureAlgorithm中的散列函数计算得到信息摘要，并利用tbsCertificate中的公钥解密SignatureValue得到信息摘要，然后对比双方的信息摘要，判断是否一致；如果一致，则成功；如果不一致，则失败。 HTTPS就挖到这里，接下来理解Charles抓取HTTPS的包就水到渠成了。 ","date":"2017-08-19","objectID":"/https-charles/:0:2","tags":["Tech Notes"],"title":"从Charles抓包理解HTTPS","uri":"/https-charles/"},{"categories":null,"content":"Charles抓HTTPS原理 我们的终端手机或电脑安装charles的根证书，表示信任Charles的根证书 charles作为代理，接收server返回的证书，而向终端返回自己伪造的证书 对终端来说不care，证书验证通过后，生成密钥用charles证书的公钥加密返回给了charles, charles自认能解密拿到这个密钥，然后再用server的公钥加密，发送给server 两端的HTTPS通信，就都被charles捕获到了 ","date":"2017-08-19","objectID":"/https-charles/:0:3","tags":["Tech Notes"],"title":"从Charles抓包理解HTTPS","uri":"/https-charles/"},{"categories":null,"content":"抓包步骤 配置HTTP代理 proxy \u003e Proxy settings \u003e 选择在8888端口上监听，然后确定; 勾选SOCKS proxy，还能截获到浏览器的http访问请求。 配置SSL代理 proxy \u003e SSL Proxy settings \u003e勾选 Enable SSL Proxying; 点add添加需要监视的域名，支持 *号通配符，端口一般都是443 电脑安装charles根证书(抓取电脑流量) Help \u003e SSL Proxying \u003e Install Charles Root Certificate 手机安装charles根证书（抓取手机流量） 在Safri上打开Charles的根证书下载网址： chls.pro/ssl；点击安装 配置完成，手机端wifi设置里开启手动开启代理，ip为电脑IP，端口为8888(charles配置） ","date":"2017-08-19","objectID":"/https-charles/:0:4","tags":["Tech Notes"],"title":"从Charles抓包理解HTTPS","uri":"/https-charles/"},{"categories":null,"content":"抓包 当我们想分析线上请求的数据和响应是否符合预期，这个时候抓包就成了一种好的选择。抓包工具charles原理很简单用来做代理服务器，中途截获请求，即可展示request和response的内容。当想抓取 https 的请求，就需要做些额外的工作呢。首先得搞清楚抓取 https请求的原理吧，那么就得先挖一挖https。 ","date":"2017-08-19","objectID":"/https-charles/:0:1","tags":["Tech Notes"],"title":"从Charles抓包理解HTTPS","uri":"/https-charles/"},{"categories":null,"content":"HTTPS原理总结 引用一段常见的概述： HTTPS(Hyper Text Transfer Protocol Secure)，是一种基于SSL/TLS的HTTP，所有的HTTP数据都是在SSL/TLS协议封装之上进行传输的。HTTPS协议是在HTTP协议的基础上，添加了SSL/TLS握手以及数据加密传输，也属于应用层协议。所以，研究HTTPS协议原理，最终就是研究SSL/TLS协议。 简单说来，HTTPS就是增强HTTP的安全性，加密传输内容。为了保证这个加密的有效，有了SSL/TSL握手。所以，理解https就是理解这个握手过程。我画了下面这个图展示这个握手过程 针对此图，注明3点 核心是为了用对称加密(如AES）对传输内容进行加密 为了保证这个这个加密密钥的安全，采用非对称加密(如RSA）来传递密钥 为了防止中间人攻击，而必须验证服务器公钥的有效性，所以有了数字证书来做验证 其中数字证书验证是一个关键步骤，其实这个数字证书是一个证书信任链。因为网站的数字证书不是由root CA直接颁发的，而是由intermediate CA颁发的。 证书链是从网站证书开始，逐级向上，一直到根证书。每个证书的数字签名都是上级证书的私钥签署的。 浏览器和系统预置了受信任的根证书 验证到受信任的根证书，则验证通过(已经被浏览器信任，这是验证过程有效的前提） 一次验证过程如图展示 以X.509v3证书举例，server返回的证书其包含三部分：tbsCertificate、SignatureAlgorithm、SignatureValue； 浏览器读取证书中的tbsCertificate部分（明文），使用SignatureAlgorithm中的散列函数计算得到信息摘要，并利用tbsCertificate中的公钥解密SignatureValue得到信息摘要，然后对比双方的信息摘要，判断是否一致；如果一致，则成功；如果不一致，则失败。 HTTPS就挖到这里，接下来理解Charles抓取HTTPS的包就水到渠成了。 ","date":"2017-08-19","objectID":"/https-charles/:0:2","tags":["Tech Notes"],"title":"从Charles抓包理解HTTPS","uri":"/https-charles/"},{"categories":null,"content":"Charles抓HTTPS原理 我们的终端手机或电脑安装charles的根证书，表示信任Charles的根证书 charles作为代理，接收server返回的证书，而向终端返回自己伪造的证书 对终端来说不care，证书验证通过后，生成密钥用charles证书的公钥加密返回给了charles, charles自认能解密拿到这个密钥，然后再用server的公钥加密，发送给server 两端的HTTPS通信，就都被charles捕获到了 ","date":"2017-08-19","objectID":"/https-charles/:0:3","tags":["Tech Notes"],"title":"从Charles抓包理解HTTPS","uri":"/https-charles/"},{"categories":null,"content":"抓包步骤 配置HTTP代理 proxy \u003e Proxy settings \u003e 选择在8888端口上监听，然后确定; 勾选SOCKS proxy，还能截获到浏览器的http访问请求。 配置SSL代理 proxy \u003e SSL Proxy settings \u003e勾选 Enable SSL Proxying; 点add添加需要监视的域名，支持 *号通配符，端口一般都是443 电脑安装charles根证书(抓取电脑流量) Help \u003e SSL Proxying \u003e Install Charles Root Certificate 手机安装charles根证书（抓取手机流量） 在Safri上打开Charles的根证书下载网址： chls.pro/ssl；点击安装 配置完成，手机端wifi设置里开启手动开启代理，ip为电脑IP，端口为8888(charles配置） ","date":"2017-08-19","objectID":"/https-charles/:0:4","tags":["Tech Notes"],"title":"从Charles抓包理解HTTPS","uri":"/https-charles/"},{"categories":null,"content":"发现问题 当我们发现脚本或者任务执行的时间不符合我们的预期，这个时候就应该想办法优化了。 首先分析慢，慢的原因是什么 这个时候可以借助像 pfofile 或者性能更好的 cprofile 来调查函数的运行的耗时。借助 line_profiler分析每行代码的耗时。 此次的优化直接分析代码： recon_db = ReconciliationMongo(NODE_DEFAULT).transaction cil_db = CILMongo(NODE_DEFAULT).transaction cil_transactions = list(cil_db.find(query)) count = len(cil_transactions) index = 1 for cil_transaction in cil_transactions: try: merchant_code = cil_transaction['merchant_id'] business_config = BusinessConfig.objects.filter(merchant_code=merchant_code).first() business_uid = business_config.business.uid.hex if business_config else None business_name = business_config.business.name if business_config else None order_number = None 发现上面代码的耗时其实主要就是在与mongo的网络IO上。 ","date":"2017-08-12","objectID":"/multithreads-optmize/:0:1","tags":["Tech Notes","Python"],"title":"记一次优化I/O密集型任务","uri":"/multithreads-optmize/"},{"categories":null,"content":"优化办法 问题既然是在IO上，自然想到利用多线程应该能加快执行时间 引入Python的并发库 concurrent.futures来启用多线程 cil_db = CILMongo(NODE_DEFAULT).transaction cil_transactions = cil_db.find(query) count = cil_transactions.count() with concurrent.futures.ThreadPoolExecutor(max_workers=threads_num) as executor: index = 1 futures = (executor.submit(process, cil_transaction) for cil_transaction in cil_transactions) for future in concurrent.futures.as_completed(futures): if future.done(): print('{}/{}'.format(index, count)) index += 1 def process(cil_transaction): # 一波操作 这里启用多少线程数量，一般推荐 处理器数量 * 5。 应该根据CPU的使用情况测试找到一个比较合适的数量，这样效果比较理想。 ","date":"2017-08-12","objectID":"/multithreads-optmize/:0:2","tags":["Tech Notes","Python"],"title":"记一次优化I/O密集型任务","uri":"/multithreads-optmize/"},{"categories":null,"content":"测试运行(本地) 优化之前跑完指定的数据量，耗时 340-350s 优化之后跑完相同的数据量，10个线程 耗时 40-45s；15个线程30-32s 此次优化只是任务的一部分，继续》》》》》 ","date":"2017-08-12","objectID":"/multithreads-optmize/:0:3","tags":["Tech Notes","Python"],"title":"记一次优化I/O密集型任务","uri":"/multithreads-optmize/"},{"categories":null,"content":"问题2 利用collection.Counter对字典做累加操作 counter = Counter({ 'amount': 0, 'fee': 0, 'original_amount': 0, 'settlement_amount': 0, 'deduct_points_amount': 0, 'deduct_prepay_amount': 0, 'deduct_pay_buff_amount': 0, 'deduct_direct_amount': 0, 'deduct_tracker_amount': 0, 'deduct_huipay_amount': 0, }) for transaction in transactions: counter += Counter({ 'amount': float(transaction['merchant_amount'].to_decimal()), 'fee': float(transaction['merchant_processing_fee'].to_decimal()), 'original_amount': float(transaction['original_amount'].to_decimal()), 'settlement_amount': float(transaction['merchant_settlement_amount'].to_decimal()), 'deduct_points_amount': float(transaction.get('deduct_points_amount', Decimal128('0')).to_decimal()), 'deduct_prepay_amount': float(transaction.get('deduct_prepay_amount', Decimal128('0')).to_decimal()), 'deduct_pay_buff_amount': float( transaction.get('deduct_pay_buff_amount', Decimal128('0')).to_decimal()), 'deduct_direct_amount': float(transaction.get('deduct_direct_amount', Decimal128('0')).to_decimal()), 'deduct_tracker_amount': float(transaction.get('deduct_tracker_amount', Decimal128('0')).to_decimal()), 'deduct_huipay_amount': float(transaction.get('deduct_huipay_amount', Decimal128('0')).to_decimal()), }) 分析Counter做累加性能可能不如直接对字典做累加操作，在ipython中做如下测试 用 %prun func() # profile 粗粒度分析整个函数的运行时间 用 %lprun -f func func() # line_profiler 分析每一行代码的运行时间。首先通过 %load_ext line_profiler显示导入line__profiler 才能运行 %lprun 下面为测试结果： 上面测试结果清晰的展示 Counter 没给我们带来性能上的提升。所以，我们在写代码爽快的同时，当对代码性能有要求时，不确定的时候应该多验证，多测试，找到比较理想的方案。 ","date":"2017-08-12","objectID":"/multithreads-optmize/:0:4","tags":["Tech Notes","Python"],"title":"记一次优化I/O密集型任务","uri":"/multithreads-optmize/"},{"categories":null,"content":"发现问题 当我们发现脚本或者任务执行的时间不符合我们的预期，这个时候就应该想办法优化了。 首先分析慢，慢的原因是什么 这个时候可以借助像 pfofile 或者性能更好的 cprofile 来调查函数的运行的耗时。借助 line_profiler分析每行代码的耗时。 此次的优化直接分析代码： recon_db = ReconciliationMongo(NODE_DEFAULT).transaction cil_db = CILMongo(NODE_DEFAULT).transaction cil_transactions = list(cil_db.find(query)) count = len(cil_transactions) index = 1 for cil_transaction in cil_transactions: try: merchant_code = cil_transaction['merchant_id'] business_config = BusinessConfig.objects.filter(merchant_code=merchant_code).first() business_uid = business_config.business.uid.hex if business_config else None business_name = business_config.business.name if business_config else None order_number = None 发现上面代码的耗时其实主要就是在与mongo的网络IO上。 ","date":"2017-08-12","objectID":"/multithreads-optmize/:0:1","tags":["Tech Notes","Python"],"title":"记一次优化I/O密集型任务","uri":"/multithreads-optmize/"},{"categories":null,"content":"优化办法 问题既然是在IO上，自然想到利用多线程应该能加快执行时间 引入Python的并发库 concurrent.futures来启用多线程 cil_db = CILMongo(NODE_DEFAULT).transaction cil_transactions = cil_db.find(query) count = cil_transactions.count() with concurrent.futures.ThreadPoolExecutor(max_workers=threads_num) as executor: index = 1 futures = (executor.submit(process, cil_transaction) for cil_transaction in cil_transactions) for future in concurrent.futures.as_completed(futures): if future.done(): print('{}/{}'.format(index, count)) index += 1 def process(cil_transaction): # 一波操作 这里启用多少线程数量，一般推荐 处理器数量 * 5。 应该根据CPU的使用情况测试找到一个比较合适的数量，这样效果比较理想。 ","date":"2017-08-12","objectID":"/multithreads-optmize/:0:2","tags":["Tech Notes","Python"],"title":"记一次优化I/O密集型任务","uri":"/multithreads-optmize/"},{"categories":null,"content":"测试运行(本地) 优化之前跑完指定的数据量，耗时 340-350s 优化之后跑完相同的数据量，10个线程 耗时 40-45s；15个线程30-32s 此次优化只是任务的一部分，继续》》》》》 ","date":"2017-08-12","objectID":"/multithreads-optmize/:0:3","tags":["Tech Notes","Python"],"title":"记一次优化I/O密集型任务","uri":"/multithreads-optmize/"},{"categories":null,"content":"问题2 利用collection.Counter对字典做累加操作 counter = Counter({ 'amount': 0, 'fee': 0, 'original_amount': 0, 'settlement_amount': 0, 'deduct_points_amount': 0, 'deduct_prepay_amount': 0, 'deduct_pay_buff_amount': 0, 'deduct_direct_amount': 0, 'deduct_tracker_amount': 0, 'deduct_huipay_amount': 0, }) for transaction in transactions: counter += Counter({ 'amount': float(transaction['merchant_amount'].to_decimal()), 'fee': float(transaction['merchant_processing_fee'].to_decimal()), 'original_amount': float(transaction['original_amount'].to_decimal()), 'settlement_amount': float(transaction['merchant_settlement_amount'].to_decimal()), 'deduct_points_amount': float(transaction.get('deduct_points_amount', Decimal128('0')).to_decimal()), 'deduct_prepay_amount': float(transaction.get('deduct_prepay_amount', Decimal128('0')).to_decimal()), 'deduct_pay_buff_amount': float( transaction.get('deduct_pay_buff_amount', Decimal128('0')).to_decimal()), 'deduct_direct_amount': float(transaction.get('deduct_direct_amount', Decimal128('0')).to_decimal()), 'deduct_tracker_amount': float(transaction.get('deduct_tracker_amount', Decimal128('0')).to_decimal()), 'deduct_huipay_amount': float(transaction.get('deduct_huipay_amount', Decimal128('0')).to_decimal()), }) 分析Counter做累加性能可能不如直接对字典做累加操作，在ipython中做如下测试 用 %prun func() # profile 粗粒度分析整个函数的运行时间 用 %lprun -f func func() # line_profiler 分析每一行代码的运行时间。首先通过 %load_ext line_profiler显示导入line__profiler 才能运行 %lprun 下面为测试结果： 上面测试结果清晰的展示 Counter 没给我们带来性能上的提升。所以，我们在写代码爽快的同时，当对代码性能有要求时，不确定的时候应该多验证，多测试，找到比较理想的方案。 ","date":"2017-08-12","objectID":"/multithreads-optmize/:0:4","tags":["Tech Notes","Python"],"title":"记一次优化I/O密集型任务","uri":"/multithreads-optmize/"},{"categories":null,"content":"实现方法总述 用 zing-gallery 启动一个相册服务(nodejs), 使用很简单, 一个命令 npm run start就启动了。 修改blog页面代码 把相册页面链接到blog菜单栏 有点像把🐘装进冰箱的操作哈！！ ","date":"2017-08-05","objectID":"/blog-zing-gallery/:1:0","tags":["Tech Notes"],"title":"使用 zing-gallery 添加相册","uri":"/blog-zing-gallery/"},{"categories":null,"content":"分步走起 ","date":"2017-08-05","objectID":"/blog-zing-gallery/:2:0","tags":["Tech Notes"],"title":"使用 zing-gallery 添加相册","uri":"/blog-zing-gallery/"},{"categories":null,"content":"1. 安装启动相册服务 clone zing-gallery 到服务器上 git clone https://github.com/lxkaka/zing-gallery.git 把照片放到 resources/photos 下，相册的相关配置比如封面，密码都在 config.js 下设置。修改方式可参照文件里的配置。举例： module.exports = { title: ’lxkaka Gallery’, wording: { noAccess: ‘抱歉，你没有权限访问’ }, albums: { “个人”: { description : “私密”, name: “个人”, password: “233”, //passwordTips: “密码是233” }, “landscape”: { description : “风光掠影”, thumbnail : “WechatIMG24.jpg” }, “cj2017”: { description : “2017cj 小记”, thumbnail : “IMG_1646.jpg” } } } * 启动相册服务进程 前提 npm 已经安装 `npm run start` 默认端口是3000, 可在 *app.js* 里修改 现在访问 **服务器域名或ip:3000** 应该就能看到自己的相册 ### 2. 修改主菜单ejs,添加相册入口 在 *layout/_partial/left_col* 下修改主菜单部分 \u003c% for (var i in theme.menu){ %\u003e \u003c%= i %\u003e \u003c%} %\u003e 我的相册 ``` ","date":"2017-08-05","objectID":"/blog-zing-gallery/:2:1","tags":["Tech Notes"],"title":"使用 zing-gallery 添加相册","uri":"/blog-zing-gallery/"},{"categories":null,"content":"3. 配置nginx监听相册请求 修改ningx配置文件，添加转发配置 # pass the gallery request to node server listening on 127.0.0.1:3000 location /gallery/ { proxy_pass http://127.0.0.1:3000/; } 这里需要注意nginx的 proxy_pass 路径的问题，代理转发时，如果在 proxy_pass后面的url带上 /，表示绝对路径；没有 **/**则为相对路径。 所以上面例子，当nignx匹配到 /gallery/ 的路径时，转发路径是 http://127.0.0.1:3000/ 而不是 http://127.0.0.1:3000/gallery ","date":"2017-08-05","objectID":"/blog-zing-gallery/:2:2","tags":["Tech Notes"],"title":"使用 zing-gallery 添加相册","uri":"/blog-zing-gallery/"},{"categories":null,"content":"4. 用supervisor管理进程 supervisor这里就不多说了，直接来使用方法。 首先 ubuntu下推荐用 apt-get 方式安装supervisor, 因为这种方式自动生成配置文件，会开机自动启动supervisord，不需要手动设置。（当然可以选择 pip install） apt-get install supervisor 修改配置supervisor服务端配置文件 supervisord.conf 默认位置 /etc/supervisord.conf 增加进程管理配置文件，推荐用 include 方式。 [include] files = /etc/supervisor/conf.d/*.conf 在上面include路径下添加进程管理配置文件 node_gallery.conf ``` [program:zing-gallery] ;进程名，对应supervisor客户端supervisorctl中对进程管理的名字 command=npm run start ;启动命令 autostart=true ;supervisord启动的时候启动 directory=/home/lxkaka/zing-gallery ;进程的启动目录 autorestart=true ;进程异常退出后自动重启 startsecs=10 ;进程启动多少秒之后，此时状态如果是running，则认为启动成功 startretries=5 ;最大启动重试次数 ``` 其他详细配置可参看 http://www.cnblogs.com/ajianbeyourself/p/5534737.html 至此，相册服务也实现了自动启动和重启。相册功能算是添加完成了。 zing-gallery项目地址 https://github.com/litten/zing-gallery ","date":"2017-08-05","objectID":"/blog-zing-gallery/:2:3","tags":["Tech Notes"],"title":"使用 zing-gallery 添加相册","uri":"/blog-zing-gallery/"},{"categories":null,"content":"实现方法总述 用 zing-gallery 启动一个相册服务(nodejs), 使用很简单, 一个命令 npm run start就启动了。 修改blog页面代码 把相册页面链接到blog菜单栏 有点像把🐘装进冰箱的操作哈！！ ","date":"2017-08-05","objectID":"/blog-zing-gallery/:1:0","tags":["Tech Notes"],"title":"使用 zing-gallery 添加相册","uri":"/blog-zing-gallery/"},{"categories":null,"content":"分步走起 ","date":"2017-08-05","objectID":"/blog-zing-gallery/:2:0","tags":["Tech Notes"],"title":"使用 zing-gallery 添加相册","uri":"/blog-zing-gallery/"},{"categories":null,"content":"1. 安装启动相册服务 clone zing-gallery 到服务器上 git clone https://github.com/lxkaka/zing-gallery.git 把照片放到 resources/photos 下，相册的相关配置比如封面，密码都在 config.js 下设置。修改方式可参照文件里的配置。举例： module.exports = { title: ’lxkaka Gallery’, wording: { noAccess: ‘抱歉，你没有权限访问’ }, albums: { “个人”: { description : “私密”, name: “个人”, password: “233”, //passwordTips: “密码是233” }, “landscape”: { description : “风光掠影”, thumbnail : “WechatIMG24.jpg” }, “cj2017”: { description : “2017cj 小记”, thumbnail : “IMG_1646.jpg” } } } * 启动相册服务进程 前提 npm 已经安装 `npm run start` 默认端口是3000, 可在 *app.js* 里修改 现在访问 **服务器域名或ip:3000** 应该就能看到自己的相册 ### 2. 修改主菜单ejs,添加相册入口 在 *layout/_partial/left_col* 下修改主菜单部分 \u003c% for (var i in theme.menu){ %\u003e \u003c%= i %\u003e \u003c%} %\u003e 我的相册 ``` ","date":"2017-08-05","objectID":"/blog-zing-gallery/:2:1","tags":["Tech Notes"],"title":"使用 zing-gallery 添加相册","uri":"/blog-zing-gallery/"},{"categories":null,"content":"3. 配置nginx监听相册请求 修改ningx配置文件，添加转发配置 # pass the gallery request to node server listening on 127.0.0.1:3000 location /gallery/ { proxy_pass http://127.0.0.1:3000/; } 这里需要注意nginx的 proxy_pass 路径的问题，代理转发时，如果在 proxy_pass后面的url带上 /，表示绝对路径；没有 **/**则为相对路径。 所以上面例子，当nignx匹配到 /gallery/ 的路径时，转发路径是 http://127.0.0.1:3000/ 而不是 http://127.0.0.1:3000/gallery ","date":"2017-08-05","objectID":"/blog-zing-gallery/:2:2","tags":["Tech Notes"],"title":"使用 zing-gallery 添加相册","uri":"/blog-zing-gallery/"},{"categories":null,"content":"4. 用supervisor管理进程 supervisor这里就不多说了，直接来使用方法。 首先 ubuntu下推荐用 apt-get 方式安装supervisor, 因为这种方式自动生成配置文件，会开机自动启动supervisord，不需要手动设置。（当然可以选择 pip install） apt-get install supervisor 修改配置supervisor服务端配置文件 supervisord.conf 默认位置 /etc/supervisord.conf 增加进程管理配置文件，推荐用 include 方式。 [include] files = /etc/supervisor/conf.d/*.conf 在上面include路径下添加进程管理配置文件 node_gallery.conf ``` [program:zing-gallery] ;进程名，对应supervisor客户端supervisorctl中对进程管理的名字 command=npm run start ;启动命令 autostart=true ;supervisord启动的时候启动 directory=/home/lxkaka/zing-gallery ;进程的启动目录 autorestart=true ;进程异常退出后自动重启 startsecs=10 ;进程启动多少秒之后，此时状态如果是running，则认为启动成功 startretries=5 ;最大启动重试次数 ``` 其他详细配置可参看 http://www.cnblogs.com/ajianbeyourself/p/5534737.html 至此，相册服务也实现了自动启动和重启。相册功能算是添加完成了。 zing-gallery项目地址 https://github.com/litten/zing-gallery ","date":"2017-08-05","objectID":"/blog-zing-gallery/:2:3","tags":["Tech Notes"],"title":"使用 zing-gallery 添加相册","uri":"/blog-zing-gallery/"},{"categories":null,"content":"部署架构 目的很简单，就是在服务器上搭建git服务(自己服务器上的GitHub). hexo部署的时候，把本地repo推到远程repo. 利用 git hook触发远程repo更新 服务器的根目录指向远程repo ","date":"2017-07-22","objectID":"/deploy-to-aliyun-ecs/:1:0","tags":["Tech Notes"],"title":"Hexo blog部署到阿里云ecs小结","uri":"/deploy-to-aliyun-ecs/"},{"categories":null,"content":"部署步骤 ","date":"2017-07-22","objectID":"/deploy-to-aliyun-ecs/:2:0","tags":["Tech Notes"],"title":"Hexo blog部署到阿里云ecs小结","uri":"/deploy-to-aliyun-ecs/"},{"categories":null,"content":"1.服务器搭建git服务 安装git 以 Ubuntu为例，user.name user.email配置随意 sudo apt-get install git 建立远程repo mkdir /home/lxkaka/blog cd /hoem/lxkaka/blog git init --bare blog.git ## --bare指明为裸仓库，看不到内容 编写hook脚本 vi blog.git/hooks/post-receive # !/bin/sh echo \"Upstate Start...\" unset GIT_DIR git pull origin master echo \"Upstate Success!\" 保存并修改为可执行权限： chmod +x hoos/post-receive ","date":"2017-07-22","objectID":"/deploy-to-aliyun-ecs/:2:1","tags":["Tech Notes"],"title":"Hexo blog部署到阿里云ecs小结","uri":"/deploy-to-aliyun-ecs/"},{"categories":null,"content":"2. Nginx安装及配置 服务器上如果没有安装，先apt-get install 然后修改nginx 配置文件 /etc/nginx/nginx.conf 看到里面并没有server部分，引入了别的配置文件。 编辑/etc/nginx/sites-enabled下的配置文件 ``` server { listen 80; #端口 server_name localhost 你的域名或者ip; #域名或IP root /home/lxkaka/blog; #站点根目录 charset utf-8; #文件编码 index index.html index.htm; #首页 error_page 404 /404.html; #404页面 error_page 500 502 503 504 /50x.html; #服务端错误页面 #url访问匹配路径，可以添加多个 location / { index index.html index.htm; root /home/lxkaka/blog; #这里可以是绝对路径或者相对路径，基于站点根目录 } } * 检查nginx配置 `nginx -t -c /etc/nginx/nginx.conf` `nignx -t ## 在当前配置文件目录下,简写形式` * 重启nignx `/etc/init.d/nignx restart` 至此远端服务器的工作完成 ### 3. 本地修改配置 * 修改hexo目录下的_config.yml ``` deploy: type: git repo: 用户@域名或ip:/home/lxkaka/blog/bog.git branch: master ``` * 测试： 修改或新建某篇blog后 `hexo g --d` * 在服务器上clone一下repo,才能看到发布的静态文件 `cd /home/lxkaka/blog` `git clone blog.git` 如果一切顺利，访问你的域名，博客就呈现出来了。 ### 4. 可能遇到的问题： * nginx配置好后，访问500 解决：问题是ecs的安全组规则拒绝80端口的访问，去管理后台配置一下安全组规则。 * 访问403 解决: 查看nginx进程的拥有者看是否具有相关文件的权限 `ps aux|grep nginx`, 发现 `worker`的拥有者是www-data. 修改`nignx.conf`的user 为`root`或者具有权限的用户。 * cannot run hooks/post-receive: No such file or directory 解决：检查脚本内容是否正确，是否拥有权限。 ","date":"2017-07-22","objectID":"/deploy-to-aliyun-ecs/:2:2","tags":["Tech Notes"],"title":"Hexo blog部署到阿里云ecs小结","uri":"/deploy-to-aliyun-ecs/"},{"categories":null,"content":"部署架构 目的很简单，就是在服务器上搭建git服务(自己服务器上的GitHub). hexo部署的时候，把本地repo推到远程repo. 利用 git hook触发远程repo更新 服务器的根目录指向远程repo ","date":"2017-07-22","objectID":"/deploy-to-aliyun-ecs/:1:0","tags":["Tech Notes"],"title":"Hexo blog部署到阿里云ecs小结","uri":"/deploy-to-aliyun-ecs/"},{"categories":null,"content":"部署步骤 ","date":"2017-07-22","objectID":"/deploy-to-aliyun-ecs/:2:0","tags":["Tech Notes"],"title":"Hexo blog部署到阿里云ecs小结","uri":"/deploy-to-aliyun-ecs/"},{"categories":null,"content":"1.服务器搭建git服务 安装git 以 Ubuntu为例，user.name user.email配置随意 sudo apt-get install git 建立远程repo mkdir /home/lxkaka/blog cd /hoem/lxkaka/blog git init --bare blog.git ## --bare指明为裸仓库，看不到内容 编写hook脚本 vi blog.git/hooks/post-receive # !/bin/sh echo \"Upstate Start...\" unset GIT_DIR git pull origin master echo \"Upstate Success!\" 保存并修改为可执行权限： chmod +x hoos/post-receive ","date":"2017-07-22","objectID":"/deploy-to-aliyun-ecs/:2:1","tags":["Tech Notes"],"title":"Hexo blog部署到阿里云ecs小结","uri":"/deploy-to-aliyun-ecs/"},{"categories":null,"content":"2. Nginx安装及配置 服务器上如果没有安装，先apt-get install 然后修改nginx 配置文件 /etc/nginx/nginx.conf 看到里面并没有server部分，引入了别的配置文件。 编辑/etc/nginx/sites-enabled下的配置文件 ``` server { listen 80; #端口 server_name localhost 你的域名或者ip; #域名或IP root /home/lxkaka/blog; #站点根目录 charset utf-8; #文件编码 index index.html index.htm; #首页 error_page 404 /404.html; #404页面 error_page 500 502 503 504 /50x.html; #服务端错误页面 #url访问匹配路径，可以添加多个 location / { index index.html index.htm; root /home/lxkaka/blog; #这里可以是绝对路径或者相对路径，基于站点根目录 } } * 检查nginx配置 `nginx -t -c /etc/nginx/nginx.conf` `nignx -t ## 在当前配置文件目录下,简写形式` * 重启nignx `/etc/init.d/nignx restart` 至此远端服务器的工作完成 ### 3. 本地修改配置 * 修改hexo目录下的_config.yml ``` deploy: type: git repo: 用户@域名或ip:/home/lxkaka/blog/bog.git branch: master ``` * 测试： 修改或新建某篇blog后 `hexo g --d` * 在服务器上clone一下repo,才能看到发布的静态文件 `cd /home/lxkaka/blog` `git clone blog.git` 如果一切顺利，访问你的域名，博客就呈现出来了。 ### 4. 可能遇到的问题： * nginx配置好后，访问500 解决：问题是ecs的安全组规则拒绝80端口的访问，去管理后台配置一下安全组规则。 * 访问403 解决: 查看nginx进程的拥有者看是否具有相关文件的权限 `ps aux|grep nginx`, 发现 `worker`的拥有者是www-data. 修改`nignx.conf`的user 为`root`或者具有权限的用户。 * cannot run hooks/post-receive: No such file or directory 解决：检查脚本内容是否正确，是否拥有权限。 ","date":"2017-07-22","objectID":"/deploy-to-aliyun-ecs/:2:2","tags":["Tech Notes"],"title":"Hexo blog部署到阿里云ecs小结","uri":"/deploy-to-aliyun-ecs/"},{"categories":null,"content":"Signals 用处 django有一个“信号分配器”（signal dispatcher）使得当一些动作在框架的其他地方发生的时候，解耦的应用可以得到提醒。简单说，就是一些动作发生的时候，信号允许特定的发送者去提醒一些接受者，这是很有用的设计，因为可能多处代码对某些事件是很感兴趣，比如删除动作。 为此，django提供了很多内置的信号，当django触发某些动作时而通知到用户代码，包括： django.db.models.signals.pre_save \u0026 django.db.models.signals.post_save modle’s save()被调用之前或之后，发送signal django.db.models.signals.pre_delete \u0026 django.db.models.signals.post_delete model’s delete()或者query_set’s delete()被调用之前或之后，发送signal django.db.models.signals.m2m_changed 当modle的 ManyToManyField有修改时触发 django.core.signals.request_started \u0026 django.core.signals.request_finished 当Django开始或者完成一个http请求 同样Django支持自定义Signal 如: from django.dispatch import Signal my_signal = Signal(providing_args=['instance', 'content']) ","date":"2017-07-17","objectID":"/django_signals/:1:0","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"监听 signals 要想接受信号，首先要用Signal.connect()注册一个接收器函数。发送signal,则接收函数被调用 Signal.connect(receiver[,sender=None,weak=True,dispatch_uid=None]) 参数解释： receiver：连接到这个信号的回调函数 sender：信号的发送者 weak：是否是弱引用，默认是真。因此，如果你的接收器是是一个本地函数，会被当做垃圾回收，如果你不想，请在使用connect()方法的时候使用weak=False(1.9中以后被废弃） dispatch_uid：一个唯一的标识符给信号接收器，避免重复的信号被发送 ","date":"2017-07-17","objectID":"/django_signals/:2:0","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"定义接收函数 def my_callback(sendr, **kwargs): print 'request finished' 注意的是所有的信号处理器都需要这两个参数：sender和 **kwargs。因为所有的信号都是发送关键字参数的，可能处理的时候没有任何参数，但不意味着在处理的过程中（在你写的处理函数之前）有任何的参数生成，如果没有传 kwargs参数的话，可能会发生问题；基于这样的考虑，这两个参数都是必须的。 ","date":"2017-07-17","objectID":"/django_signals/:3:0","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"连接接收函数 有两中方法可以使用 手动connect from django.core.signals import request_finished request_finished.connect(my_callback) 用装饰器receiver from django.core.signals import request_finished from django.dispatch import receiver @receiver(request_finished) def my_callback(sender, **kwargs): print(\"Request finished!\") ","date":"2017-07-17","objectID":"/django_signals/:4:0","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"使用完整示例 from django.db.models.signals import post_save from django.dispatch import receiver @receiver(post_save, sender=GroupSettings) def group_settings_post_save_trigger(sender, instance, created, *args, **kwargs): \"\"\" 本方法会在 GroupSettings.save() 后调用，量大，并且并不保证事务一定成功 所以一定注意本方法内一定是调用异步任务，并且业务逻辑并没有严格的数据一致性需求 :param sender: :type instance: GroupSettings :type created: bool :param args: :param kwargs: \"\"\" from api.activity_manager import send_feed_message from api.constants import FeedType if created and instance.business_group: for business in Business.objects.filter(business_group=instance.business_group): send_feed_message(business, FeedType.REBOOT) ","date":"2017-07-17","objectID":"/django_signals/:5:0","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"注意： update() 并不会触发pre_save, post_save信号 所以当有这样的操作： GroupSettings.objects.filter(business_group=business_group).update(**filterd) 想触发信号发送时，可以这样修改： group = GroupSettings.objects.filter(business_group=business_group).first() if group: for key, value in fileted.items(): setattr(group, key, value) group.save() ","date":"2017-07-17","objectID":"/django_signals/:5:1","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"Signals 用处 django有一个“信号分配器”（signal dispatcher）使得当一些动作在框架的其他地方发生的时候，解耦的应用可以得到提醒。简单说，就是一些动作发生的时候，信号允许特定的发送者去提醒一些接受者，这是很有用的设计，因为可能多处代码对某些事件是很感兴趣，比如删除动作。 为此，django提供了很多内置的信号，当django触发某些动作时而通知到用户代码，包括： django.db.models.signals.pre_save \u0026 django.db.models.signals.post_save modle’s save()被调用之前或之后，发送signal django.db.models.signals.pre_delete \u0026 django.db.models.signals.post_delete model’s delete()或者query_set’s delete()被调用之前或之后，发送signal django.db.models.signals.m2m_changed 当modle的 ManyToManyField有修改时触发 django.core.signals.request_started \u0026 django.core.signals.request_finished 当Django开始或者完成一个http请求 同样Django支持自定义Signal 如: from django.dispatch import Signal my_signal = Signal(providing_args=['instance', 'content']) ","date":"2017-07-17","objectID":"/django_signals/:1:0","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"监听 signals 要想接受信号，首先要用Signal.connect()注册一个接收器函数。发送signal,则接收函数被调用 Signal.connect(receiver[,sender=None,weak=True,dispatch_uid=None]) 参数解释： receiver：连接到这个信号的回调函数 sender：信号的发送者 weak：是否是弱引用，默认是真。因此，如果你的接收器是是一个本地函数，会被当做垃圾回收，如果你不想，请在使用connect()方法的时候使用weak=False(1.9中以后被废弃） dispatch_uid：一个唯一的标识符给信号接收器，避免重复的信号被发送 ","date":"2017-07-17","objectID":"/django_signals/:2:0","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"定义接收函数 def my_callback(sendr, **kwargs): print 'request finished' 注意的是所有的信号处理器都需要这两个参数：sender和 **kwargs。因为所有的信号都是发送关键字参数的，可能处理的时候没有任何参数，但不意味着在处理的过程中（在你写的处理函数之前）有任何的参数生成，如果没有传 kwargs参数的话，可能会发生问题；基于这样的考虑，这两个参数都是必须的。 ","date":"2017-07-17","objectID":"/django_signals/:3:0","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"连接接收函数 有两中方法可以使用 手动connect from django.core.signals import request_finished request_finished.connect(my_callback) 用装饰器receiver from django.core.signals import request_finished from django.dispatch import receiver @receiver(request_finished) def my_callback(sender, **kwargs): print(\"Request finished!\") ","date":"2017-07-17","objectID":"/django_signals/:4:0","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"使用完整示例 from django.db.models.signals import post_save from django.dispatch import receiver @receiver(post_save, sender=GroupSettings) def group_settings_post_save_trigger(sender, instance, created, *args, **kwargs): \"\"\" 本方法会在 GroupSettings.save() 后调用，量大，并且并不保证事务一定成功 所以一定注意本方法内一定是调用异步任务，并且业务逻辑并没有严格的数据一致性需求 :param sender: :type instance: GroupSettings :type created: bool :param args: :param kwargs: \"\"\" from api.activity_manager import send_feed_message from api.constants import FeedType if created and instance.business_group: for business in Business.objects.filter(business_group=instance.business_group): send_feed_message(business, FeedType.REBOOT) ","date":"2017-07-17","objectID":"/django_signals/:5:0","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":"注意： update() 并不会触发pre_save, post_save信号 所以当有这样的操作： GroupSettings.objects.filter(business_group=business_group).update(**filterd) 想触发信号发送时，可以这样修改： group = GroupSettings.objects.filter(business_group=business_group).first() if group: for key, value in fileted.items(): setattr(group, key, value) group.save() ","date":"2017-07-17","objectID":"/django_signals/:5:1","tags":["Tech Notes","back-end","Django"],"title":"Django Signals小结","uri":"/django_signals/"},{"categories":null,"content":" 相关背景知识，从 WSGI 说起: Web应用框架的选择将限制可用的Web服务器的选择，反之亦然。那时的Python应用程序通常是为CGI，FastCGI，mod_python中的一个而设计，甚至是为特定Web服务器的自定义的API接口而设计的。WSGI 是为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口。WSGI区分为两个部分：一为“服务器”或“网关”，另一为“应用程序”或“应用框架”。在处理一个WSGI请求时，服务器会为应用程序提供环境信息及一个回调函数（Callback Function）。当应用程序完成处理请求后，通过前述的回调函数，将结果回传给服务器。 wsgi server可以理解为一个符合wsgi规范的web server，接收request请求，封装一系列环境变量，按照wsgi规范调用注册的wsgi app，最后将response返回给客户端。 a. 服务器创建socket，监听端口，等待客户端连接 b. 当有请求来时，服务器解析客户端信息放到环境变量environ中，并调用绑定的handler来处理请求 c. handler解析这个http请求，将请求信息例如method，path等放到environ中 d. wsgi handler再将一些服务器端信息也放到environ中，最后服务器信息，客户端信息，本次请求信息全部都保存到了环境变量environ中 e. wsgi handler 调用注册的wsgi app，并将environ和回调函数（start_response)传给wsgi app f. wsgi app 将reponse header/status/body 回传给wsgi handler g. 最终handler还是通过socket将response信息塞回给客户端。 CGI: 外部应用程序（CGI程序）与Web服务器之间的接口标准，是在CGI程序和Web服务器之间传递信息的规程。简单点说CGI就是规定Web server要传哪些数据、以什么样的格式传递给 CGI 程序。CGI方式在遇到连接请求（用户请求）先要创建cgi的子进程，激活一个CGI进程，然后处理请求，处理完后结束这个子进程。这就是fork-and-execute模式。所以用cgi方式的服务器有多少连接请求就会有多少cgi子进程，子进程反复加载是cgi性能低下的主要原因。当用户请求数量非常多时，会大量挤占系统的资源如内存，CPU时间等，造成效能低下。 FASTCGI： FastCGI是从CGI发展改进而来的。Fastcgi会先启一个master，解析配置文件，初始化执行环境，然后再启动多个worker。当请求过来时，master会传递给一个worker，然后立即可以接受下一个请求。这样就避免了重复的劳动，效率自然是高。而且当worker不够用时，master可以根据配置预先启动几个worker等着。 uWSGI uWSGI是一个Web服务器，它实现了WSGI协议、uwsgi、http等协议。uWSGI，既不用wsgi协议也不用FastCGI协议，而是自创了一个uwsgi的协议，uwsgi协议是一个uWSGI服务器自有的协议， 它是一个二进制协议,能够携带任意类型的信息，每一个uwsgi packet前4byte为传输信息类型描述，它与WSGI相比是两样东西。 uWSGI 保留了 fastcgi 的优点，实现进程控制，预先设置好启动多个worker处理请求。 nginx 通过 ngx_http_uwsgi_module模块把请求传递个 uWSGI 服务器。 示例配置 location / { include uwsgi_params; uwsgi_pass localhost:9000; } 常用配置参数 uwsgi_pass [protocol://]address; 设置uwsgi服务器的协议和地址，协议可是uwsgi或suwsgi（uwsgi over ssl）； 地址可以是ip地址，域名，和可选的端口： uwsgi_pass localhost:9000; uwsgi_pass uwsgi://localhost:9000; uwsgi_pass suwsgi://[2001:db8::1]:9090; 也可是unix socket： uwsgi_pass unix:/tmp/uwsgi.socket; uwsgi_read_timeout time; Default:uwsgi_read_timeout 60s; 定义从uwsgi服务器读取响应的超时时间，如果在超时时间内uwsgi服务器没有传输任何东西，连接会被断开。 uwsgi_params 定义了传递到uWSGI服务器的参数，示例： \u003cpre\u003e uwsgi_param QUERY_STRING $query_string; uwsgi_param REQUEST_METHOD $request_method; uwsgi_param CONTENT_TYPE $content_type; uwsgi_param CONTENT_LENGTH $content_length; uwsgi_param REQUEST_URI $request_uri; uwsgi_param PATH_INFO $document_uri; uwsgi_param DOCUMENT_ROOT $document_root; uwsgi_param SERVER_PROTOCOL $server_protocol; uwsgi_param REMOTE_ADDR $remote_addr; uwsgi_param REMOTE_PORT $remote_port; uwsgi_param SERVER_PORT $server_port; uwsgi_param SERVER_NAME $server_name; \u003c/pre\u003e ![nignx-uwsgi2](https://pics.lxkaka.wang/nginx_uwsgi2.png) * uWSGI 常用配置项 **socket or uwsgi-socket** 指定uwsgi的客户端将要连接的socket的路径（使用UNIX socket的情况）或者地址（使用网络地址的情况）。你最多可以同时指定8个socket选项。当使用命令行变量时，可以使用“-s”这个缩写。 `--socket /tmp/uwsgi.sock` 以上配置将会绑定到 /tmp/uwsgi.sock 指定的UNIX socket **protocol** 设置默认的通信协议（uwsgi，http，fastcgi） `--protocol \u003cprotocol\u003e` **processes or workers** 为预先派生模式设置工作进程的数量。这个设置是app能实现简单并且安全的并发能力的基础。设置的工作进程越多，就能越快的处理请求。每一个工作进程都等同于一个系统进程，它消耗内存，所以需要小心设置工作进程的数量。如果设置的数量太多，就有可能是系统崩溃。 `--processes 8` 以上配置会产生8个工作进程 **harakiri** 这个选项会设置harakiri超时时间。如果一个请求花费的时间超过了这个harakiri超时时间，那么这个请求都会被丢弃，并且当前处理这个请求的工作进程会被回收再利用（即重启）。 `--harakiri 60` 这个设置会使uwsgi丢弃所有需要60秒才能处理完成的请求。 harakiri-verbose 当一个请求被harakiri杀掉以后，你将在uWSGI日志中得到一条消息。激活这个选项会打印出额外的信息（例如，在linux中会打印出当前的syscall） `--harakiri-verbose` 以上配置会开启harakiri的额外信息。 **daemonize** 使进程在后台运行，并将日志打到指定的日志文件或者udp服务器--daemonize /var/log/uwsgi.log 这个指令会让uWSGI在后台运行并将日志打到 /var/log/uwsgi.log文件中。 **buffer-size** 设置用于uwsgi包解析的内部缓存区大小。默认是4k。如果接受一个拥有很多请求头的大请求，可以增加这个值到64k。 `--buffer-size 32768` 这个命令会允许uWSGI服务器接收最大为32k的uwsgi包，再大的包就会被拒绝。 **auto-procname** 这个选项将自动给uWSGI的进程设置一些有意义的名字，例如“uWSGI master”， “uWSGI worker 1”， “uWSGI worker 2”。 **procname-prefix** 这个选项为进程名指定前缀。 `--procname-prefix \u003cvalue\u003e` **procname-prefix-spaced** 用这个选项给进程名指定前缀时，前缀和进程名之间有空格分隔。 `--procname-prefix-spaced \u003cvalue\u003e` **procname-append** 这个选项为进程名增加指定的后缀。 `--procname-append \u003cvalue\u003e` **master** 启动主进程。 **max-requests** 为每个工作进程设置请求数的上限。当一个工作进程处理的请求数达到这个值，那么该工作进程就会被回收重用（重启）。你可以使用这个选项来默默地对抗内存泄漏（尽管这类情况使用rel","date":"2016-11-14","objectID":"/uwsgi1/:0:0","tags":["Tech Notes","back-end","web server","uwsgi"],"title":"uWSGI--一个WSGI server","uri":"/uwsgi1/"},{"categories":null,"content":" 相关背景知识，从 WSGI 说起: Web应用框架的选择将限制可用的Web服务器的选择，反之亦然。那时的Python应用程序通常是为CGI，FastCGI，mod_python中的一个而设计，甚至是为特定Web服务器的自定义的API接口而设计的。WSGI 是为Python语言定义的Web服务器和Web应用程序或框架之间的一种简单而通用的接口。WSGI区分为两个部分：一为“服务器”或“网关”，另一为“应用程序”或“应用框架”。在处理一个WSGI请求时，服务器会为应用程序提供环境信息及一个回调函数（Callback Function）。当应用程序完成处理请求后，通过前述的回调函数，将结果回传给服务器。 wsgi server可以理解为一个符合wsgi规范的web server，接收request请求，封装一系列环境变量，按照wsgi规范调用注册的wsgi app，最后将response返回给客户端。 a. 服务器创建socket，监听端口，等待客户端连接 b. 当有请求来时，服务器解析客户端信息放到环境变量environ中，并调用绑定的handler来处理请求 c. handler解析这个http请求，将请求信息例如method，path等放到environ中 d. wsgi handler再将一些服务器端信息也放到environ中，最后服务器信息，客户端信息，本次请求信息全部都保存到了环境变量environ中 e. wsgi handler 调用注册的wsgi app，并将environ和回调函数（start_response)传给wsgi app f. wsgi app 将reponse header/status/body 回传给wsgi handler g. 最终handler还是通过socket将response信息塞回给客户端。 CGI: 外部应用程序（CGI程序）与Web服务器之间的接口标准，是在CGI程序和Web服务器之间传递信息的规程。简单点说CGI就是规定Web server要传哪些数据、以什么样的格式传递给 CGI 程序。CGI方式在遇到连接请求（用户请求）先要创建cgi的子进程，激活一个CGI进程，然后处理请求，处理完后结束这个子进程。这就是fork-and-execute模式。所以用cgi方式的服务器有多少连接请求就会有多少cgi子进程，子进程反复加载是cgi性能低下的主要原因。当用户请求数量非常多时，会大量挤占系统的资源如内存，CPU时间等，造成效能低下。 FASTCGI： FastCGI是从CGI发展改进而来的。Fastcgi会先启一个master，解析配置文件，初始化执行环境，然后再启动多个worker。当请求过来时，master会传递给一个worker，然后立即可以接受下一个请求。这样就避免了重复的劳动，效率自然是高。而且当worker不够用时，master可以根据配置预先启动几个worker等着。 uWSGI uWSGI是一个Web服务器，它实现了WSGI协议、uwsgi、http等协议。uWSGI，既不用wsgi协议也不用FastCGI协议，而是自创了一个uwsgi的协议，uwsgi协议是一个uWSGI服务器自有的协议， 它是一个二进制协议,能够携带任意类型的信息，每一个uwsgi packet前4byte为传输信息类型描述，它与WSGI相比是两样东西。 uWSGI 保留了 fastcgi 的优点，实现进程控制，预先设置好启动多个worker处理请求。 nginx 通过 ngx_http_uwsgi_module模块把请求传递个 uWSGI 服务器。 示例配置 location / { include uwsgi_params; uwsgi_pass localhost:9000; } 常用配置参数 uwsgi_pass [protocol://]address; 设置uwsgi服务器的协议和地址，协议可是uwsgi或suwsgi（uwsgi over ssl）； 地址可以是ip地址，域名，和可选的端口： uwsgi_pass localhost:9000; uwsgi_pass uwsgi://localhost:9000; uwsgi_pass suwsgi://[2001:db8::1]:9090; 也可是unix socket： uwsgi_pass unix:/tmp/uwsgi.socket; uwsgi_read_timeout time; Default:uwsgi_read_timeout 60s; 定义从uwsgi服务器读取响应的超时时间，如果在超时时间内uwsgi服务器没有传输任何东西，连接会被断开。 uwsgi_params 定义了传递到uWSGI服务器的参数，示例： \u003cpre\u003e uwsgi_param QUERY_STRING $query_string; uwsgi_param REQUEST_METHOD $request_method; uwsgi_param CONTENT_TYPE $content_type; uwsgi_param CONTENT_LENGTH $content_length; uwsgi_param REQUEST_URI $request_uri; uwsgi_param PATH_INFO $document_uri; uwsgi_param DOCUMENT_ROOT $document_root; uwsgi_param SERVER_PROTOCOL $server_protocol; uwsgi_param REMOTE_ADDR $remote_addr; uwsgi_param REMOTE_PORT $remote_port; uwsgi_param SERVER_PORT $server_port; uwsgi_param SERVER_NAME $server_name; \u003c/pre\u003e ![nignx-uwsgi2](https://pics.lxkaka.wang/nginx_uwsgi2.png) * uWSGI 常用配置项 **socket or uwsgi-socket** 指定uwsgi的客户端将要连接的socket的路径（使用UNIX socket的情况）或者地址（使用网络地址的情况）。你最多可以同时指定8个socket选项。当使用命令行变量时，可以使用“-s”这个缩写。 `--socket /tmp/uwsgi.sock` 以上配置将会绑定到 /tmp/uwsgi.sock 指定的UNIX socket **protocol** 设置默认的通信协议（uwsgi，http，fastcgi） `--protocol \u003cprotocol\u003e` **processes or workers** 为预先派生模式设置工作进程的数量。这个设置是app能实现简单并且安全的并发能力的基础。设置的工作进程越多，就能越快的处理请求。每一个工作进程都等同于一个系统进程，它消耗内存，所以需要小心设置工作进程的数量。如果设置的数量太多，就有可能是系统崩溃。 `--processes 8` 以上配置会产生8个工作进程 **harakiri** 这个选项会设置harakiri超时时间。如果一个请求花费的时间超过了这个harakiri超时时间，那么这个请求都会被丢弃，并且当前处理这个请求的工作进程会被回收再利用（即重启）。 `--harakiri 60` 这个设置会使uwsgi丢弃所有需要60秒才能处理完成的请求。 harakiri-verbose 当一个请求被harakiri杀掉以后，你将在uWSGI日志中得到一条消息。激活这个选项会打印出额外的信息（例如，在linux中会打印出当前的syscall） `--harakiri-verbose` 以上配置会开启harakiri的额外信息。 **daemonize** 使进程在后台运行，并将日志打到指定的日志文件或者udp服务器--daemonize /var/log/uwsgi.log 这个指令会让uWSGI在后台运行并将日志打到 /var/log/uwsgi.log文件中。 **buffer-size** 设置用于uwsgi包解析的内部缓存区大小。默认是4k。如果接受一个拥有很多请求头的大请求，可以增加这个值到64k。 `--buffer-size 32768` 这个命令会允许uWSGI服务器接收最大为32k的uwsgi包，再大的包就会被拒绝。 **auto-procname** 这个选项将自动给uWSGI的进程设置一些有意义的名字，例如“uWSGI master”， “uWSGI worker 1”， “uWSGI worker 2”。 **procname-prefix** 这个选项为进程名指定前缀。 `--procname-prefix \u003cvalue\u003e` **procname-prefix-spaced** 用这个选项给进程名指定前缀时，前缀和进程名之间有空格分隔。 `--procname-prefix-spaced \u003cvalue\u003e` **procname-append** 这个选项为进程名增加指定的后缀。 `--procname-append \u003cvalue\u003e` **master** 启动主进程。 **max-requests** 为每个工作进程设置请求数的上限。当一个工作进程处理的请求数达到这个值，那么该工作进程就会被回收重用（重启）。你可以使用这个选项来默默地对抗内存泄漏（尽管这类情况使用rel","date":"2016-11-14","objectID":"/uwsgi1/:0:0","tags":["Tech Notes","back-end","web server","uwsgi"],"title":"uWSGI--一个WSGI server","uri":"/uwsgi1/"},{"categories":null,"content":"1. 行内元素和块级元素的学习 块级元素：总是以一个块的形式表现出来，并且跟同级的兄弟一次竖直排列，左右撑满。 行内元素：行内元素不占据单独的空间，依附于块级元素，行内元素没有自己的区域。它同样是DOM树的一个节点，在这点上，行内元素和块级元素是没有区别的。行内元素不可以设置宽与高，但可以与其他行内元素位于同一行，行内元素内一般不可以包含块级元素。行内元素的高度一般由元素内部的字体大小决定，宽度由内容的长度控制。常见的行内元素有a,em,strong等。 ","date":"2015-11-30","objectID":"/pageformatnotes/:1:0","tags":["Tech Notes","js","css"],"title":"页面外观样式笔记","uri":"/pageformatnotes/"},{"categories":null,"content":"行内元素的特点： 和其他元素都在一行上； 高，行高及外边距和内边距不可改变； 宽度就是它的文字或图片的宽度，不可改变 行内元素只能容纳文本或者其他行内元素 ","date":"2015-11-30","objectID":"/pageformatnotes/:1:1","tags":["Tech Notes","js","css"],"title":"页面外观样式笔记","uri":"/pageformatnotes/"},{"categories":null,"content":"块级元素的特点： 总是在新行上开始； 高度，行高以及外边距和内边距都可控制； 宽度缺省是它的容器的100%，除非设定一个宽度 它可以容纳内联元素和其他块元素 hexo generate 时出错： YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key at line 4, column 1 注意页面开始的title，date，tag冒号后必须有空格 updating…… ","date":"2015-11-30","objectID":"/pageformatnotes/:1:2","tags":["Tech Notes","js","css"],"title":"页面外观样式笔记","uri":"/pageformatnotes/"},{"categories":null,"content":"1. 行内元素和块级元素的学习 块级元素：总是以一个块的形式表现出来，并且跟同级的兄弟一次竖直排列，左右撑满。 行内元素：行内元素不占据单独的空间，依附于块级元素，行内元素没有自己的区域。它同样是DOM树的一个节点，在这点上，行内元素和块级元素是没有区别的。行内元素不可以设置宽与高，但可以与其他行内元素位于同一行，行内元素内一般不可以包含块级元素。行内元素的高度一般由元素内部的字体大小决定，宽度由内容的长度控制。常见的行内元素有a,em,strong等。 ","date":"2015-11-30","objectID":"/pageformatnotes/:1:0","tags":["Tech Notes","js","css"],"title":"页面外观样式笔记","uri":"/pageformatnotes/"},{"categories":null,"content":"行内元素的特点： 和其他元素都在一行上； 高，行高及外边距和内边距不可改变； 宽度就是它的文字或图片的宽度，不可改变 行内元素只能容纳文本或者其他行内元素 ","date":"2015-11-30","objectID":"/pageformatnotes/:1:1","tags":["Tech Notes","js","css"],"title":"页面外观样式笔记","uri":"/pageformatnotes/"},{"categories":null,"content":"块级元素的特点： 总是在新行上开始； 高度，行高以及外边距和内边距都可控制； 宽度缺省是它的容器的100%，除非设定一个宽度 它可以容纳内联元素和其他块元素 hexo generate 时出错： YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key at line 4, column 1 注意页面开始的title，date，tag冒号后必须有空格 updating…… ","date":"2015-11-30","objectID":"/pageformatnotes/:1:2","tags":["Tech Notes","js","css"],"title":"页面外观样式笔记","uri":"/pageformatnotes/"},{"categories":null,"content":"用Raphaeljs画图，细节记录 ","date":"2015-11-29","objectID":"/buidingdynamicdemo/:0:0","tags":["Tech Notes","js"],"title":"建立动态演示的记录","uri":"/buidingdynamicdemo/"},{"categories":null,"content":"1. paper.path的用法 举例：用Q command The Q command (or q for relative points) describes a curve drawn from the current point on a path to the point (x, y) using (x1, y1) as a control point. For example, consider the following code: paper.path(['M', 50, 150, 'Q', 225, 20, 400, 150]); 这样的表示方法，其中的坐标都是绝对值。 如果像这样写,则 c 后面的坐标都为相对起点的距离。 r.path(\"M320,240c-50,100,50,110,0,190\").attr({fill: \"none\", \"stroke-width\": 2}); This draws the quadratic Bézier curve shown. The equivalent path using the lowercase variant of the command would be “M 50,150 q 175,-130 350,0”, where the (x, y) and (x1, y1) parameters are the relative distances from the start point (50, 100): ","date":"2015-11-29","objectID":"/buidingdynamicdemo/:1:0","tags":["Tech Notes","js"],"title":"建立动态演示的记录","uri":"/buidingdynamicdemo/"},{"categories":null,"content":"2. 其他用法 给曲线加箭头 r.path(['M',xS,yS,'L',x1,y1]).attr({'arrow-end': 'classic-wide-long'}); 加文本 r.text(x,y,text).attr({'font-size': 20}); 其他参数可参考文档 画圆 r.circle(x,y,radius).attr({fill: 'none', stroke: color, 'stroke-width': 2}); 把多个raphael object 放到一个类似数组的对象里面，可以一次操作多个元素。 paper.set(); 例子: var pathSet = r.set(); if(peerChoice \u0026\u0026 peerPath){ pathSet.forEach(function(element,index){ element.remove(); }); } 一个比较好的raphael tutorial WebRTC + RLNC 分布式存储系统动态演示Demo截图 ","date":"2015-11-29","objectID":"/buidingdynamicdemo/:2:0","tags":["Tech Notes","js"],"title":"建立动态演示的记录","uri":"/buidingdynamicdemo/"},{"categories":null,"content":"用Raphaeljs画图，细节记录 ","date":"2015-11-29","objectID":"/buidingdynamicdemo/:0:0","tags":["Tech Notes","js"],"title":"建立动态演示的记录","uri":"/buidingdynamicdemo/"},{"categories":null,"content":"1. paper.path的用法 举例：用Q command The Q command (or q for relative points) describes a curve drawn from the current point on a path to the point (x, y) using (x1, y1) as a control point. For example, consider the following code: paper.path(['M', 50, 150, 'Q', 225, 20, 400, 150]); 这样的表示方法，其中的坐标都是绝对值。 如果像这样写,则 c 后面的坐标都为相对起点的距离。 r.path(\"M320,240c-50,100,50,110,0,190\").attr({fill: \"none\", \"stroke-width\": 2}); This draws the quadratic Bézier curve shown. The equivalent path using the lowercase variant of the command would be “M 50,150 q 175,-130 350,0”, where the (x, y) and (x1, y1) parameters are the relative distances from the start point (50, 100): ","date":"2015-11-29","objectID":"/buidingdynamicdemo/:1:0","tags":["Tech Notes","js"],"title":"建立动态演示的记录","uri":"/buidingdynamicdemo/"},{"categories":null,"content":"2. 其他用法 给曲线加箭头 r.path(['M',xS,yS,'L',x1,y1]).attr({'arrow-end': 'classic-wide-long'}); 加文本 r.text(x,y,text).attr({'font-size': 20}); 其他参数可参考文档 画圆 r.circle(x,y,radius).attr({fill: 'none', stroke: color, 'stroke-width': 2}); 把多个raphael object 放到一个类似数组的对象里面，可以一次操作多个元素。 paper.set(); 例子: var pathSet = r.set(); if(peerChoice \u0026\u0026 peerPath){ pathSet.forEach(function(element,index){ element.remove(); }); } 一个比较好的raphael tutorial WebRTC + RLNC 分布式存储系统动态演示Demo截图 ","date":"2015-11-29","objectID":"/buidingdynamicdemo/:2:0","tags":["Tech Notes","js"],"title":"建立动态演示的记录","uri":"/buidingdynamicdemo/"},{"categories":null,"content":"1.创建自己的Github page 登录 https://github.com/lxkaka ,创建一个新repository，repository name 设置为 lxkaka.github.io,其他选项默认。点确认 然后在这个仓库的页面 https://github.com/lxkaka/lxkaka.github.io 的右边栏点Settings 往下拉找到GitHub Pages -\u003e Automatic Page generator. 访问 https://lxkaka.github.io 就能看到页面。 ","date":"2015-11-24","objectID":"/blog-md/:1:0","tags":["Tech Notes"],"title":"Github Page+Hexo搭建blog","uri":"/blog-md/"},{"categories":null,"content":"2. 安装Hexo 首先安装 hexo modul $ npm install -g hexo 建立一个文件夹，在次文件夹下 $ hexo init $ npm install 本地的Hexo博客已经初步搭建起来，输入以下指令: $ hexo generate//可以简写为 hexo g $ hexo server//可以简写为 hexo s ","date":"2015-11-24","objectID":"/blog-md/:2:0","tags":["Tech Notes"],"title":"Github Page+Hexo搭建blog","uri":"/blog-md/"},{"categories":null,"content":"3. 部署Hexo到GitHub上 将本地博客deploy到GitHub上,输入以下指令 $ npm install hexo-deployer-git --save 然后打开博客目录的_config.yml文件，设置 url: http://lxkaka.github.io/ deploy的type为git，repository(repo) 为：https://github.com/lxkaka/lxkaka.github.io.git（需要在type和repo后面加上一个空格再填写）。（这一步是绑定自己域名）最后在Hexo目录中找到source文件夹，在其中新建一个文件CNAME，没有后缀名（建议使用Sublime Text新建）内容为你的域名（即ＸXXXXX．XXXXXX，不要加ＷＷＷ之类的前缀），保存。输入以下指令： $ hexo g $ hexo deploy//可简写为 hexo d 在执行 hexo deploy 后,出现 error deployer not found:git 的错误处理 输入代码： npm install hexo-deployer-git --save 目前还没绑定自己的域名。所以只能lxkaka.github.io 访问 ","date":"2015-11-24","objectID":"/blog-md/:3:0","tags":["Tech Notes"],"title":"Github Page+Hexo搭建blog","uri":"/blog-md/"},{"categories":null,"content":"发表新文章 用hexo发表新文章 $ hexo n ''my new post‘’ 其中my new post为文章标题，执行命令后，会在项目\\source_posts中生成my new post.md文件，用编辑器打开编写即可。 当然，也可以直接在\\source_posts中新建一个md文件。 写完后，推送到服务器上，执行 $ hexo g #生成 $ hexo d #部署 # 可与hexo g合并为 hexo d -g ","date":"2015-11-24","objectID":"/blog-md/:4:0","tags":["Tech Notes"],"title":"Github Page+Hexo搭建blog","uri":"/blog-md/"},{"categories":null,"content":"Note: 头像不显示解决办法: 接修改layout/_partial/left-col.ejs的第六行和第八行为： \u003cimg src=\"\u003c%=theme.avatar%\u003e\" class=\"js-avatar show\"\u003e \u003cimg src=\"\u003c%=theme.avatar%\u003e\" class=\"js-avatar show\" style=\"width: 100%;height: 100%;opacity: 1;\"\u003e ","date":"2015-11-24","objectID":"/blog-md/:5:0","tags":["Tech Notes"],"title":"Github Page+Hexo搭建blog","uri":"/blog-md/"},{"categories":null,"content":"1.创建自己的Github page 登录 https://github.com/lxkaka ,创建一个新repository，repository name 设置为 lxkaka.github.io,其他选项默认。点确认 然后在这个仓库的页面 https://github.com/lxkaka/lxkaka.github.io 的右边栏点Settings 往下拉找到GitHub Pages -\u003e Automatic Page generator. 访问 https://lxkaka.github.io 就能看到页面。 ","date":"2015-11-24","objectID":"/blog-md/:1:0","tags":["Tech Notes"],"title":"Github Page+Hexo搭建blog","uri":"/blog-md/"},{"categories":null,"content":"2. 安装Hexo 首先安装 hexo modul $ npm install -g hexo 建立一个文件夹，在次文件夹下 $ hexo init $ npm install 本地的Hexo博客已经初步搭建起来，输入以下指令: $ hexo generate//可以简写为 hexo g $ hexo server//可以简写为 hexo s ","date":"2015-11-24","objectID":"/blog-md/:2:0","tags":["Tech Notes"],"title":"Github Page+Hexo搭建blog","uri":"/blog-md/"},{"categories":null,"content":"3. 部署Hexo到GitHub上 将本地博客deploy到GitHub上,输入以下指令 $ npm install hexo-deployer-git --save 然后打开博客目录的_config.yml文件，设置 url: http://lxkaka.github.io/ deploy的type为git，repository(repo) 为：https://github.com/lxkaka/lxkaka.github.io.git（需要在type和repo后面加上一个空格再填写）。（这一步是绑定自己域名）最后在Hexo目录中找到source文件夹，在其中新建一个文件CNAME，没有后缀名（建议使用Sublime Text新建）内容为你的域名（即ＸXXXXX．XXXXXX，不要加ＷＷＷ之类的前缀），保存。输入以下指令： $ hexo g $ hexo deploy//可简写为 hexo d 在执行 hexo deploy 后,出现 error deployer not found:git 的错误处理 输入代码： npm install hexo-deployer-git --save 目前还没绑定自己的域名。所以只能lxkaka.github.io 访问 ","date":"2015-11-24","objectID":"/blog-md/:3:0","tags":["Tech Notes"],"title":"Github Page+Hexo搭建blog","uri":"/blog-md/"},{"categories":null,"content":"发表新文章 用hexo发表新文章 $ hexo n ''my new post‘’ 其中my new post为文章标题，执行命令后，会在项目\\source_posts中生成my new post.md文件，用编辑器打开编写即可。 当然，也可以直接在\\source_posts中新建一个md文件。 写完后，推送到服务器上，执行 $ hexo g #生成 $ hexo d #部署 # 可与hexo g合并为 hexo d -g ","date":"2015-11-24","objectID":"/blog-md/:4:0","tags":["Tech Notes"],"title":"Github Page+Hexo搭建blog","uri":"/blog-md/"},{"categories":null,"content":"Note: 头像不显示解决办法: 接修改layout/_partial/left-col.ejs的第六行和第八行为： \u003cimg src=\"\u003c%=theme.avatar%\u003e\" class=\"js-avatar show\"\u003e \u003cimg src=\"\u003c%=theme.avatar%\u003e\" class=\"js-avatar show\" style=\"width: 100%;height: 100%;opacity: 1;\"\u003e ","date":"2015-11-24","objectID":"/blog-md/:5:0","tags":["Tech Notes"],"title":"Github Page+Hexo搭建blog","uri":"/blog-md/"}]